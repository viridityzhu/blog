<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/favicon-16x16.png" color="#222">
  <meta name="google-site-verification" content="K3y4o5NSJ9eVzz8TCQgPtVHIJuRbuu1Ajq_EBHJyeL0">
  <meta name="msvalidate.01" content="0C0CC354F1706E8CC56204E595D7E3B6">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jyzhu.top","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.8.2","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":true,"nav":null,"activeClass":"valine"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/./public/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="Learning NeRF Reading List Classical  Mildenhall et al. introduced NeRF at ECCV 2020 in the now seminal Neural Radiance Field paper. This is done by storing the density and radiance in a neural volume">
<meta property="og:type" content="article">
<meta property="og:title" content="NeRF Notebook - Part One">
<meta property="og:url" content="https://jyzhu.top/NeRF-Notebook-Part-One/index.html">
<meta property="og:site_name" content="Tianke Youke">
<meta property="og:description" content="Learning NeRF Reading List Classical  Mildenhall et al. introduced NeRF at ECCV 2020 in the now seminal Neural Radiance Field paper. This is done by storing the density and radiance in a neural volume">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221205193423558.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221205193614341.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208172055153.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208172209556.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221205195659841.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208162026398.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208162541770.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208163050867.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208163526289.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208163746237.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208164038729.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208165534056.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208165714329.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208170131069.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208170955887.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208171806113.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208173124734.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208175453557.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208181457315.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221208182302568-0494983.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221211234430727.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221211234923290.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212005012783.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212004946040.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212005908926.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212010316991.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212211525928.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212213802209.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212212838893.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212213151594.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221212213601773.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104183222294.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104183453487.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104185014312.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104185544623.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104202425695.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104202625346.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104203311551.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104204330741.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104204738067.png">
<meta property="og:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20230104204937204.png">
<meta property="article:published_time" content="2023-01-12T09:33:39.000Z">
<meta property="article:modified_time" content="2023-01-12T09:38:18.916Z">
<meta property="article:author" content="Jiayin Zhu">
<meta property="article:tag" content="Computer Vision">
<meta property="article:tag" content="NeRF">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jyzhu.top/Users/jiayinzhu/Documents/笔记/放图/image-20221205193423558.png">


<link rel="canonical" href="https://jyzhu.top/NeRF-Notebook-Part-One/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://jyzhu.top/NeRF-Notebook-Part-One/","path":"NeRF-Notebook-Part-One/","title":"NeRF Notebook - Part One"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>NeRF Notebook - Part One | Tianke Youke</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Tianke Youke" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Tianke Youke</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">A Base for Secreting and Running at Night</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-top"><a href="/top/" rel="section"><i class="fa fa-signal fa-fw"></i>排行榜</a></li>
        <li class="menu-item menu-item-notes"><a href="/categories/Computer-Notes/" rel="section"><i class="fa fa-book fa-fw"></i>笔记</a></li>
        <li class="menu-item menu-item-thoughts"><a href="/categories/thoughts/" rel="section"><i class="fas fa-exclamation fa-fw"></i>随记</a></li>
        <li class="menu-item menu-item-poems"><a href="/categories/poems/" rel="section"><i class="fas fa-cocktail fa-fw"></i>诗</a></li>
        <li class="menu-item menu-item-books"><a href="/books/" rel="section"><i class="fas fa-book-open fa-fw"></i>读书</a></li>
        <li class="menu-item menu-item-movies"><a href="/movies/" rel="section"><i class="fa fa-film fa-fw"></i>电影</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">76</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>日期<span class="badge">166</span></a></li>
        <li class="menu-item menu-item-guestbook"><a href="/guestbook/" rel="section"><i class="fas fa-comment-dots fa-fw"></i>留言板</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#learning-nerf"><span class="nav-number">1.</span> <span class="nav-text">Learning NeRF</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#reading-list"><span class="nav-number">1.1.</span> <span class="nav-text">Reading List</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#classical"><span class="nav-number">1.1.1.</span> <span class="nav-text">Classical</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#survey"><span class="nav-number">1.1.2.</span> <span class="nav-text">Survey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cvpr"><span class="nav-number">1.1.3.</span> <span class="nav-text">2021CVPR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cvpr-1"><span class="nav-number">1.1.4.</span> <span class="nav-text">2022 CVPR</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#useful-references"><span class="nav-number">1.2.</span> <span class="nav-text">Useful References:</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#what-is-nerf"><span class="nav-number">2.1.</span> <span class="nav-text">What is NeRF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sdf---signed-distance-function"><span class="nav-number">2.1.1.</span> <span class="nav-text">SDF - Signed Distance Function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#features-of-nerf"><span class="nav-number">2.2.</span> <span class="nav-text">Features of NeRF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#problem-formulation"><span class="nav-number">2.3.</span> <span class="nav-text">Problem Formulation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#techniques"><span class="nav-number">3.</span> <span class="nav-text">2. Techniques</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#network-architecture"><span class="nav-number">3.1.</span> <span class="nav-text">2.1. Network Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#input-encoding"><span class="nav-number">3.1.1.</span> <span class="nav-text">1. Input Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#activation-functions"><span class="nav-number">3.1.2.</span> <span class="nav-text">2. Activation functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#symmetry-invariance-equivariance"><span class="nav-number">3.1.3.</span> <span class="nav-text">3. Symmetry, Invariance &amp; Equivariance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hybrid-representations"><span class="nav-number">3.2.</span> <span class="nav-text">2.2. Hybrid representations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tradeoffs-of-choosing-a-proper-representation"><span class="nav-number">3.2.1.</span> <span class="nav-text">Tradeoffs of choosing a proper representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#grid"><span class="nav-number">3.2.2.</span> <span class="nav-text">1. Grid</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#point-cloud"><span class="nav-number">3.2.3.</span> <span class="nav-text">2. point cloud</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mesh"><span class="nav-number">3.2.4.</span> <span class="nav-text">3. Mesh</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multiplanar-images"><span class="nav-number">3.2.5.</span> <span class="nav-text">4. Multiplanar Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multiresolution-grids"><span class="nav-number">3.2.6.</span> <span class="nav-text">5. Multiresolution grids</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hash-grids"><span class="nav-number">3.2.7.</span> <span class="nav-text">6. Hash grids</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#codebook-grids"><span class="nav-number">3.2.8.</span> <span class="nav-text">7. Codebook grids</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bounding-volume-hierarchies"><span class="nav-number">3.2.9.</span> <span class="nav-text">8. Bounding Volume Hierarchies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#others-voxel"><span class="nav-number">3.2.10.</span> <span class="nav-text">9. Others (voxel)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#differentiable-forward-maps"><span class="nav-number">3.3.</span> <span class="nav-text">2.3. Differentiable Forward Maps</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#differentiable-rendering"><span class="nav-number">3.3.1.</span> <span class="nav-text">Differentiable rendering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#differentiability-of-the-rendering-function-itself"><span class="nav-number">3.3.2.</span> <span class="nav-text">Differentiability of the rendering function itself</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#differentiation-itself"><span class="nav-number">3.3.3.</span> <span class="nav-text">Differentiation itself</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#special-identity-operator"><span class="nav-number">3.3.4.</span> <span class="nav-text">Special: Identity Operator</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#prior-based-reconstruction-of-neural-fields"><span class="nav-number">3.4.</span> <span class="nav-text">2.4. Prior-based reconstruction of neural fields</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#how-does-the-latent-code-look-like"><span class="nav-number">3.4.1.</span> <span class="nav-text">How does the latent code look like?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#autodecoder-instead-of-encoder-decoder"><span class="nav-number">3.4.2.</span> <span class="nav-text">Autodecoder instead of Encoder-decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#light-field-networks----dont-need-to-render-anymore"><span class="nav-number">3.4.3.</span> <span class="nav-text">Light field networks -- Don&#39;t need to render anymore</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#outlook"><span class="nav-number">3.4.4.</span> <span class="nav-text">Outlook</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#manipulate-neural-fields"><span class="nav-number">3.5.</span> <span class="nav-text">2.5. Manipulate Neural Fields</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#editing-the-input-via-explicit-geometry-left-up"><span class="nav-number">3.5.1.</span> <span class="nav-text">Editing the input via Explicit geometry (left-up)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#editing-the-input-via-neural-flow-fields-left-down"><span class="nav-number">3.5.2.</span> <span class="nav-text">Editing the input via Neural Flow Fields (left-down)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#editing-network-parameters-via-explicit-geometry-right-up"><span class="nav-number">3.5.3.</span> <span class="nav-text">Editing network parameters via Explicit geometry (right-up)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#editing-network-parameters-via-neural-fields-right-down"><span class="nav-number">3.5.4.</span> <span class="nav-text">Editing network parameters via Neural Fields (right-down)</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jiayin Zhu</p>
  <div class="site-description" itemprop="description">温柔而狂野地，<br>我被折断，我的躯体四散；<br>尽管如此，<br>我要的不是岸，我要海浪翻卷。<br><br>厚重而高大的，<br>我被阻断，我的灵魂茫然；<br>尽管如此，<br>我要的不是墙，我要热烈而肆虐的自由。</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">166</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">76</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/viridityzhu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;viridityzhu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhuj.yin@gmail.com" title="E-Mail → mailto:zhuj.yin@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/5243842610" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;5243842610" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/jiayinz" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;jiayinz" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/kayzhuep" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;kayzhuep" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
  </div>


<p></p>
<p>Visitor Map</p>
<div>
<!--   <script type="text/javascript" id="clstr_globe" src="/js/clustrmaps.js?d=gWCOZyJlHF_Sc1eqXROD53yLLxxfC2y7Ytvw9JUfmFg"></script> -->
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=f9f9f9&w=a&t=n&d=gWCOZyJlHF_Sc1eqXROD53yLLxxfC2y7Ytvw9JUfmFg&co=383838&cmo=825bc4&cmn=9acd32&ct=f4f4f4'></script>
</div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jyzhu.top/NeRF-Notebook-Part-One/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiayin Zhu">
      <meta itemprop="description" content="温柔而狂野地，<br>我被折断，我的躯体四散；<br>尽管如此，<br>我要的不是岸，我要海浪翻卷。<br><br>厚重而高大的，<br>我被阻断，我的灵魂茫然；<br>尽管如此，<br>我要的不是墙，我要热烈而肆虐的自由。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tianke Youke">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NeRF Notebook - Part One
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表</span>

      <time title="创建时间:2023-01-12 17:33:39" itemprop="dateCreated datePublished" datetime="2023-01-12T17:33:39+08:00">2023-01-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Computer-Notes/" itemprop="url" rel="index"><span itemprop="name">Computer Notes</span></a>
        </span>
    </span>

  
    <span id="/NeRF-Notebook-Part-One/" class="post-meta-item leancloud_visitors" data-flag-title="NeRF Notebook - Part One" title="阅读量">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读量:</span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Valine:</span>
  
    <a title="valine" href="/NeRF-Notebook-Part-One/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/NeRF-Notebook-Part-One/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">字数:</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="learning-nerf">Learning NeRF</h1>
<h2 id="reading-list">Reading List</h2>
<h3 id="classical">Classical</h3>
<ul>
<li><p>Mildenhall <em>et al.</em> introduced NeRF at ECCV 2020 in the now seminal <a target="_blank" rel="noopener" href="https://www.matthewtancik.com/nerf">Neural Radiance Field paper</a>.</p>
<p>This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images.</p></li>
<li><p><a target="_blank" rel="noopener" href="https://m-niemeyer.github.io/project-pages/giraffe/index.html">GIRAFFE</a>: Compositional Generative Neural Feature Fields</p></li>
</ul>
<h3 id="survey">Survey</h3>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.03805">Apr 2020 - State of the Art on Neural Rendering</a></li>
</ul>
<h3 id="cvpr">2021CVPR</h3>
<p>2021年CVPR还有许多相关的精彩工作发表。例如，提升网络的泛化性：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://alexyu.net/pixelnerf/">pixelNeRF</a>：将每个像素的特征向量而非像素本身作为输入，允许网络在不同场景的多视图图像上进行训练，学习场景先验，然后测试时直接接收一个或几个视图为输入合成新视图。</li>
<li><a target="_blank" rel="noopener" href="https://ibrnet.github.io/">IBRNet</a>：学习一个适用于多种场景的通用视图插值函数，从而不用为每个新的场景都新学习一个模型才能渲染；且网络结构上用了另一个时髦的东西 Transformer。</li>
<li><a target="_blank" rel="noopener" href="https://apchenstu.github.io/mvsnerf/">MVSNeRF</a>：训练一个具有泛化性能的先验网络，在推理的时候只用3张输入图片就重建一个新的场景。</li>
</ul>
<p>针对动态场景的NeRF:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://nerfies.github.io/">Nerfies</a>：多使用了一个多层感知机来拟合形变的SE(3) field，从而建模帧间场景形变。Nerfies: Deformable Neural Radiance Fields</li>
<li><a target="_blank" rel="noopener" href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a>：多使用了一个多层感知机来拟合场景形变的displacement。</li>
<li><a href="https://link.zhihu.com/?target=http%3A//www.cs.cornell.edu/~zl548/NSFF/">Neural Scene Flow Fields</a>：多提出了一个scene flow fields来描述时序的场景形变。</li>
</ul>
<p>其他创新点：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://kai-46.github.io/PhySG-website/">PhySG</a>：用球状高斯函数模拟BRDF（高级着色的上古神器）和环境光照，针对更复杂的光照环境，能处理非朗伯表面的反射。</li>
<li><a target="_blank" rel="noopener" href="https://nex-mpi.github.io/">NeX</a>：用MPI（Multi-Plane Image ）代替NeRF的RGBσ作为网络的输出。</li>
</ul>
<h3 id="cvpr-1">2022 CVPR</h3>
<p><a target="_blank" rel="noopener" href="https://ajayj.com/dreamfields">Zero-Shot Text-Guided Object Generation with <strong>Dream Fields</strong></a></p>
<h2 id="useful-references"><strong>Useful References:</strong></h2>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://markboss.me/post/nerf_at_eccv22/?continueFlag=55ed0f6189bcd6ca987e08764bcbe945">NeRF at ECCV22 - Mark Boss</a></p>
<p><a target="_blank" rel="noopener" href="https://markboss.me/post/nerf_at_neurips22/">NeRF at NeurIPS 2022 - Mark Boss</a></p>
<p><a target="_blank" rel="noopener" href="https://dellaert.github.io/NeRF22/">NeRF at CVPR 2022 - Frank Dellaert</a></p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/PeRRp1cFuH4">CVPR 2022 Tutorial on Neural Fields in Computer Vision</a></p>
</blockquote>
<p>Bigger to learn:</p>
<ul>
<li>[ ] Above NeRF: neural rendering</li>
<li>[ ] Related theories in graphics and computer vision</li>
<li>[ ] NeRF的一作Ben Mildenhall在SIGGRAPH 2021 Course <a target="_blank" rel="noopener" href="https://www.youtube.com/watch%3Fv%3Dotly9jcZ0Jg">Advances in Neural Rendering</a>中从概率的角度推导了NeRF的体渲染公式。</li>
</ul>
<h1 id="introduction">1. Introduction</h1>
<h2 id="what-is-nerf">What is NeRF</h2>
<blockquote>
<p>Reference: Original NeRF paper; an online ariticle</p>
</blockquote>
<p>在已知视角下对场景进行一系列的捕获 (包括拍摄到的图像，以及每张图像对应的内外参)，合成新视角下的图像。</p>
<p>NeRF 想做这样一件事，不需要中间三维重建的过程，仅根据位姿内参和图像，直接合成新视角下的图像。为此 NeRF 引入了辐射场的概念，这在图形学中是非常重要的概念，在此我们给出渲染方程的定义：</p>
<p><embed src="https://pic1.zhimg.com/80/v2-1a80de23a422688b739f36828affb8ec_1440w.webp" /></p>
<p><embed src="https://pic4.zhimg.com/80/v2-c469e4968a3e6cf8ec7a81f816de4f87_1440w.webp" /></p>
<p>那么辐射和颜色是什么关系呢？简单讲就是，光就是电磁辐射，或者说是振荡的电磁场，光又有波长和频率，<span class="math inline">\(波长\times 频率=光速\)</span>，光的颜色是由频率决定的，大多数光是不可见的，人眼可见的光谱称为可见光谱，对应的频率就是我们认为的颜色：</p>
<p><embed src="https://pic1.zhimg.com/80/v2-381aa740f21b7eba1f896fd98dcc1308_1440w.webp" /></p>
<p><embed src="https://pic1.zhimg.com/80/v2-51bd3710b9f891c4c44fde12545e4fd4_1440w.webp" /></p>
<h3 id="sdf---signed-distance-function">SDF - Signed Distance Function</h3>
<p>SDF是一种计算图形学中定义距离的函数。SDF定义了空间中的点到隐式曲面的距离，该点在曲面内外决定了其SDF的正负性。</p>
<p>相较于其他像点云（point cloud）、体素（voxel）、面云（mesh）那样的经典3D模型表示方法，SDF有固定的数学方程，更关注物体的表面信息，具有可控的计算成本。</p>
<h2 id="features-of-nerf">Features of NeRF</h2>
<ul>
<li>Representation can be discrete or continuous. but the discrete representation will be a big one if you have more dimensions, e.g., 3 dim.
<ul>
<li>Actually the Plenoxels try to use 3D grids to store the fields. Fast, however, too much memory.</li>
</ul></li>
<li>Neural Field has advantages:
<ol type="1">
<li>Compactness 紧致:</li>
<li>Regularization: nn itself as inductive bias makes it easy to learn</li>
<li>Domain Agonostic: cheap to add a dimension</li>
</ol></li>
<li>also problems
<ul>
<li>Editability / Manipulability</li>
<li>Computational Complexity</li>
<li>Spectral Bias</li>
</ul></li>
</ul>
<h2 id="problem-formulation">Problem Formulation</h2>
<ul>
<li>Input: multiview images</li>
<li>Output: 3D Geometry and appearance</li>
<li>Objective:</li>
</ul>
<p><span class="math display">\[
\arg \min_x\|y-F(x)\|+\lambda P(x)
\]</span></p>
<p>y is multiview images, F is forward mapping, x is the desired 3D reconstruction.</p>
<p>F can be differentiable, then you can supervise this.</p>
<ul>
<li>nn本身就是某种constraints，你就不需要加太多handicraft constraints</li>
</ul>
<h1 id="techniques">2. Techniques</h1>
<h2 id="network-architecture">2.1. Network Architecture</h2>
<h3 id="input-encoding">1. Input Encoding</h3>
<p>Similar as NLP, they use position encodings. Like Sinusoid functions. I also remember an encoding method which takes into consider of the 光线的散射</p>
<h3 id="activation-functions">2. Activation functions</h3>
<p>ReLU is not perfect for this task. Because it 不能解决对高阶导有constraints的函数。</p>
<p>SIREN is a replacement.</p>
<h3 id="symmetry-invariance-equivariance">3. Symmetry, Invariance &amp; Equivariance</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221205193423558.png" alt="image-20221205193423558" /><figcaption>image-20221205193423558</figcaption>
</figure>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221205193614341.png" alt="image-20221205193614341" /><figcaption>image-20221205193614341</figcaption>
</figure>
<h2 id="hybrid-representations">2.2. Hybrid representations</h2>
<h3 id="tradeoffs-of-choosing-a-proper-representation">Tradeoffs of choosing a proper representation</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208172055153.png" alt="image-20221208172055153" /><figcaption>image-20221208172055153</figcaption>
</figure>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208172209556.png" alt="image-20221208172209556" /><figcaption>image-20221208172209556</figcaption>
</figure>
<p>You may choose one proper representation depending on your own application</p>
<h3 id="grid">1. Grid</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221205195659841.png" alt="image-20221205195659841" /><figcaption>image-20221205195659841</figcaption>
</figure>
<p>Input is too huge. Then you need too huge neural network. So, this grid interpolation acts like a &quot;position encoding&quot;, which encodes the low dimensional features into high dims.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208162026398.png" alt="image-20221208162026398" /><figcaption>image-20221208162026398</figcaption>
</figure>
<p>NeRFusion CVPR22: online!</p>
<h3 id="point-cloud">2. point cloud</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208162541770.png" alt="image-20221208162541770" /><figcaption>image-20221208162541770</figcaption>
</figure>
<p>Cons:</p>
<ol type="1">
<li>To access local points, you need to specifically design the data structure. Otherwise, it is O(n)!</li>
<li>Choose different kernels to retrieve nearby points' features. Oftentimes you assume it is local kernel.</li>
</ol>
<p><img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208163050867.png" alt="image-20221208163050867" style="zoom:50%;" /></p>
<h3 id="mesh">3. Mesh</h3>
<p>Unstructed grids. Compared with point clouds, meshes have connectivity info.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208163526289.png" alt="image-20221208163526289" /><figcaption>image-20221208163526289</figcaption>
</figure>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208163746237.png" alt="image-20221208163746237" /><figcaption>image-20221208163746237</figcaption>
</figure>
<h3 id="multiplanar-images">4. Multiplanar Images</h3>
<p>Something like project a 3D grid into an axis to get levels of planes.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208164038729.png" alt="image-20221208164038729" /><figcaption>image-20221208164038729</figcaption>
</figure>
<p>Pros:</p>
<ol type="1">
<li>Compact</li>
<li>Very efficient because the hardware and software designs are accelerated to these 2D operations, like bi-linear operations.</li>
</ol>
<p>Cons:</p>
<ol type="1">
<li>Resolution bias on plane axis: coz it is discrete betweens planes.</li>
</ol>
<p>This is not very wise in my opinion. It is just a temporary tradeoff given nowadays' technologies. Coz everything will be 3D in the future.</p>
<p><img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208165534056.png" alt="image-20221208165534056" />Generate 2D images from different camera views (perhaps). Key point is the tri-plane representation of 3D features.</p>
<h3 id="multiresolution-grids">5. Multiresolution grids</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208165714329.png" alt="image-20221208165714329" /><figcaption>image-20221208165714329</figcaption>
</figure>
<p>Pros:</p>
<ol start="2" type="1">
<li>Stable coz you indeed need both low and high resolution info</li>
</ol>
<h3 id="hash-grids">6. Hash grids</h3>
<p><img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208170131069.png" alt="image-20221208170131069" /> <span class="math display">\[
[x,y,z]\text{ coordinates}\rightarrow \text{Hash function()} \rightarrow \text{Fixed size codebook}
\]</span> Pros:</p>
<ol type="1">
<li>No matter how big is the original data, you can use a fixed size codebook as the input feature.</li>
<li>Can be online!</li>
</ol>
<p>Cons:</p>
<ol type="1">
<li>May still need large codebooks</li>
<li>Features not spatially local. I don't think the hash grid is a good idea if this drawback exists. But isn't there a simple way to generate features with local info remaining?</li>
</ol>
<h3 id="codebook-grids">7. Codebook grids</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208170955887.png" alt="image-20221208170955887" /><figcaption>image-20221208170955887</figcaption>
</figure>
<p>Instead of storing features of points in grids, store a (index to a) code in a codebook. The size of the codebook is fixed, so the overall size can be controlled as much smaller.</p>
<p>cons:</p>
<ol type="1">
<li>To make the indexing operation differentiable, the computing complexity rises here.</li>
<li>Using hash is to get rid of the complex data structure, but the indices bring it back.</li>
</ol>
<h3 id="bounding-volume-hierarchies">8. Bounding Volume Hierarchies</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208171806113.png" alt="image-20221208171806113" /><figcaption>image-20221208171806113</figcaption>
</figure>
<p>Commonly used method in computer graphics</p>
<h3 id="others-voxel">9. Others (voxel)</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208173124734.png" alt="image-20221208173124734" /><figcaption>image-20221208173124734</figcaption>
</figure>
<ul>
<li>For dynamic nerfs, is there any better hybrid representation? Sure.</li>
<li>Is there any explicit bias of these hybird representations that we can discover and then design regularization? Sure.</li>
</ul>
<h2 id="differentiable-forward-maps">2.3. Differentiable Forward Maps</h2>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208175453557.png" alt="image-20221208175453557" /><figcaption>image-20221208175453557</figcaption>
</figure>
<h3 id="differentiable-rendering">Differentiable rendering</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208181457315.png" alt="image-20221208181457315" /><figcaption>image-20221208181457315</figcaption>
</figure>
<p>Volume rendering can render fogs. Sphere rendering only render the solid surface, and needs ground truth supervision.? Neural renderer combines the two.</p>
<h3 id="differentiability-of-the-rendering-function-itself">Differentiability of the rendering function itself</h3>
<ul>
<li>BRDF Shading? details later.</li>
</ul>
<h3 id="differentiation-itself">Differentiation itself</h3>
<p>Design a neural network with higher order derivatives constraints and therefore directly use its derivative.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221208182302568-0494983.png" alt="image-20221208182302568" /><figcaption>image-20221208182302568</figcaption>
</figure>
<p>For example the Eikonal equation forces the neural network has a derivative as 1. Adding the eikonal loss then promises the neural network valid.</p>
<p>Generally, this kind of problems are: the solutions are constrained by its partial derivatives.</p>
<h3 id="special-identity-operator">Special: Identity Operator</h3>
<p><span class="math display">\[
\text{Reconstruction} \rightarrow \hat 1()\rightarrow \text{Sensor domain}\\
\text{Reconstruction} == \text{Sensor domain}
\]</span></p>
<p>Q&amp;A:</p>
<ul>
<li>Can we obtain a neural network in just one forward, without optimization?</li>
<li>Can we design special forward maps for specific downstream tasks, eg., classification? Absolutely yes. We can design it to represent a compact representation as the sensor domain. The key idea is to get a differentiable function to map your specific recon and sensor domain.</li>
</ul>
<h2 id="prior-based-reconstruction-of-neural-fields">2.4. Prior-based reconstruction of neural fields</h2>
<p>Sounds like a one-shot task: instead of fitting and optimizing a neural field each for one scene; let's learn a prior distribution of neural field. Then, given a specific scene, it adjusts the neural field in just one forward.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221211234430727.png" alt="image-20221211234430727" /><figcaption>image-20221211234430727</figcaption>
</figure>
<h3 id="how-does-the-latent-code-look-like">How does the latent code look like?</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221211234923290.png" alt="image-20221211234923290" /><figcaption>image-20221211234923290</figcaption>
</figure>
<ul>
<li>Global: not local. A small latent code represents a neural field
<ul>
<li>main limitation: can only represent very simple (single) object. coz if you have multiple objects in a scene, the degree of freedom grows non-linearly.</li>
<li><strong>How about giving the natural language descriptions as conditions???</strong></li>
</ul></li>
<li>Local: you get different latent codes considering the locality where you are. So, you have a prior 3D data structure to store the latent codes.
<ul>
<li>3D point clouds -&gt; grids -&gt; triplanes interpolation</li>
</ul></li>
</ul>
<blockquote>
<p>Convolutional Occupancy Networks</p>
</blockquote>
<h3 id="autodecoder-instead-of-encoder-decoder">Autodecoder instead of Encoder-decoder</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212005012783.png" alt="image-20221212005012783" /><figcaption>image-20221212005012783</figcaption>
</figure>
<ul>
<li><p>Encoder is a 2D CNN structure.</p></li>
<li><p>But while using autodecoder, the backpropogate through the forward map (i.e., the neural renderer) will give the 3D structural information to the latent codes directly. <span class="math display">\[
\text{latent code }\hat z=\arg \min_z \|\text{Render(}\Phi)-g.t.\|
\]</span> <img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212004946040.png" alt="image-20221212004946040" /></p></li>
</ul>
<p><strong>Instead of trying to build the encoder, sometimes just use the backpropogation through the forward map is helpful.</strong></p>
<h3 id="light-field-networks----dont-need-to-render-anymore">Light field networks -- Don't need to render anymore</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212005908926.png" alt="image-20221212005908926" /><figcaption>image-20221212005908926</figcaption>
</figure>
<p>Instead of learning a NeRF that you use a neural renderer to generate all points along a ray; you can learn a network to directly give you a color along a ray. So you do not use a 3d coordinate as the query, instead, use a ray.</p>
<p>But this do not work in complicated task yet.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212010316991.png" alt="image-20221212010316991" /><figcaption>image-20221212010316991</figcaption>
</figure>
<h3 id="outlook">Outlook</h3>
<ul>
<li>You don't need to use 600 images of a scene to reconstruct it. Synthesis images?</li>
<li>Open minds: other ways to skip the expensive forward map? (e.g., the light field)</li>
<li>Understanding the scene like humans do: disentangle different objects</li>
<li>Local conditioning methods? Regular grids are easy to tackle with, but it's harder for point clouds / factorized representations</li>
<li>Transformers: seems like local conditioning</li>
</ul>
<h2 id="manipulate-neural-fields">2.5. Manipulate Neural Fields</h2>
<p>Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212211525928.png" alt="image-20221212211525928" /><figcaption>image-20221212211525928</figcaption>
</figure>
<p>You can either edit the input coordinates, or edit the parameters <span class="math inline">\(\theta\)</span>.</p>
<p>On the other axis, you can edit through an explicit geometry, or an implicit neural fields.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212213802209.png" alt="image-20221212213802209" /><figcaption>image-20221212213802209</figcaption>
</figure>
<p>The following examples 落在不同的象限。</p>
<h3 id="editing-the-input-via-explicit-geometry-left-up">Editing the input via Explicit geometry (left-up)</h3>
<ul>
<li><p>You can represent each object using a separated neural field (local frame), and then compose them together in different ways.</p></li>
<li><p>If you want to manipulate not only spatially, but also <strong>temporaly</strong>, it is also possible. You can add a time coordinate as the input of the neural field network, and transform the time input.</p></li>
<li><p>You can also manipulate (especially human body) via <strong>skeleton</strong>.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212212838893.png" alt="image-20221212212838893" /><figcaption>image-20221212212838893</figcaption>
</figure>
<ul>
<li><p><strong>Beyond human</strong>, we can also first estimate different moving parts of an object, to form some skeleton structure, and then do the same.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212213151594.png" alt="Noguchi etal, CVPR22" /><figcaption>Noguchi etal, CVPR22</figcaption>
</figure></li>
</ul></li>
<li><p>Beyond rigid, we can also manipulate via <strong>mesh</strong>. coz we have plenty of manipulation tools on mesh. The deformation on mesh can be re-mapped as the deformation on the input coordinate</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20221212213601773.png" alt="image-20221212213601773" /><figcaption>image-20221212213601773</figcaption>
</figure></li>
</ul>
<h3 id="editing-the-input-via-neural-flow-fields-left-down">Editing the input via Neural Flow Fields (left-down)</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104183222294.png" alt="image-20230104183222294" /><figcaption>image-20230104183222294</figcaption>
</figure>
<p>We use the <span class="math inline">\(f_{i\rightarrow j}\)</span> to edit the <span class="math inline">\(r_{i\rightarrow j}\)</span> to represent one ray into another one.</p>
<p>We need to define the consistency here, so that the network can learn through forward and backward:</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104183453487.png" alt="image-20230104183453487" /><figcaption>image-20230104183453487</figcaption>
</figure>
<h3 id="editing-network-parameters-via-explicit-geometry-right-up">Editing network parameters via Explicit geometry (right-up)</h3>
<p>The knowledge is already in the network. So instead of editing the inputs, we can directly edit the network parameters for generating new things.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104185014312.png" alt="image-20230104185014312" /><figcaption>image-20230104185014312</figcaption>
</figure>
<ul>
<li>This proposed solution makes use of an encoder. The encoder learns to represent the rotated input as a high-dimensional latent code Z, with the same rotation R, in 3-dim space. The the following network use the latent code to generate the <span class="math inline">\(f_\theta\)</span></li>
</ul>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104185544623.png" alt="image-20230104185544623" /><figcaption>image-20230104185544623</figcaption>
</figure>
<ul>
<li>In this work, the key idea is to map the high-resolutional object and the similar but lower resolutional object into the same latent space. Then, you can easily manipulate the lower resolutional object, and it should also affect the higher resolutional one. Then, the shared latent space are put into the following neural field network, which outputs high resolutional results.</li>
</ul>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104202425695.png" alt="image-20230104202425695" /><figcaption>image-20230104202425695</figcaption>
</figure>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104202625346.png" alt="image-20230104202625346" /><figcaption>image-20230104202625346</figcaption>
</figure>
<ul>
<li>This work (Yang et al. NeurlPS'21) about shape editing is &quot;super important&quot; but the speaker does not have enough time... Basically it shows that the tools that we use to manipulate a mesh can also be used on a neural field, where we can keep some of the network parameters to make sure the basic shape of the object the same, and then the magical thing is the &quot;curvature manipulation&quot; item. Given the neural field is differentiable, this can be achieved.</li>
</ul>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104203311551.png" alt="image-20230104203311551" /><figcaption>image-20230104203311551</figcaption>
</figure>
<ul>
<li>Obeying the points (a.k.a generalization). It makes sure the manipulation done on the input points are reconstructed.</li>
</ul>
<h3 id="editing-network-parameters-via-neural-fields-right-down">Editing network parameters via Neural Fields (right-down)</h3>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104204330741.png" alt="image-20230104204330741" /><figcaption>image-20230104204330741</figcaption>
</figure>
<ul>
<li>This work constructs a reasonable latent space of the object, then do interpolation of different objects.</li>
<li>Beyond geometry, we can also manipulate <strong>color</strong></li>
</ul>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104204738067.png" alt="image-20230104204738067" /><figcaption>image-20230104204738067</figcaption>
</figure>
<p>It decomposes the network into shape and color networks, and we can edit each independently.</p>
<figure>
<img src="/Users/jiayinzhu/Documents/笔记/放图/image-20230104204937204.png" alt="image-20230104204937204" /><figcaption>image-20230104204937204</figcaption>
</figure>
<ul>
<li>This is the stylization work. It mainly depends on a different loss function, which does not search for the exact feature of the vgg, but somehow the nearest neighbor.</li>
</ul>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/DeepHuman-3D-Human-Reconstruction-from-a-Single-Image/" rel="bookmark">读 DeepHuman: 3D Human Reconstruction from a Single Image</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/Pixel2Mesh-Generating-3D-Mesh-Models-from-Single-RGB-Images/" rel="bookmark">读Pixel2Mesh- Generating 3D Mesh Models from Single RGB Images</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/Reading-Few-shot-Video-to-Video-Synthesis/" rel="bookmark">Reading Few-shot Video-to-Video Synthesis</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/" rel="bookmark">Reading NeuMan: Neural Human Radiance Field from a Single Video</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/Reading-Pixel2meshPP/" rel="bookmark">Reading Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="followme">
  <span>欢迎关注我的其他发布渠道</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="/atom.xml">
          <span class="icon">
            <i class="fa fa-rss"></i>
          </span>

          <span class="label">RSS</span>
        </a>
      </div>

      <div class="social-item">
        <a target="_blank" class="social-link" href="https://www.zhihu.com/people/jiayinz">
          <span class="icon">
            <i class="fab fa-zhihu"></i>
          </span>

          <span class="label">Zhihu</span>
        </a>
      </div>

      <div class="social-item">
        <a target="_blank" class="social-link" href="https://github.com/viridityzhu">
          <span class="icon">
            <i class="fab fa-github"></i>
          </span>

          <span class="label">GitHub</span>
        </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a>
              <a href="/tags/NeRF/" rel="tag"># NeRF</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/The-difference-between-RNN-s-output-and-h-n/" rel="prev" title="The difference between RNN's output and h_n">
                  <i class="fa fa-chevron-left"></i> The difference between RNN's output and h_n
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="valine-comments"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2018 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-tree"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiayin Zhu</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="网站总字数">251k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="网站阅读时长">3:48</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>


  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"efLA0qg1Vu8kMjj7e1SC6evd-gzGzoHsz","app_key":"uexWsXddCqHicPng8NOVKkcN","server_url":"https://efla0qg1.lc-cn-n1-shared.com","security":true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




<script class="next-config" data-name="valine" type="application/json">{"enable":true,"appId":"efLA0qg1Vu8kMjj7e1SC6evd-gzGzoHsz","appKey":"uexWsXddCqHicPng8NOVKkcN","serverURLs":"https://efla0qg1.lc-cn-n1-shared.com","placeholder":"跟我说点啥吧！\n（昵称、Email、网站都可以不填）","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"lang":null,"visitor":false,"comment_count":true,"recordIP":true,"enableQQ":false,"requiredFields":[],"shortname":"valine","el":"#valine-comments","path":"/NeRF-Notebook-Part-One/"}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.valine.el)
    .then(() => NexT.utils.getScript(
      'https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js',
      { condition: window.Valine }
    ))
    .then(() => {
      new Valine(CONFIG.valine);
    });
});
</script>

</body>
</html>
