<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tianke Youke</title>
  
  <subtitle>A Base for Secreting and Running at Night</subtitle>
  <link href="https://jyzhu.top/atom.xml" rel="self"/>
  
  <link href="https://jyzhu.top/"/>
  <updated>2022-09-01T04:57:08.650Z</updated>
  <id>https://jyzhu.top/</id>
  
  <author>
    <name>Jiayin Zhu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Reading NeuMan: Neural Human Radiance Field from a Single Video</title>
    <link href="https://jyzhu.top/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/"/>
    <id>https://jyzhu.top/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/</id>
    <published>2022-08-24T04:25:25.000Z</published>
    <updated>2022-09-01T04:57:08.650Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://arxiv.org/abs/2203.12575</p><p>作者：Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag</p><p>发表： ECCV22</p><p>开源代码： https://github.com/apple/ml-neuman</p><h2 id="why">Why：</h2><ol type="1"><li><p>人体的渲染和新姿势生成在增强现实的应用中很重要</p></li><li><p>NeRF的出现让新视角生成任务取得很大进步</p></li><li><p>但是现有工作都没有实现：根据单段wild视频，生成新的人物和新的场景</p><figure><img src="https://s2.loli.net/2022/08/24/NdDrEhojmBLCJz9.png" alt="image-20220824123146868" /><figcaption>image-20220824123146868</figcaption></figure></li></ol><h2 id="what">What：</h2><p>读前疑问：</p><ol type="1"><li>NeRF和人体SMPL模型是怎么有机统一的🤔</li></ol><h2 id="how">How：</h2><ol type="1"><li><p>输入是一段wild视频，moving camera的。用现存方法估计人体姿势、人体形状、人体mask（Mask-RCNN）、相机pose、sparse scene model、depth maps</p></li><li><p>然后训练两个 NeRF 模型，一个用于人体，一个用于由 Mask-RCNN 估计的分割mask引导的背景。 此外，通过将来自多视图重建和单目深度回归的深度估计融合在一起来规范场景 NeRF 模型</p></li><li><p>关于NeRF：(参考：<a href="https://zhuanlan.zhihu.com/p/360365941">zhihu</a>）</p><ol type="1"><li><p>NeRF是用神经辐射场建模一个场景，好处是可以生成新视角的图像。针对一个静态场景，需要提供大量相机参数已知的图片。基于这些图片训练好的神经网络，即可以从任意角度渲染出图片结果了。</p></li><li><p>它用MLP，把一个3d场景隐式地编码进神经网络里。输入为3d空间中一个点的坐标<span class="math inline">\(\bold x = (x,y,z)\)</span>和相机视角 <span class="math inline">\(\bold d = (\theta, \phi)\)</span>，输出为该点对应的体素的密度opacity，以及颜色<span class="math inline">\(\bold c = (r,g,b)\)</span>。公式就是 <span class="math display">\[f(\bold{x},\bold{v})=(\bold c, \sigma)\]</span></p></li><li><p>在具体的实现中， x 首先输入到MLP网络中，并输出 σ 和中间特征，中间特征和 d 再输入到额外的全连接层中并预测颜色。因此，体素密度只和空间位置有关，而颜色则与空间位置以及观察的视角都有关系。基于view dependent 的颜色预测，能够得到不同视角下不同的光照效果。</p></li><li><p>NeRF 函数得到的是一个3D空间点的颜色和密度信息，但当用一个相机去对这个场景成像时，所得到的2D 图像上的一个像素实际上对应了一条从相机出发的射线上的所有连续空间点。后续就有各种各样高效的方式来进行可微渲染了，本质上都是从这条射线上采样，获得平均的颜色信息。</p></li></ol></li></ol><h3 id="人体模型nerfsmpl">人体模型：NeRF+SMPL</h3><p>我主要关注的就是人体模型这部分了。总体来说，做法就是：</p><p>首先生成人体NeRF模型，然后用ROMP生成逐帧的人体SMPL模型，然后定义一个canonical的人体模型（主要是去掉姿势这个变量，变成大字型人体），根据像素点在SMPL模型上对应的位置，再对应到canonical模型上，学到人体的外貌。（其实训练中NeRF和SMPL模型是一起学的，没有分得那么开的先后顺序。）</p><figure><img src="https://s2.loli.net/2022/08/24/JSycDH34UlMBsEW.png" alt="image-20220824180150642" /><figcaption>image-20220824180150642</figcaption></figure><p>具体来说：</p><ol type="1"><li><p>对于某一帧图像，用ROMP估计人体的SMPL模型，但采取了一些改良：</p><ol type="1"><li>利用densepose估计人体的silhouette，以及MMPose估计人体的2D joints；根据这些结果优化SMPL参数</li></ol></li><li><p>把刚刚得到的SMPL模型warp成一个canonical的大字型人体模型，这个warp变换称为<span class="math inline">\(\mathcal T\)</span></p></li><li><p>怎么把图像中的像素点对应到canonical的大字型人体模型上呢？</p><ol type="1"><li><p>首先生成人体NeRF模型</p></li><li><p>对于空间中的每个点<span class="math inline">\(\bold x_f=\bold r_f(t)\)</span> （这里的f是第f帧图像），它都可以由一条射线<span class="math inline">\(\bold r\)</span>上对应的像素点渲染而来；那么对这个点直接应用前面的变换<span class="math inline">\(\mathcal{T}\)</span>，就得到它在canonical空间中对应的点了，<span class="math inline">\(\bold x&#39;_f = \mathcal{T}_{\theta_f}(\bold x_f)\)</span></p></li><li><p>但是因为SMPL的估计不是很准确，这个变换<span class="math inline">\(\mathcal{T}\)</span>也不是很准确，所以这里提出来，通过在训练中同时优化SMPL模型 <span class="math inline">\(\theta_f\)</span>和人体NeRF模型的方式，可以提升效果。</p></li><li><p>还有，还加了一个MLP改错网络<span class="math inline">\(\mathcal{E}\)</span>改正warping的误差。最终结果就是： <span class="math display">\[\bold {\tilde x&#39;_f} = \mathcal{T}_{\theta_f}(\bold x_f) + \mathcal{E}(\bold x_f, f)\]</span></p></li><li><p>此时相机视角也需要校正：对于射线ray上的第i个样本点， <span class="math display">\[\bold d(t_i)&#39;_f = \bold {\hat x}&#39;_f(t_i) - \bold {\hat x}&#39;_f(t_{i-1})\]</span></p></li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：https://arxiv.org/abs/2203.12575&lt;/p&gt;
&lt;p&gt;作者：Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag&lt;/p&gt;
&lt;p&gt;发表： ECCV22&lt;/p&gt;
&lt;p&gt;开源代码： https://github.com/apple/ml-neuman&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="3D Generation" scheme="https://jyzhu.top/tags/3D-Generation/"/>
    
  </entry>
  
  <entry>
    <title>It seems impossible to access USB devices in Docker on MacOS</title>
    <link href="https://jyzhu.top/It-seems-impossible-to-access-USB-devices-in-Docker-on-MacOS/"/>
    <id>https://jyzhu.top/It-seems-impossible-to-access-USB-devices-in-Docker-on-MacOS/</id>
    <published>2022-05-16T11:59:12.000Z</published>
    <updated>2022-09-01T04:57:56.460Z</updated>
    
    <content type="html"><![CDATA[<p>According to <a href="https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd" class="uri">https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd</a>, it seems impossible to forward USB to a Docker container on MacOS, coz the Docker is running in a virtual environment via <a href="https://github.com/docker/for-mac/issues/900">hyperkit</a>.</p><p>First of all, ports of the host (i.e., MacOS) cannot be directly accessed by any virtual environment (i.e., Docker) on it. So, &quot;you first have to expose it to the virtual machine where Docker is running&quot;. However, Docker is running on hyperkit, which doesn't support usb forwarding.</p><p>The author provided another way to do it, that is to use<code>docker-machine</code>, which uses a Virtualbox VM to host the <code>dockerd</code> daemon, to replace the original docker. Then... why bother still using Docker, instead of just using Virtualbox to run the seperated environment?</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;According to &lt;a href=&quot;https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd&quot; class=&quot;uri&quot;&gt;https://dev.to/rubberduck/using-usb-with-</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Docker" scheme="https://jyzhu.top/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Reading Video-to-Video Synthesis</title>
    <link href="https://jyzhu.top/Reading-Video-to-Video-Synthesis/"/>
    <id>https://jyzhu.top/Reading-Video-to-Video-Synthesis/</id>
    <published>2022-05-13T09:36:36.000Z</published>
    <updated>2022-09-01T04:59:24.160Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf</p><p>作者：<a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p><p>发表： NeurIPS 2018</p><p>Project：https://tcwang0509.github.io/vid2vid/</p><p>Github：https://github.com/NVIDIA/vid2vid</p><hr /><blockquote><p><strong>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</strong></p><p>这篇论文的背景是当时已经有比较好的pic2pic生成模型了。要让vid2vid也work的话，最重要的应该就是帧与帧之间consistency的问题。所以我会想在将pic2pic生成模型应用在video的基础上，对帧之间加上consistency loss。但直接这样肯定效率很低，因为一个视频中帧与帧之间肯定会包含大量冗余信息嘛，应该还需要想办法让帧之间信息共享，这样模型只需要预测后一帧与前一帧不同的地方，减少运算。例如，用一个小网络预测图像中静态与动态的部分，不知道可不可行。</p><p><strong>看完文章后</strong>：我感觉我在大体思路上把握准了，主要矛盾确实如此。但</p><ol type="1"><li>作者没有用consistency loss，而是用gan的思路，设计了一个条件视频鉴别器 <span class="math inline">\(D_V\)</span>，鉴别视频在时序上的动态是否真实自然。</li><li>我没有optical flow，光流，这方面的知识储备；作者利用一个网络预测optical flow，就可以直接根据前一帧图像得到后一帧图像中对应的像素点了，而且这样的结果能够很consistent。对于前一帧图像中没有对应的像素，再用一个补洞网络补洞。这样就解决了效率问题。</li><li>作者还利用了特征嵌入方法，实现了多模态视频的生成，这是我第一次了解到的方法，感觉很有趣。</li></ol></blockquote><h2 id="why">Why：</h2><ol type="1"><li>图像水平上的生成被研究得很好，但是视频上的此前却比较少；图像生成的成果如果直接放在视频上的话，效果不太妙，因为帧与帧之间缺乏连贯性。所以是需要一些temporal上的改进的</li></ol><h2 id="what">What：</h2><ol type="1"><li>利用GAN和一些时间-空间对抗目标（spatio-temporal adverbial objective），来实现video to video的生成。</li><li>用不同类型的输入来生成新的照片写实风格的视频，效果很好。</li><li>是一个全新定义的vid2vid任务，主要新点在于：输入的vid并不是完整的视频帧，而是一些可以操控的语义信息，例如segmentation masks, sketches, and poses</li></ol><p>读前疑问：</p><ol type="1"><li>在此之前没有比较好的vid2vid，这篇论文是在什么条件下实现了很好的vid2vid效果呢，比如其他方面的技术革新？我觉得主要是利用了gan，一个图像鉴别器+一个视频鉴别器相配合，取得很好的生成效果。除此之外我觉得optical flow用在这里也很好，效率高而且生成效果连贯（但是不知道新不新）；另外特征嵌入方法用在这里也很好，实现了根据语义信息来生成新视频，而且可以是多模态的视频</li><li>摘要里强调的时间-空间对抗目标（spatio-temporal adverbial objective）到底是什么？我感觉正文里好像没有再特别强调时间-空间这一对目标了……根据我自己的理解的话，主要就是那一对鉴别器：时间上--视频鉴别器鉴别在时序上的动态是否真实自然，空间上--图像鉴别器鉴别一张图像在空间上是否真实自然。</li></ol><h2 id="how">How：</h2><h3 id="定义vid2vid任务">定义vid2vid任务</h3><ol type="1"><li><p>定义输入的语义信息序列为<span class="math inline">\(s\)</span>，对应的真实视频序列为<span class="math inline">\(x\)</span>，模型生成的视频序列为<span class="math inline">\(\tilde x\)</span>，则模型的目标是在给定<span class="math inline">\(s\)</span>的条件下，让<span class="math inline">\(\tilde x\)</span>的条件分布拟合<span class="math inline">\(x\)</span>的条件分布</p></li><li><figure><img src="https://s2.loli.net/2022/09/01/VfJE42QpgUB5zxt.png" alt="image-20220524143313609" /><figcaption>image-20220524143313609</figcaption></figure><p>D是discriminator，G是generator。整个任务就变成了一个最大最小优化问题，论文主要通过设计网络和时空优化目标来解决这个问题。</p></li><li><p>为了简化问题，做了一个Markov假设：当前第t帧生成的视频<span class="math inline">\(\tilde x_t\)</span>，由且仅由第t帧输入<span class="math inline">\(s_t\)</span> + 前L帧输入<span class="math inline">\(s_{t-L}^{t-1}\)</span> + 前L帧生成的视频<span class="math inline">\(\tilde x_{t-L}^{t-1}\)</span>这三个因素决定。<img src="https://s2.loli.net/2022/09/01/Q9MPRD5otda3mAq.png" alt="image-20220524144030781" /></p><p>实验里，L取了个不大不小的2.建立了一个神经网络F，递归地逐帧生成视频。</p></li></ol><h3 id="网络架构">网络架构</h3><ol type="1"><li><p>网络F定义如下：</p><figure><img src="https://s2.loli.net/2022/09/01/5OA4ptSUjI9Mn7J.png" alt="image-20220524144722968" /><figcaption>image-20220524144722968</figcaption></figure><p>给定<span class="math inline">\((\tilde x_{t-L}^{t-1}, s_{t}^{t-1})\)</span>作为输入。对于与上一帧图像有关联的像素点，网络会利用optical flow来warp（扭曲？）上一帧像素点，得到这一帧新的像素点。对应等式的前半部分。还有一些像素是上一帧图像里没有的，这时候就需要生成来填充。对应等式的后半部分。具体来说：</p><ul><li>用一个optical flow预测网络W来估计从上一帧到这一帧的optical flow <span class="math inline">\(\tilde w_{t-1}\)</span>.</li><li>用一个生成器H来生成需要填充的像素<span class="math inline">\(\tilde h_t\)</span>.</li><li>用一个mask预测网络M来生成mask <span class="math inline">\(\tilde m_t\)</span>. 这个mask不是非0即1的，而是包含了0-1之间的连续值。这样做是为了更好地融合W和H生成的结果。比如说，在 zoom in 的情况下，一个物体逐渐靠近，那么它会逐帧放大。如果仅仅利用optical flow的扭曲结果，那么这个物体就会变得模糊。因此，还需要生成器来填充一些细节。有了soft mask，warp的像素和生成的像素就可以融合。</li></ul></li><li><p>用了coarse-to-fine的方法来生成高分辨率的视频</p></li><li><p>用了2个不同的discriminator来减轻gan训练中的mode collapse问题（模式倒塌，即生成的结果是很逼真，但是多样性不足）。</p><ol type="1"><li>条件图像鉴别器 <span class="math inline">\(D_I\)</span>，顾名思义，鉴别每一帧图像是否真实的</li><li>条件视频鉴别器 <span class="math inline">\(D_V\)</span>，鉴别视频在时序上的动态是否真实自然。给定optical flow，鉴别K个连续的帧</li></ol></li></ol><h3 id="losses">losses</h3><p><span class="math display">\[\mathop{min}\limits_{F} ( \mathop{max}\limits_{D_I}\mathcal{L}_I (F, D_I ) + \mathop{max}\limits_{D_V}  \mathcal{L}_V (F, D_V )) + λ_W L_W (F ),\]</span></p><ol type="1"><li><span class="math inline">\(\mathcal{L}_I\)</span> 是条件图像鉴别器 <span class="math inline">\(D_I\)</span>的gan loss：<img src="https://s2.loli.net/2022/09/01/J1mVkCnzWYZiaR9.png" alt="image-20220530194307049" />，其中，<span class="math inline">\(\phi_I\)</span> 就是从第1～T帧中随机取1帧的操作</li><li><span class="math inline">\(\mathcal{L}_V\)</span>是条件视频鉴别器 <span class="math inline">\(D_V\)</span>的 gan loss：<img src="https://s2.loli.net/2022/09/01/rq6yiaRSHUnEYzg.png" alt="image-20220530194419594" />，和图像的如出一辙，<span class="math inline">\(\phi_V\)</span> 就是从第1～T帧中随机取连续K帧的操作</li><li><span class="math inline">\(L_W\)</span> 是flow estimation loss：<img src="https://s2.loli.net/2022/09/01/AuZf1zlcihE3KsU.png" alt="image-20220530194534485" />，包括两部分，1. 真实flow和估计flow的端点误差 2. 把前一帧扭曲到后一帧的warp loss</li></ol><h3 id="前景-背景先验">前景-背景先验</h3><p>通过语义分割，给模型提供了一个前景、背景的先验信息，同时把补洞网络拆分成了两个：</p><ol type="1"><li>背景补洞网络：这个很容易，因为整个大背景其实可以由optical flow很准确地预测出来，补洞网络只需要补一点点从画面外面刚进来的部分</li><li>前景补洞网络：这个比较难，因为前景物品往往占比不大，但是又动作幅度很大，前景补洞网络需要从零开始生成很多东西</li></ol><p>通过用户实验，证明大部分人觉得有这个前景-背景先验之后，效果更好。</p><h3 id="多模态生成">多模态生成</h3><p>网络F是一个单模态映射函数，这意味着输入一个视频，它也只能生成一个视频。那怎样让它根据同一个输入视频，输出多个不同的视频呢？这里采用了特征嵌入方法（feature embedding scheme）。</p><ol type="1"><li>输入源视频的同时，也输入instance级别的语义分割mask <span class="math inline">\(s_t\)</span></li><li>训练一个图像编码器E，它把每一帧真实图像<span class="math inline">\(x_t\)</span>编码成d维的特征map（论文中d取3）。然后用一个instance-wise的平均池化，来让同一个物体的所有像素分享共同的特征向量。得到的这个instance-wise平均池化后的图像特征map称为<span class="math inline">\(z_t\)</span></li><li>这个<span class="math inline">\(z_t\)</span>，加上mask <span class="math inline">\(s_t\)</span>，再被输入到网络F</li><li>以上是训练的过程。训练结束后，对每个类型的对象的特征向量的高斯分布拟合一个混合高斯分布。</li><li>测试的时候，利用每个物体所对应的类型的分布，可以sample特征向量，进而生成新视频了。给出不同的特征向量，F就能生成不同的视频了</li></ol><figure><img src="https://tcwang0509.github.io/vid2vid/paper_gifs/cityscapes_change_styles.gif" alt="img" /><figcaption>img</figcaption></figure><h3 id="实现的细节">实现的细节</h3><ol type="1"><li>coarse-to-fine的训练：512 × 256, 1024 × 512, and 2048 × 1024 resolutions，这三种分辨率，先从低的开始训练起，逐渐增加到高的。</li><li>mask预测网络M和flow预测网络W共享权重，只有输出层不一样。</li><li>图像鉴别器是一个多尺度PatchGAN</li><li>除了空间上的多尺度，视频鉴别器还会考虑多个帧率，即时间上的多尺度，确保短期和长期都能consistent</li><li>2k分辨率，8个v100 gpus，训练10天……</li><li>datasets：<ol type="1"><li>Cityscapes：训练DeepLabV3网络来获得所有的语义分割mask，用FlowNet2的结果作为optical flow的ground truth，用Mask R- CNN的结果作为instance- level 语义mask的gt</li><li>Apolloscape</li><li>Face video dataset：FaceForensics dataset里的真实视频</li><li>Dance video dataset：从YouTube下载的跳舞视频💃</li></ol></li></ol><h3 id="结果">结果</h3><p>图像生成模型pix2pixHD和视频风格迁移模型COVST作为baseline。FID（论文定义的视频变种）跟baseline比略好，但human preference score（论文定义的由人来打分，评估哪个视频更真实）高很多。</p><p>通过更改语义mask，可以控制生成视频中的物体种类；通过更改特征向量，可以控制生成视频中的物体外观；在未来视频预测上也有很好的性能。</p><h3 id="局限性">局限性</h3><ol type="1"><li>缺少一个物体内部的具体信息，生成转弯的车的时候效果比较差。论文提出或许可以通过增加3D信息作为输入来解决</li><li>在整个视频中，一个物体的外观有时候还是前后不一致</li><li>偶然情况下，一辆车的颜色可能会逐渐发生变化</li><li>当通过更改语义信息来操纵视频生成的时候，例如把树改成房子，偶然会出现一部分变成房子，另一部分树变了形的情况（？是这个意思吗）。这或许可以通过采用更粗糙的语义标签的方式解决，因为这样模型就不会对标签形状过于敏感</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://tcwang0509.github.io/&quot;&gt;Ting-Chun Wang&lt;/a&gt;, &lt;a href=&quot;http://mingyuliu.net/&quot;&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://www.cs.cmu.edu/~junyanz/&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;https://liuguilin1225.github.io/&quot;&gt;Guilin Liu&lt;/a&gt;, Andrew Tao, &lt;a href=&quot;http://jankautz.com/&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://catanzaro.name/&quot;&gt;Bryan Catanzaro&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发表： NeurIPS 2018&lt;/p&gt;
&lt;p&gt;Project：https://tcwang0509.github.io/vid2vid/&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="Video Generation" scheme="https://jyzhu.top/tags/Video-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Reading Few-shot Video-to-Video Synthesis</title>
    <link href="https://jyzhu.top/Reading-Few-shot-Video-to-Video-Synthesis/"/>
    <id>https://jyzhu.top/Reading-Few-shot-Video-to-Video-Synthesis/</id>
    <published>2022-05-13T08:49:57.000Z</published>
    <updated>2022-09-01T05:00:11.103Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：http://arxiv.org/abs/1910.12713</p><p>作者：<a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, Andrew Tao, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p><p>发表： NeurIPS 2019</p><p>Project： https://nvlabs.github.io/few-shot-vid2vid</p><p>Github：https://github.com/NVLabs/few-shot-vid2vid</p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p><p>首先这个任务选题对我来说很新，我之前都没有意识到过这方面的问题。如果告诉我有这样的问题，需要去解决的话，我的直观的想法会受到这篇论文作者的上一篇中提到的 特征嵌入方法 所影响：会想也通过将一类物体的特征编码起来，然后通过学习不同个体的特征编码，来实现不同风格的视频生成。</p></blockquote><h2 id="why">Why：</h2><p>当今vid2vid方法的两个局限性：</p><ol type="1"><li>需要大量数据，尤其是需要生成的这个人的视频数据</li><li>泛化能力有限，比如说只能在训练集中包含的人上生成新的pose-to-human视频，不能泛化到训练集中不存在的人上</li></ol><p>所以这篇论文就是想解决这两个问题。</p><h2 id="what">What：</h2><ol type="1"><li><p>任务是Video-to-video synthesis，即利用输入的语义视频（例如人的姿势、街景），生成写实的视频。例如说，人体姿势生成的任务，就是首先收集一个人做大量不同动作的视频，作为训练集；然后向模型中输入动作序列，让模型生成该人做该动作的视频。再比如街景生成，也是以大量街景视频作为训练集，然后向模型中输入语义mask序列，让它生成风格类似的全新街景。</p></li><li><p>这篇论文提出了一个网络，其中包括一个网络权重生成模块（novel network weight generation module）和attention机制</p></li><li><p>这个方法的创新点在于，只需要在测试时，向模型提供少量的在训练集中没出现过的新的人物的图像，它就能生成这个新的人的视频。</p><figure><img src="https://s2.loli.net/2022/08/10/IUKDziV4AtOwsxu.png" alt="image-20220513171420180" /><figcaption>image-20220513171420180</figcaption></figure><p>上图中，左边是现存方法，它们基本上对于每个人，都需要在单独的训练集上训练。右边是这篇论文提出的方法，只需要训练一次，然后输入一些示范图像，就可以泛化到新的人上。</p></li></ol><p>读前疑问：</p><ol type="1"><li>说是利用少量的新的人物的示范图像，生成网络权重。意思是以原本的vid2vid网络的权重作为输出？为什么？我的更直观的想法是，直接用一个新网络，学习新人物的图像，然后把output给concat或者加进旧网络的output中……另外，直接作用于网络权重上，在我的粗浅理解中，会不会造成信息的损失呢？还是说本质上没差？ related work里提到这类网络属于adaptive network，跟常规网络相比有不同的inductive bias（想想也是），有对应的应用任务。或许我之后再了解一下这块。</li><li>标题中的few-shot是什么意思，就是指更少的data、更高的泛化性吗？这是一类任务吧，从少量标注的样本中学习的意思。这个确实就是啊，只需要一点点示范图像，就可以生成图中这个人/物的新video。</li><li>作者的上篇论文是利用gan，这篇又用上了attention，为什么作出这样本质的改变呢？</li></ol><h2 id="how">How：</h2><ol type="1"><li>视频生成任务可以分成3类：<ol type="1"><li>unconditional synthesis：随机生成视频片段</li><li>future video prediction</li><li>vid2vid：把语义输入转变成现实风的视频。这篇论文就是属于这个任务，不过它聚焦的点在于few shot，即通过在测试的时候输入少量图像，让生成的视频可以泛化到没见过的domain上</li></ol></li><li><p>vid2vid是前一篇工作的内容啦。reference：<a href="jyzhu.top/reading-video-to-video-synthesis"><em>Reading vid2vid</em></a></p></li><li><p>few-shot本质上就是多加了个网络E，用来生成原补洞网络H的权重。至于原本还有两个网络W和M，他们都不需要改动，因为他们都是基于上一帧生成的图像进行变形的，代表一种运动，而和视频本质的内容没有关系。</p></li><li><p>精髓一图：</p><figure><img src="https://s2.loli.net/2022/08/10/T6is35cjyHlvaNe.png" alt="image-20220810193845710" /><figcaption>image-20220810193845710</figcaption></figure></li><li><p>用最新的SOTA语义图像生成模型SPADE代替了上一篇论文中的网络H。SPADE包含several spatial modulation branches and a main image synthesis branch。不过网络E只更新SPADE模型中的spatial modulation branches的权重，因为1这样量比较小，2这样可以避免一个直接从input image到output image的短路（我尚没有深究原因）。</p></li><li><p>权重生成模块E。</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：http://arxiv.org/abs/1910.12713&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://tcwang0509.github.io/&quot;&gt;Ting-Chun Wang&lt;/a&gt;, &lt;a href=&quot;http://mingyuliu.net/&quot;&gt;Ming-Yu Liu&lt;/a&gt;, Andrew Tao, &lt;a href=&quot;https://liuguilin1225.github.io/&quot;&gt;Guilin Liu&lt;/a&gt;, &lt;a href=&quot;http://jankautz.com/&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://catanzaro.name/&quot;&gt;Bryan Catanzaro&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发表： NeurIPS 2019&lt;/p&gt;
&lt;p&gt;Project： https://nvlabs.github.io/few-shot-vid2vid&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="Video" scheme="https://jyzhu.top/tags/Video/"/>
    
  </entry>
  
  <entry>
    <title>Live demo of CodeTyping via Pythonanywhere</title>
    <link href="https://jyzhu.top/live-demo-of-code-typing-via-pythonanywhere/"/>
    <id>https://jyzhu.top/live-demo-of-code-typing-via-pythonanywhere/</id>
    <published>2022-04-07T15:24:15.000Z</published>
    <updated>2022-04-07T15:34:41.597Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://user-images.githubusercontent.com/39082096/149967763-a9bb56c5-6411-4d86-90d3-f1e22845e2a8.png" alt="image" /><figcaption>image</figcaption></figure><p><a href="http://jyzhu.pythonanywhere.com/">Code Typing Practice</a> (or source code on Github: <a href="https://github.com/viridityzhu/code-typing">here</a>) is a tiny web page that I wrote for fun last semester, which is for me myself to practice code typing. It is a naive Django web app (with bugs🤪). BUT! I find a service to deploy it lively today. That's what is worth noting down now.</p><span id="more"></span><h2 id="note">Note</h2><p>Initially I tried to deploy this demo on Vercel.com. But it was too troublesome coz it does not support Django by default. Thankfully, I found <a href="https://www.pythonanywhere.com/">pythonanywhere</a>, on which each user can deploy one web app without payment. What's the best is that it is really easy to deploy: it provides access to Bash console.</p><p>Two things to be noted:</p><ol type="1"><li>Every 3 months, I have to login into the pythonanywhere to extend my web app, otherwise it will be killed.</li><li>The bug cost most of my time is that in the <code>view.py</code> I had used relative path to the static code files. However, I should use absolute path, with adding <code>BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))</code> ahead.</li></ol><h2 id="todo">TODO</h2><p>Actually, after learned the MERN frame this semester, I am now aware of how naive this project is. However, I love Python, so it doesn't matter if i still regard Django as a hobby🤨. Who knows... I haven't even spent my time on that course project...</p><p>Now that the live demo is achieved, I might think of polishing this little project a bit.</p><ul><li>[ ] Fix bugs. Though i've already forgotten what those bugs are...</li><li>[ ] Replace the stupid code snippets...</li><li>[ ] Add the feature to compute time cost and typing speed. Also, save typing records.</li><li>[ ] Explicitly support other kinds of typing materials, and also support uploading customize materials.</li></ul>]]></content>
    
    
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/39082096/149967763-a9bb56c5-6411-4d86-90d3-f1e22845e2a8.png&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;image&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;http://jyzhu.pythonanywhere.com/&quot;&gt;Code Typing Practice&lt;/a&gt; (or source code on Github: &lt;a href=&quot;https://github.com/viridityzhu/code-typing&quot;&gt;here&lt;/a&gt;) is a tiny web page that I wrote for fun last semester, which is for me myself to practice code typing. It is a naive Django web app (with bugs🤪). BUT! I find a service to deploy it lively today. That&#39;s what is worth noting down now.&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Django" scheme="https://jyzhu.top/tags/Django/"/>
    
    <category term="Python" scheme="https://jyzhu.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>三月的人生主题是复杂性</title>
    <link href="https://jyzhu.top/complexity-again/"/>
    <id>https://jyzhu.top/complexity-again/</id>
    <published>2022-03-14T07:26:36.000Z</published>
    <updated>2022-03-14T07:48:22.442Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我放弃理解很多东西</p><p>我开始拥抱</p><p>惊人的复杂性</p></blockquote><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="450" src="//music.163.com/outchain/player?type=0&amp;id=7326559292&amp;auto=1&amp;height=430"></iframe><p>这两天列了一个名叫「精神状况」的歌单，顾名思义，遴选了大抵是最能有效表达我近来精神状况的9首歌（不出所料以摇滚为主）。奇妙的是，一方面我自诩听歌时对歌词甚是重视，另一方面最爱的歌竟然歌词含量不足50%。想想现实，或许是我放弃理解很多东西（用理性），开始拥抱复杂性了吧（用感性？）。</p><p><img src="https://s2.loli.net/2022/03/14/W5bxdr9w3egqRGF.jpg" style="zoom: 33%;" /></p><hr /><h3 id="最近随想">最近随想：</h3><ul><li>「活着很累，在漫长的生命中，积累的痛苦与折磨会变得绵长。」所以人长大了开始喝酒，一遍遍喝酒，只是因为积压的痛苦无法消解吧。不过这样的好处就是会有一天不再像年轻时候那样怕死了</li><li>我很悲戚，这个世界的悲戚底色近年也逐渐显露，看得浅的人也能举目就看见灰黑色了。很无望，年复一年埋头活着，一抬头就搞不清楚在盼什么。想到父母渐老，大小毛病接连不断；就连我自己都开始显露一些身体不好的迹象，就实在是难过。想到我的生活，海外留学，专业学术，与父母的生活，柴米油盐，家长里短，我们的世界是割裂的，我整个人也感到一种割裂，待在父母身边或者远方，都很悲哀。这种悲哀甚至只是稍纵即逝的，它哪怕能长存一些，我也能对生活稍多些把握。只是时间总会过得很快，可预见的未来还会面临变更，伴随更深、更无力的悲哀，例如作为一个成年人需承担的一个家庭的压力，例如重要的人的衰老、疾病、死亡。</li><li>我有时候对人生很随便，糟糕境遇的发生会被钝感而忽略，或者很快视若无睹，例如疫情和疫情后的世界。只是每一件事，也不至于对我全无影响，我的荒诞感随着它们一层一层加深。</li><li>昨天去外公墓上挂社了，和妈妈、外婆一起。外婆跟外公说很多话，问他这里风景好不好，有没有去哪里钓鱼。妈妈偷偷抹了几次眼泪。我最没用，眼泪大颗大颗滴在纸钱上，都不好烧了。可是我看见墓碑上外公的名字，他笑容灿烂的彩色照片，只是很想念很想念。妈妈让我作揖的时候可以心里跟外公说一些话，告诉他不要担心、请他保佑。可是我一边作揖一边心里只有一个声音：尕公啊 尕公诶[泪]</li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;我放弃理解很多东西&lt;/p&gt;
&lt;p&gt;我开始拥抱&lt;/p&gt;
&lt;p&gt;惊人的复杂性&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;330&quot; height=&quot;450&quot; src=&quot;//music.163.com/outchain/player?type=0&amp;amp;id=7326559292&amp;amp;auto=1&amp;amp;height=430&quot;&gt;
&lt;/iframe&gt;
&lt;p&gt;这两天列了一个名叫「精神状况」的歌单，顾名思义，遴选了大抵是最能有效表达我近来精神状况的9首歌（不出所料以摇滚为主）。奇妙的是，一方面我自诩听歌时对歌词甚是重视，另一方面最爱的歌竟然歌词含量不足50%。想想现实，或许是我放弃理解很多东西（用理性），开始拥抱复杂性了吧（用感性？）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.loli.net/2022/03/14/W5bxdr9w3egqRGF.jpg&quot; style=&quot;zoom: 33%;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>复杂性</title>
    <link href="https://jyzhu.top/complexity/"/>
    <id>https://jyzhu.top/complexity/</id>
    <published>2022-03-14T07:22:53.000Z</published>
    <updated>2022-03-14T07:26:15.368Z</updated>
    
    <content type="html"><![CDATA[<p>我放弃理解很多东西</p><p>我开始拥抱</p><p>惊人的复杂性</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我放弃理解很多东西&lt;/p&gt;
&lt;p&gt;我开始拥抱&lt;/p&gt;
&lt;p&gt;惊人的复杂性&lt;/p&gt;
</summary>
      
    
    
    
    <category term="poems" scheme="https://jyzhu.top/categories/poems/"/>
    
    
  </entry>
  
  <entry>
    <title>Reading Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation</title>
    <link href="https://jyzhu.top/Reading-Pixel2meshPP/"/>
    <id>https://jyzhu.top/Reading-Pixel2meshPP/</id>
    <published>2022-02-06T09:59:29.000Z</published>
    <updated>2022-03-14T07:49:35.086Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://arxiv.org/abs/1908.01491</p><p>作者：Chao Wen, Yinda Zhang, Zhuwen Li, Yanwei Fu</p><p>发表： ICCV 2019</p><p>链接： https://arxiv.org/abs/1908.01491</p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p></blockquote><h2 id="why">Why：</h2><ol type="1"><li>单视角图像3D重建模的效果不够好，尤其是背面，而且泛化能力也差。</li><li>所以增加多个视角的图像：更多视觉信息，且有已经定义得很好的传统方法。</li><li>但是传统方法需要更大量的图像；这时候深度学习模型可以隐式编码视角间的关联，就派上了用场。</li><li>很有用，但是欠研究。</li></ol><h2 id="what">What：</h2><ol type="1"><li>利用多视角图像，固定相机pose参数，利用GCN，从粗糙逐渐精细地变形，生成3D mesh重建模。</li><li>采样mesh模型顶点周围的区域，利用perceptual feature推理出对mesh的形变。</li><li>对于不同种类的物体泛化能力很好。</li></ol><p>读前疑问：</p><ol type="1"><li>似乎就是给pixel2mesh加上一层壳，应用在多视角图像上🤔那么这里的创新点本质在哪儿呢？</li><li>GCN用处大吗，为什么用它？</li><li>采样mesh顶点周围的区域，是什么意思？</li><li>多视角3D重建，应用真的广泛吗？</li></ol><h2 id="how">How：</h2><ol type="1"><li>多视角变形网络 Multi-View Deformation Network (MDN)</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：https://arxiv.org/abs/1908.01491&lt;/p&gt;
&lt;p&gt;作者：Chao Wen, Yinda Zhang, Zhuwen Li, Yanwei Fu&lt;/p&gt;
&lt;p&gt;发表： ICCV 2019&lt;/p&gt;
&lt;p&gt;链接： https://arxiv.org/abs/1908.01491&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="Deep Learning" scheme="https://jyzhu.top/tags/Deep-Learning/"/>
    
    <category term="3D Reconstruction" scheme="https://jyzhu.top/tags/3D-Reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Reading Self-supervised Single-view 3D Reconstruction via Semantic Consistency</title>
    <link href="https://jyzhu.top/Reading-Self-supervised-Single-view-3D-Reconstruction-via-Semantic-Consistency/"/>
    <id>https://jyzhu.top/Reading-Self-supervised-Single-view-3D-Reconstruction-via-Semantic-Consistency/</id>
    <published>2022-01-22T06:49:01.000Z</published>
    <updated>2022-02-05T10:17:41.929Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://arxiv.org/abs/2003.06473</p><p>作者：Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani, Ming-Hsuan Yang, and Jan Kautz</p><p>发表： ECCV 2020</p><p>链接： https://github.com/NVlabs/UMR</p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p></blockquote><h2 id="why">Why：</h2><ol type="1"><li>在3d重建模任务中，同时预测形状、相机位置和材质是一个很大的问题，因为它内在的不确定性。</li><li>现有方法都需要借助各种手段：3D层面的监督、2D语义关键点、shading（这是什么？）、特定类别的3D template 、多视角等等。这些方法需要大量人力，所以很难广泛应用。</li><li>人类会直觉感知到一个物体包括各个部分，比如鸟有两只腿、两个翅膀、一个头，从而识别物体。类似的，cv受此启发，也可以将一个物体定义为多个可变形的部分的集合。</li></ol><h2 id="what">What：</h2><ol type="1"><li>仅需要单张图片+轮廓mask，利用语义一致性，实现自监督3D重建模</li><li>思路：1. 每个物体可以看作由可变形的部分组成；2. 对同一类型的不同物体，它们的每一部分在语义上都是一致的</li><li>通过自监督学习大量同类的图片，可以建立重建的mesh模型与图片之间的语义一致性。这样在同时预测形状、相机位置和材质的时候，可以降低模糊性。</li><li>第一个做到不需要特定类别的template mesh模型或者语义关键点，就可以从单视角图像中实现3d重建模。因此，这个方法可以推广到各种物体类别，而不需要类别的标签</li></ol><p>读前疑问：</p><h2 id="how">How：</h2><h3 id="模型">模型</h3><figure><img src="https://s2.loli.net/2022/01/27/x2BKozeFVpgUPkE.png" alt="image-20220122145159190" /><figcaption>image-20220122145159190</figcaption></figure><ol type="1"><li>（a）是原始图片。需要同一类别的大量图片一起作为输入</li><li>（b）用SCOPS模型（另一篇工作），对图像进行语义分割的结果。这个模型也是自监督的</li><li>（c）标准语义 uv map（Canonical semantic uv map）：<ol type="1"><li>理论上，同一类物体的mesh模型，尽管各自都有不同的形状，但每个点的语义含义都是一致的。</li><li>因此，根据前一步生成的大量语义分割结果，可以生成一张对应这个类别的Canonical语义uv map。</li></ol></li><li>（d）由前一步生成的Canonical语义uv map，可以得到重建的mesh模型表面的点对应的语义标签</li><li>橘色箭头：这个就是语义一致性了，它鼓励2D图像和3D模型之间的语义标签相互一致。这样，就可以解决前面提到过的在3D重建模的时候的“相机-形状不确定性”这个难题</li></ol><h3 id="具体方法">具体方法</h3><h4 id="cmr是baseline">CMR是baseline</h4><ol type="1"><li>用三个decoder <span class="math inline">\(D_{shape}\ D_{camera}\ D_{texture}\)</span> 同时预测mesh模型的形状、相机和材质<ol type="1"><li>形状 <span class="math inline">\(V=\tilde V + \Delta V\)</span>，其中 $V $ 是某类物体的template mesh模型，<span class="math inline">\(\Delta V\)</span> 是预测出来的点的偏移量</li><li>相机pose <span class="math inline">\(\theta\)</span> 是 weak perspective transformation （？）</li><li>材质 <span class="math inline">\(I_{flow}\)</span> 是 UV flow，是将输入图片到UV空间的映射，然后它可以被一个已经定义好的函数<span class="math inline">\(\phi\)</span>映射到mesh模型的表面的每一个点</li></ol></li><li>但是CMR需要人工标注的关键点作为输入，这篇论文主要就是把它去掉了。去掉之后呢，会出现相机+形状同时预测时Ambiguity的问题，所以就想方设法解决这个问题。</li></ol><figure><img src="https://s2.loli.net/2022/01/27/TkWsiJxMN38tKGu.png" alt="image-20220122160143711" /><figcaption>image-20220122160143711</figcaption></figure><h4 id="语义一致性解决相机形状同时预测时的ambiguity">语义一致性：解决相机+形状同时预测时的Ambiguity</h4><p>也就是Fig 3中红色框的部分。</p><ol type="1"><li><p><em>语义部件不变性 semantic part invariance：</em></p><ol type="1"><li>对于2D图像，用SCOPS（自监督co-part语义分割，另一篇论文的方法）可以很准确地对物体各个部分进行分割</li><li>对于3D mesh，每个点的语义含义是固定不变的，就算每个物体会有各自的形变</li></ol></li><li><p><em>语义一致性</em>：</p><ol type="1"><li><p><img src="https://s2.loli.net/2022/01/27/TzCkKA462gRbOFa.png" alt="image-20220124121618081" style="zoom:50%;" /></p><p>从Fig 4 (i) 可以看到，如果没有语义一致性，mesh模型中原本对应头的顶点被当作了翅膀尖，这样错误的变形对应了错误的相机pose。这就是相机+形状同时预测时的Ambiguity。</p></li><li><p>前面已经提到过，可以为每个具体类别生成一张标准语义 uv map（Canonical semantic uv map）。这里，就可以让 每个物体的2D语义分割结果 与 标准语义 uv map 保持一致性，从而让3D模型的每个语义部件跟2D图像里相应的位置有对应关系。这样可以很好地解决相机-形状Ambiguity问题。</p></li></ol></li></ol><h5 id="通过scops实现2d图像中部件的分割">通过SCOPS实现2D图像中部件的分割</h5><p><img src="https://s2.loli.net/2022/01/27/YFNpoKzfxkBRSbI.png" alt="image-20220124114332065" style="zoom:50%;" /></p><p>SCOPS 是自监督的方法，从一类物体的大量图片中发掘共同的语义部件。Fig 10第二行就是它的结果。后面还会提到，通过本文的方法，还可以反过来提升SCOPS的结果：利用生成的标准语义UV map作为伪标注反过来进行监督。</p><h5 id="通过标准语义uv-map实现3d模型中部件的分割">通过标准语义uv map实现3D模型中部件的分割</h5><ol type="1"><li><p>已经有了：</p><ol type="1"><li>模型学到的texture flow <span class="math inline">\(I_{flow}\)</span>可以将输入图片映射到UV空间，然后它可以被一个已经定义好的函数<span class="math inline">\(\phi\)</span>映射到mesh模型的表面的每一个点</li><li>通过SCOPS生成的图像 <span class="math inline">\(i\)</span> 的语义分割结果 <span class="math inline">\(P^i\in R^{H\times W\times N_p}\)</span>， 其中H和W是长和宽， <span class="math inline">\(N_p\)</span> 是语义部件数量</li></ol></li><li><p>这样的话，通过模型的 <span class="math inline">\(I_{flow}\)</span> 就可以把2D的语义分割结果 <span class="math inline">\(P^i\)</span> 映射到 UV 空间，把这个称为 语义UV map</p></li><li><p>理论上来说，同一类别的所有物体都应该得到同一个语义UV map，因为 1. 根据语义部件不变性，mesh模型的每个顶点对应的语义部件都是固定不变的 2. UV map和3d mesh 中的点又是通过<span class="math inline">\(\Phi\)</span>映射的关系，每个顶点对应的UV map上的坐标也是不变的。</p></li><li><p>但是因为SCOPS + <span class="math inline">\(I_{flow}\)</span> 的误差，各个物体生成的语义UV map事实上很不一样。所以这里提出了对 标准语义UV map <span class="math inline">\(\bar P_{uv}\)</span> 的估计方法：</p><ol type="1"><li><p>通过某种方法选择出训练集中效果比较好的子集 <span class="math inline">\(\mathcal{U}\)</span>，对它们的结果进行加和，</p><p><strong>选择样本的方式</strong>：</p><ol type="1"><li>首先选择最好的那一个样本，即 perceptual distance（3D投影到2D的图像与原始RGB图像的知觉距离？）最小的</li><li>然后选择K个跟这个最好的样本最接近的样本，即它们的语义UV map最接近</li></ol><p>公式如下：</p></li><li><p><span class="math display">\[\bar P_{uv}=\frac{1}{|\mathcal{U}|}\sum_{i\in \mathcal{U}}I^i_{flow}(P^i)\]</span></p><p>其中 <span class="math inline">\(I^i_{flow}(P^i)\)</span> 就是通过 <span class="math inline">\(I_{flow}\)</span> 映射语义分割结果 <span class="math inline">\(P^i\)</span> 得到的 语义UV map。</p></li></ol></li></ol><h5 id="d-和-3d-间的语义一致性">2D 和 3D 间的语义一致性</h5><ol type="1"><li><p><em>基于概率的约束 Probability-based constraint</em></p><ol type="1"><li><p><span class="math display">\[L_{sp}=||P^i-\mathcal{R}(\Phi (\bar P_{uv});\theta^i)||^2\]</span></p><p>标准语义UV map <span class="math inline">\(\bar P_{uv}\)</span> 由预定义好的函数 <span class="math inline">\(\Phi\)</span> 映射到 3D mesh表面，然后采用预测好的相机pose <span class="math inline">\(\theta^i\)</span> ，用可微分渲染 <span class="math inline">\(\mathcal{R}\)</span> 将3D模型渲染到2D，然后将结果与对应的由SCOPS生成的部件分割概率图 <span class="math inline">\(P^i\)</span> 做均方误差。</p></li><li><p>注：这个由SCOPS生成的图像分割结果 <span class="math inline">\(P^i\)</span> 是概率数值的形式</p></li><li><p>经验性地选择了采用均方误差MSE，比 KullbackLeibler divergence 效果好</p></li></ol></li><li><p><em>基于顶点的约束 Vertex-based constraint</em></p><ol type="1"><li><p>让3D模型投影回2D之后，被分类到某个语义part的顶点仍然处在图像中该part对应的区域</p></li><li><p><span class="math display">\[L_{sv}=\sum^{N_p}_{p=1} \frac{1}{|\bar V_p|}Chamfer(\mathcal{R}(\bar V_p;\theta^i),Y_p^i)\]</span></p><p>其中，<span class="math inline">\(\bar V_p\)</span> 是已经学好的某类物体的template mesh中属于部件p的那部分，<span class="math inline">\(Y_p^i\)</span>是原始2D图像中属于部件p的那部分，<span class="math inline">\(N_p\)</span> 是语义部件数量。</p></li><li><p>用Chamfer distance是因为投影后的顶点和原始的像素点并不是严格一对一对应的关系</p></li><li><p>用某类物体的template mesh，就可以让网络学相机pose；反之，假如用单个具体物体的mesh的话，网络就仅仅会对3D物体的形状进行扭曲，不会学到正确的相机pose了【我有点不理解为啥】</p></li></ol></li></ol><h3 id="渐进的训练方法em">渐进的训练方法EM</h3><ol type="1"><li><p>之所以要用渐进式训练，是因为</p><ol type="1"><li>需要3D重建模网络首先学会一个大体上可用的texture encoder <span class="math inline">\(I_{flow}\)</span>，然后才能生成标准语义UV map，</li><li>这样还能先生成对应具体类别的template mesh，一方面加速网络的收敛，一方面可以用在前面提到的<em>基于顶点的约束</em>中。</li></ol></li><li><p>但是，如果直接把template mesh和重建模模型全都一起学习的话，效果不好；所以就提出了：EM训练步骤（expectation-maximization期望最大化？），就是先固定一部分学习另一部分。</p><ol type="1"><li><p><strong>E</strong>：固定标准语义UV map和template（初始是球体），<strong>训练重建模网络</strong>。200轮。</p><p>loss包括：</p><ol type="1"><li><p>3D投影到2D的图像与gt剪影的 IoU ✖️ -1</p></li><li><p>3D投影到2D的图像与原始RGB图像的 perceptual distance（知觉距离？）</p></li><li><p>前面提到的基于概率的约束和基于顶点的约束</p></li><li><p>材质循环一致性 Texture cycle consistency：</p><ol type="1"><li><figure><img src="https://s2.loli.net/2022/01/27/SU3aFdKZrOtAiws.png" alt="image-20220124182432533" /><figcaption>image-20220124182432533</figcaption></figure><p>学习texture flow的时候最大的问题：颜色相似的3D mesh的面会被对应到错误的2D图像的像素点上</p></li><li><p>这是一个cycle：强制预测出来的texture flow（2D to 3D）和相机投影（3D to 2D）二者一致。</p></li><li><p>首先定义了<span class="math inline">\(\mathcal{C}_{in}^j\)</span>、 <span class="math inline">\(\mathcal{C}_{out}^j\)</span>分别是输入图像中被映射到三角形面<span class="math inline">\(j\)</span>的一定数量像素点的几何中心，和从三角形面<span class="math inline">\(j\)</span>渲染回2D图像时对应的一定数量像素点的几何中心。公式如下： <span class="math display">\[\mathcal{C}_{in}^j = \frac{1}{N_c}\sum^{N_c}_{m=1}\Phi(I_{flow}(\mathcal{G}^m))_j;\\ \mathcal{C}_{out}^j = \frac{\sum^{H\times W}_{m=1}\mathcal{W}_j^m\times \mathcal{G}^m}{\sum^{H\times W}_{m=1}\mathcal{W}_j^m}\]</span> 其中，<span class="math inline">\(\mathcal{G}^m\)</span>是投影图像的标准坐标网格（包含了像素的坐标<span class="math inline">\((u,v)\)</span>值），<span class="math inline">\(\Phi\)</span>是UV map，<span class="math inline">\(I_{flow}\)</span>把像素映射到3D mesh的面<span class="math inline">\(j\)</span>上；<span class="math inline">\(N_c\)</span>是对应到面<span class="math inline">\(j\)</span>的像素点的数量；<span class="math inline">\(\mathcal{W}\)</span>是可微分渲染时生成的概率map，每个<span class="math inline">\(\mathcal{W}_j^m\)</span>表示面 j 被投影到像素 m 上的概率。</p><ul><li>把重建模mesh模型渲染成2D图像，用的是 Soft Rasterizer，而不是CMR中用的 Neural Mesh Renderer，因为前者可以提供概率map，供texture cycle consistency使用</li></ul></li><li><p>那么，材质循环一致性就是让<span class="math inline">\(\mathcal{C}_{in}^j\)</span>接近 <span class="math inline">\(\mathcal{C}_{out}^j\)</span>： <span class="math display">\[L_{tcyc} = \frac{1}{|F|}\sum^{|F|}_{j=1}||\mathcal{C}_{in}^j-\mathcal{C}_{out}^j||^2_F\]</span></p></li></ol></li><li><p>还有写在附录里的两个loss：</p><ol type="1"><li>graph Laplacian constraint 来鼓励mesh表面平滑【从pixel-mesh中来的】</li><li>edge regularization 来惩罚大小不规则的面 代码里似乎是flatten loss</li></ol></li><li><p>还有写在附录里的对抗训练loss</p></li></ol></li><li><p><strong>M</strong>：利用训练好的重建模网络，更新template（从球体开始）和标准语义UV map。</p><ol type="1"><li><p>template一开始是球体，然后每训练K轮，对它进行一次更新： <span class="math display">\[\bar V_t=\bar V_{t-1} + D_{shape}(\frac{1}{|\mathcal{Q}|}\sum_{i\in \mathcal{Q}}E(I^i))\]</span> <span class="math inline">\(V_{t}\)</span>和 <span class="math inline">\(V_{t-1}\)</span>是更新前后的template，I是输入的图片，经过E生成3D属性，D是形状 encoder。Q是经过某种方式选择出来的部分样本。</p><p><strong>选择样本的方式</strong>：</p><ol type="1"><li>首先选择最好的那一个样本，即与ground truth轮廓的IoU最小的</li><li>然后选择K个跟这个最好的样本最接近的样本，即这些样本的gt轮廓与最好的样本的gt轮廓的IoU越小则越接近</li></ol></li><li><p>这样的话，template <span class="math inline">\(V_t\)</span> 就是选出来的样本的平均形状</p></li></ol></li><li><p>整个训练过程会包括两轮，每轮都包括一个E和一个M。（两轮分别就是代码中的<code>train_s1</code> <code>train_s2</code>。）在E中，训练200 epoch 重建模网络，然后在M中用训练好的网络更新template和标准语义UV map。注意在第一轮中（一轮包括一个E和M），只训练重建模网络，而没有语义一致性约束。</p></li></ol></li></ol><h3 id="实验">实验</h3><ol type="1"><li>数据集：PASCAL3D+中的车和摩托车、CUB-200-2011中的鸟、ImageNet中的马 斑马 牛、OpenImages中的企鹅</li><li>局限性：<ol type="1"><li>依赖于SCOPS提供语义分割，有时候语义分割不准确的话结果就不好</li><li>比较少见的相机pose很难</li><li>细节性的地方效果不好，比如正在飞的鸟的两个翅膀、斑马的腿等</li></ol></li></ol><h1 id="questions">Questions</h1><ol type="1"><li><p>为什么要stage2 ？</p><ol type="1"><li>这两个 stage的主要区别就是：stage1的时候没有用语义一致性约束，在stage2才加上。因为一开始texture flow encoder效果并不好，avg_uv也不准确，所以干脆先不用。所以分成s1和s2，最主要的就是因为在s1训练完之后，重建模网络已经大体可以用了，这时候就可以调用<code>avg_uv.py</code>来生成标准语义UV map，供s2的时候语义一致性用。</li><li>附录里说，从效果上来看，2个stage比1个效果要好，且已经足够好了，有这张图对比了一下：<img src="https://s2.loli.net/2022/01/27/Q26BbRypnrKj7AU.png" alt="image-20220127171646865" /></li></ol></li><li><p>avg_uv 就是学 seg map -&gt; uv map的么？</p><ol type="1"><li>seg map -&gt; uv map这个过程是重建模网络中texture flow这个部分做的事情</li><li>avg_uv就是论文里说的 标准语义 uv map（Canonical semantic uv map）：<ol type="1"><li>理论上，同一类物体的mesh模型，尽管各自都有不同的形状，但每个点的语义含义都是一致的</li><li>因此，对于某一类物体的大量图像数据集（比如鸟），可以生成一张对应这整个类别的avg_uv</li><li>利用这个avg_uv，相当于是给整个类别的template打上了语义标签，后续计算语义一致性约束的时候可以用。</li><li>这个avg_uv的计算过程：<ol type="1"><li>首先用SCOPS（另一篇工作，无监督的）生成所有图像的语义分割结果seg map，然后用重建模网络学到的texture flow映射成uv map</li><li>选择效果最好的一部分instances，对它们的uv map取平均，得到avg_uv</li></ol></li></ol></li></ol></li><li><p>paper 里面 有说固定camera 学shape？ 那代码里有fix camera预测么？</p><p>我好像没有读到paper里有具体说到固定camera学shape耶……</p><p>论文里提到要解决camera-shape一起学时的ambiguity的问题，但不是固定一个学另一个，而是利用avg_uv来实现语义一致性：让 每个物体的2D seg uv map 与 avg_uv 保持一致性，从而让3D模型的每个语义部件跟2D图像里相应的位置有对应关系。</p><p><img src="https://s2.loli.net/2022/01/27/TzCkKA462gRbOFa.png" alt="image-20220124121618081" style="zoom:50%;" /></p><p>从这张图里可以看到，如果没有语义一致性，mesh模型中原本对应头的顶点被当作了翅膀尖，这样的camera就是错误的，错误的camera又造成了错误的shape。而有了语义一致性，就能利用语义让camera 更准确，这样就能跟着提升shape</p></li><li><p>按照他的说法， 先是feature avg 然后 decode 出 average shape。那么这个feature就要学好一点，否则平均容易成球形。那么这个feature 还有其他loss在上面么？比如我们smr 上还有 consistency loss 但是加在 delta_vertice上？他有加在feature上么？否则不能确保这个 feature avg 了以后 还有意义</p><p>我可能没有懂这个问题耶……学长说的是不是计算category level 的 template这个步骤呢？我觉得这个步骤里面保证效果好的方式有这几点比较关键：</p><ol type="1"><li>更新category level 的 template是从M步骤才开始进行的；在此之前，E步骤中会在固定template的前提下，单独训练重建模网络200轮，这个过程中的loss还是挺多的，除了语义一致性没有用以外，其他的loss都用了，包括论文里提出的texture cycle consistency，还有附录里提到的graph Laplacian constraint、edge constraint等等</li><li>计算average template的时候，并不是用了所有数据，而是选择了最好的一部分instances：<ol type="1"><li>首先选择最好的那一个instance，即与ground truth mask的IoU最小的</li><li>然后选择K个跟这个最好的样本最接近的样本，即这些样本的gt mask与最好的样本的gt mask的IoU越小则越接近</li></ol></li></ol></li><li><p>代码里面还放了一些 external的code，有用到么？</p><ol type="1"><li>一个是SoftRas，用来把重建模mesh模型渲染成2D图像。论文里提到用它而不是CMR中用的 Neural Mesh Renderer，是因为它可以提供概率map，供texture cycle consistency使用</li><li>另一个是Neural Mesh Renderer，备选的renderer</li><li>再就是PerceptualSimilarity，用来计算了perceptual loss</li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：https://arxiv.org/abs/2003.06473&lt;/p&gt;
&lt;p&gt;作者：Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani, Ming-Hsuan Yang, and Jan Kautz&lt;/p&gt;
&lt;p&gt;发表： ECCV 2020&lt;/p&gt;
&lt;p&gt;链接： https://github.com/NVlabs/UMR&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="3D Reconstruction" scheme="https://jyzhu.top/tags/3D-Reconstruction/"/>
    
    <category term="Self-supervised" scheme="https://jyzhu.top/tags/Self-supervised/"/>
    
  </entry>
  
  <entry>
    <title>rebellious-person</title>
    <link href="https://jyzhu.top/rebellious-person/"/>
    <id>https://jyzhu.top/rebellious-person/</id>
    <published>2022-01-09T20:21:06.000Z</published>
    <updated>2022-01-09T20:21:35.424Z</updated>
    
    <content type="html"><![CDATA[<p>有的时候我觉得叛逆和听话并不是矛盾的特质。有一种本质的叛逆其实是想清楚了想要什么，然后选择了听话这一个行动模式。为什么说这是叛逆呢，因为这不是真的听话，是某种最极端的不听话。我已经见过了很多的乖乖人，许多一眼就能分辨出来，是真乖乖听话，还是叛逆人所伪装。这种伪装听话的叛逆人大概还有另一个特质，就是有多面性。很多人误把其特异的那些面当作真实面目，觉得听话一面是伪装；其实不然，全都是真实的样貌，没有必要分明。学习一个人要学会容忍他的复杂性。</p><p>我有想过用一些色彩更容易辨明的词来代替「叛逆」：自主，自知，清醒，成熟，明白自己想要什么……可是这都差了点意思。还是叛逆好，因为这毕竟有一种孤勇感在里面；也不贴切，大概是虫子蠕动的guyong感。想象这样一个画面：在肠道一样的世界里，大家都乖乖顺滑着逐渐化为某种排泄物，但是其中有一些guyong者，就是叛逆人。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;有的时候我觉得叛逆和听话并不是矛盾的特质。有一种本质的叛逆其实是想清楚了想要什么，然后选择了听话这一个行动模式。为什么说这是叛逆呢，因为这不是真的听话，是某种最极端的不听话。我已经见过了很多的乖乖人，许多一眼就能分辨出来，是真乖乖听话，还是叛逆人所伪装。这种伪装听话的叛逆人</summary>
      
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>obj format debugging -- vertices order determines face orientation and faces are invisible from back</title>
    <link href="https://jyzhu.top/obj-format-debugging-vertices-order/"/>
    <id>https://jyzhu.top/obj-format-debugging-vertices-order/</id>
    <published>2022-01-05T11:52:42.000Z</published>
    <updated>2022-01-05T12:10:49.164Z</updated>
    
    <content type="html"><![CDATA[<p>Okayyyyyyyy!!!</p><ol type="1"><li><p><strong>Vertices order determines face orientation</strong>.</p><p>e.g., <code>f 1 2 3</code> and <code>f 1 3 2</code> are 2 opposite faces</p></li><li><p>Another thing is, <strong>faces are invisible from backside by default!!!</strong></p><p>In <strong>Meshlab</strong> here is a setting <code>back-face</code> which by default is <code>single</code>. If set it as <code>double</code>, then the face will be visible from backside.<img src="https://s2.loli.net/2022/01/05/CjwX378IKQYpznb.png" /></p><p>Meanwhile, the <strong>Preview</strong> of MacOS also makes faces transparent from opposite orientation. Like this: <img src="https://s2.loli.net/2022/01/05/vVI5SZk1d8cCqx6.png" alt="Stupid sphere no?" style="zoom:50%;" /></p></li></ol><p>Finally my sphere is correct:</p><p><img src="https://s2.loli.net/2022/01/05/jyMqxdsRCiLDlSf.png" alt="correct sphere" style="zoom:50%;" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Okayyyyyyyy!!!&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Vertices order determines face orientation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;e.g., &lt;code&gt;f 1 2 3&lt;/code&gt; and &lt;code&gt;f 1 3 2&lt;/code&gt; are 2 opposite faces&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Another thing is, &lt;strong&gt;faces are invisible from backside by default!!!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;Meshlab&lt;/strong&gt; here is a setting &lt;code&gt;back-face&lt;/code&gt; which by default is &lt;code&gt;single&lt;/code&gt;. If set it as &lt;code&gt;double&lt;/code&gt;, then the face will be visible from backside.&lt;img src=&quot;https://s2.loli.net/2022/01/05/CjwX378IKQYpznb.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;Meanwhile, the &lt;strong&gt;Preview&lt;/strong&gt; of MacOS also makes faces transparent from opposite orientation. Like this: &lt;img src=&quot;https://s2.loli.net/2022/01/05/vVI5SZk1d8cCqx6.png&quot; alt=&quot;Stupid sphere no?&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally my sphere is correct:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.loli.net/2022/01/05/jyMqxdsRCiLDlSf.png&quot; alt=&quot;correct sphere&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Mesh" scheme="https://jyzhu.top/tags/Mesh/"/>
    
    <category term="3D" scheme="https://jyzhu.top/tags/3D/"/>
    
  </entry>
  
  <entry>
    <title>Last day of 2021</title>
    <link href="https://jyzhu.top/Last-day-of-2021/"/>
    <id>https://jyzhu.top/Last-day-of-2021/</id>
    <published>2021-12-31T09:17:59.000Z</published>
    <updated>2021-12-31T09:20:00.070Z</updated>
    
    <content type="html"><![CDATA[<p>没有太多要说的。这首词正好表达了一切我想表达的：</p><blockquote><p>一向年光有限身，等闲离别易销魂，酒筵歌席莫辞频。</p><p>满目山河空念远，落花风雨更伤春，不如怜取眼前人。</p><p>——晏殊《浣溪沙》</p></blockquote><p>共勉。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;没有太多要说的。这首词正好表达了一切我想表达的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一向年光有限身，等闲离别易销魂，酒筵歌席莫辞频。&lt;/p&gt;
&lt;p&gt;满目山河空念远，落花风雨更伤春，不如怜取眼前人。&lt;/p&gt;
&lt;p&gt;——晏殊《浣溪沙》&lt;/p&gt;
&lt;/blockquote&gt;
</summary>
      
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/categories/thoughts/"/>
    
    
    <category term="Literature" scheme="https://jyzhu.top/tags/Literature/"/>
    
  </entry>
  
  <entry>
    <title>免于未来</title>
    <link href="https://jyzhu.top/release-future/"/>
    <id>https://jyzhu.top/release-future/</id>
    <published>2021-12-22T17:04:57.000Z</published>
    <updated>2021-12-22T17:09:10.767Z</updated>
    
    <content type="html"><![CDATA[<p>明天，太阳照常升起<br />世上所有的颜色将逐渐掩埋我眼中的悲伤<br />消失的国度 消失的人<br />我怎么相信啊<br />人怎么可能会消失呢<br />化作一抔黄土<br />去到另一个世界<br />没有人伤心的时候会不信彼岸吧？<br />何时也都能跳舞<br />只是舞在有些事面前都全无意义<br />除了生死<br />哪一桩不是闲事<br />每个人一生中都沉溺在无关紧要的琐事中<br />以免被生命的悲伤侵扰<br />以关闭感官<br />然而酒浇进大海的愁里<br />就好像把我掩埋在这个世界中<br />或者用这个世界的颜色掩埋我<br />或者用琐碎掩埋所有生者<br />可是掩埋一切吧<br />不要掩埋我的亲人<br />重要的亲人啊<br />多希望世界因你从此停摆<br />就停下来吧<br />让我免于未来<br />为什么明天<br />太阳还要照常升起啊</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;明天，太阳照常升起&lt;br /&gt;
世上所有的颜色将逐渐掩埋我眼中的悲伤&lt;br /&gt;
消失的国度 消失的人&lt;br /&gt;
我怎么相信啊&lt;br /&gt;
人怎么可能会消失呢&lt;br /&gt;
化作一抔黄土&lt;br /&gt;
去到另一个世界&lt;br /&gt;
没有人伤心的时候会不信彼岸吧？&lt;br /&gt;
何</summary>
      
    
    
    
    <category term="poems" scheme="https://jyzhu.top/categories/poems/"/>
    
    
    <category term="grandpa" scheme="https://jyzhu.top/tags/grandpa/"/>
    
  </entry>
  
  <entry>
    <title>My Grandfather</title>
    <link href="https://jyzhu.top/My-Grandfather/"/>
    <id>https://jyzhu.top/My-Grandfather/</id>
    <published>2021-12-22T16:57:42.000Z</published>
    <updated>2021-12-22T17:30:30.803Z</updated>
    
    <content type="html"><![CDATA[<p>我的外公去世了，转眼就快半个月了。这件事情在我心中，从不敢相信的说法，变成了模糊的、不真实的说法。我实际上时时提起，它在任何时候都萦绕在我脑中，以至于都很难相信已经过去半个月了；可是其实又很难提起，提起总显得轻飘飘的，这是压在我心中最沉重的一件事。</p><span id="more"></span><p>我当然很爱我的外公，他是一位正直、善良、坚强的人。我的脑中积攒了很多很多关于他的事迹，都是从外婆、妈妈口中听来的。</p><p>外公自己其实跟我交流得很少，时至今日印象最深刻的，竟然是小学的时候外公给我煮面吃，很严厉地批评我：面汤也必须喝完，不能浪费！小时候的我挑食、食量小，这个训诫让我很畏惧。可是我从来没想过面汤也是需要珍惜的事物，因此被外公点醒，也颇感惭愧。以至于记到今日，并且将刻进骨子里。</p><p>我高中的时候其实以十分饱满的感情，在一篇周记中认真地写过外公的事迹与形象。妈妈觉得很感动。我目前倒是不敢翻出来看。多少年后才会无意中看到，然后哭掉呢？我不敢想。</p><p>我的外公啊，他的人生过得应该很不容易的。从小失去双亲，人生中有过一段穷困潦倒的日子，才养成极度节约的习惯。他固执到偏执的程度，才成为这样一位威严的长辈，可是也正因为这性格，年轻时工作中受到过很大的挫折。他的头脑与才能一定是值得尊敬的，琴棋书画都会一些不说，还会修各种电路，拥有一个颇为神秘、包罗万象的工具柜。</p><p>外公迷信算命，我的名字就是他起的，经过很仔细的推算。弟弟出生的时候，外公虽然仍在世，但已经病重，遗憾没能给他起名了。妈妈说，一定是外公算了太多命，触犯了什么规则而受到惩罚，所以晚年自己命苦，云云。最后真的好苦啊，那样一位思维敏捷的智者，终日口不能述、手不能书，忍受着身体各处的疼痛，一天天因为营养不良而消瘦下去，生命的活力不可逆地暗淡下去。</p><p>这些生命不可逆的运动一度让我陷入最深的恐惧。我能想象，但我能想象几何呢。最后一次见到外公已是半年前了，后来的外公是什么样子呢。他在我心中的形象，一切的事迹，相关的回忆，都已经在时间中定格了。</p><p>这是我最伤感的事情。</p><p>一个人还活着，那意味着随着时间，他也是在流动的。可是一个人死去，他就凝固了，这是我还不能接受的事情。跟永恒相比，人的一生短暂得要命。跟浩瀚相比，人的命轻得不值一提。但在我心中，重要的人，其重量就是无法衡量的，其离去就是切切实实地，将我的生命也随之割去了一部分。然而，关于我的生命被割走这件事，我感到感激，也感到踏实。</p><p>我的感想在人之逝世这件事面前，多少微不足道。但我仍然会一遍遍地回忆这件事，我会再一遍遍地提起这件事。这是我对外公思念的方式。</p><blockquote><p>我知道这世界本如露水般短暂。然而然而。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;我的外公去世了，转眼就快半个月了。这件事情在我心中，从不敢相信的说法，变成了模糊的、不真实的说法。我实际上时时提起，它在任何时候都萦绕在我脑中，以至于都很难相信已经过去半个月了；可是其实又很难提起，提起总显得轻飘飘的，这是压在我心中最沉重的一件事。&lt;/p&gt;</summary>
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/categories/thoughts/"/>
    
    
    <category term="grandpa" scheme="https://jyzhu.top/tags/grandpa/"/>
    
  </entry>
  
  <entry>
    <title>读 DeepHuman: 3D Human Reconstruction from a Single Image</title>
    <link href="https://jyzhu.top/DeepHuman-3D-Human-Reconstruction-from-a-Single-Image/"/>
    <id>https://jyzhu.top/DeepHuman-3D-Human-Reconstruction-from-a-Single-Image/</id>
    <published>2021-12-15T06:46:11.000Z</published>
    <updated>2022-02-05T09:45:06.464Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_DeepHuman_3D_Human_Reconstruction_From_a_Single_Image_ICCV_2019_paper.html" class="uri">https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_DeepHuman_3D_Human_Reconstruction_From_a_Single_Image_ICCV_2019_paper.html</a></p><p>作者：Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, Yebin Liu</p><p>发表： ICCV2019</p><p>Code： <a href="https://github.com/ZhengZerong/DeepHuman" class="uri">https://github.com/ZhengZerong/DeepHuman</a></p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p></blockquote><h2 id="why">Why：</h2><ol type="1"><li>现在的人体重建模工作大多需要多视角图像或者多temporal图像，从单张图像重建人体仍然是具有挑战性的工作。</li><li>单张图像重建模的工作，就算是利用人体template（SMPL）的，最好的效果也只是重现形状和姿势，但是在服装层面上的呈现效果很差。BodyNet已经尝试了，但是效果也很差。</li><li>现在的公开数据集都没有大分辨率的、包含普通服饰的表面特征的3D人体数据集。所以只好自己搞了一个。</li></ol><h2 id="what">What：</h2><ol type="1"><li>提出了DeepHuman，体到体（volume to volume）的3D人体重建模</li><li>将从SMPL模型生成的 稠密语义表示 作为额外的输入，来让重建的物体表面更清晰，甚至还能重建不可见的部分</li><li>通过体积特征变换（volumetric feature transformation），把不同尺度的图像特征融合起来，得以恢复精确的表面；然后通过正则提炼网络（normal refinement network)进一步细化表面</li><li>顺便提出了THuman数据集</li></ol><p>读前疑问：</p><ol type="1"><li>SMPL模型是哪来的？震惊，是需要利用HMR之类的方法先生成一个SMPL人体模型，然后再和图像一起作为输入，所以这篇论文是跟前半部分工作——生成SMPL模型——一点关系没有的🤔</li><li>不是自监督吧？不是的，是有3D mesh ground truth的</li><li>看起来效果很好，模型中应该利用了一些bottom-up的人体结构先验知识吧？我猜是不是先用的SMPL构建人体，然后再额外增加一部分网络结构，把人体表面重建得更好呢？没错。</li><li>normal refinement network是啥呢？normal是法线啦，我理解这个其实就是一个简单的UNet网络，把法线贴图upsample成两倍大小</li><li>不同scale的图像特征是怎么利用的？这里提出了一个VFT来把图像特征融合进volume特征里。图像特征怎么来的，简单的conv+relu。volume特征怎么来的，一个立体改良版UNet。所以具体做法就是，在UNet的downsample过程中的每一层，都进行一次VFT，把图像特征融合进volume特征里面。</li><li>THuman数据集好用吗？我看这篇论文有一百多的引用，是因为大家都用上了这个数据集吗？不知道诶……</li></ol><h2 id="how">How：</h2><ol type="1"><li>服装层面的重建模要做好，需要分解成两方面：<ol type="1"><li><em>可视区域的变形自由度要被限制，不然人体结构会被破坏。</em> 为此，这篇文章提出除了原始图像以外，还额外加上两种参数化的人体模型，包括3D语义体+对应的2D语义map</li><li><em>模型要能提取出几何信息，比如服装风格和皱纹。</em> 为此，提出了多尺度体积特征转换。 最后再通过法线投影层，连接生成网络和精炼网络</li></ol></li><li>总的来说，分成3个子任务：<ol type="1"><li>输入图像 -&gt; 参数化人体模型</li><li>图像+人体模型 -&gt; 表面重建</li><li>图像 -&gt; 可视表面再度精细化</li></ol></li><li>THuman数据集，包括7000个穿着230种服装的随机姿势的人体mesh</li></ol><h3 id="模型">模型：</h3><figure><img src="https://s2.loli.net/2021/12/19/GX1IB39Yb5OwAoe.png" alt="image-20211217121247234" /><figcaption>image-20211217121247234</figcaption></figure><ol type="1"><li>对于每个三维模型顶点，生成一个包含3个维度的语义码，包含其空间坐标+姿势信息，映射到二维图像I上，得到语义map Ms</li><li>体素化smpl模型，然后把语义码传播到对应体素中，得到语义体Vs</li><li>定义一个128*192*128的3d占据空间(occupancy volume) Vo，其中表面以内的所有体素值设为1，以外的为0。通过一个图像导向的体到体翻译网络，靠I和Ms的协助，把Vs重建模为Vo</li><li>因为体素体的像素有限，所以最后再用一个UNet，直接把Vo投影成一个2D<a href="https://zh.wikipedia.org/wiki/%E6%B3%95%E7%BA%BF%E8%B4%B4%E5%9B%BE">法线贴图(Normal map)</a> N，也就相当于是可视的表面</li><li>总的来说，模型由三部分组成<ol type="1"><li>图像编码G</li><li>体到体转换网络vol2vol</li><li>法线精炼网络R</li></ol></li></ol><h4 id="图像编码-g">图像编码 G</h4><p>结合I和Ms，得到多尺度的2D特征maps <span class="math inline">\(M_f^{(k)}(k=1,...,K)\)</span></p><h4 id="体特征转换-volumetric-feature-transformation-vft">体特征转换 volumetric feature transformation VFT</h4><figure><img src="https://s2.loli.net/2021/12/19/EBjbG4PztH6cNVO.png" alt="image-20211217121317806" /><figcaption>image-20211217121317806</figcaption></figure><p>VFT这个东西把前面通过图像编码得到的多尺度的2D特征maps <span class="math inline">\(M_f^{(k)}(k=1,...,K)\)</span>融合到vol2vol网络中</p><ol type="1"><li>通过一个卷积+激活，把特征map <span class="math inline">\(M_F^{(k)}\)</span>映射到模块化参数<span class="math inline">\((\alpha_k, \beta_k)\)</span>中</li><li>因为此时<span class="math inline">\((\alpha_k, \beta_k)\)</span>是二维的，而语义体<span class="math inline">\(V_f^{(k)}\)</span>是三维的，所以这里是把<span class="math inline">\(V_f^{(k)}\)</span>在z轴上分成高度为1的一层一层的切片，然后分别跟<span class="math inline">\((\alpha_k, \beta_k)\)</span>做仿射变换（理解成线性变换+平移）</li><li>仿射变换：<span class="math inline">\(VFT(V_f^{(k)}(z_i))=\alpha_k\bigodot V_f^{(k)}(z_i) + \beta_k\)</span>。最后就能得到转换后的Vf了</li></ol><p>相比于直接把特征连接起来，VFT的好处就是说它可以更好地保存形状特征，而且更快更灵活。</p><h4 id="vol2vol">vol2vol</h4><ol type="1"><li>是一个3D的Unet，以Vs和<span class="math inline">\(M_f^{(k)}(k=1,...,K)\)</span>为输入，Vo为输出</li><li>在encoder阶段用VFT把Mf信息融合进Vf来；然后利用skip- connection，这个VFT的变换信息也是可以直接传递到decoder阶段的</li></ol><h4 id="体到法线贴图的投影层-r">体到法线贴图的投影层 R</h4><p>这是一个可微分的体积到法线的投影层，可以根据占据空间直接计算出法线贴图，实现人体表面细节的呈现。</p><figure><img src="https://s2.loli.net/2021/12/19/TL5dxOVwvJGiq9Q.png" alt="image-20211217121336401" /><figcaption>image-20211217121336401</figcaption></figure><ol type="1"><li>根据占据空间生成一个深度图depth map，上图4是生成depth map的步骤：<ol type="1"><li>4（a）中，蓝色圆是输入的模型，从 <span class="math inline">\(p=(x_p,y_p)\)</span> 点开始，沿着z轴扫描每一个体素点的占据情况，就能得到图4（b）</li><li>找到最近的被占据的体素点，即是深度值<span class="math inline">\(D(p)\)</span>，如图4（c）</li><li>这个方法虽然很直观，但是直接来算并不好算，所以实际上是采用一种对体素模型进行变换然后再求值的数学方法算的</li></ol></li><li>将depth map转化成顶点图vertex map，然后用数学方法计算出法线图normal map<ol type="1"><li>根据图像中的位置，把x和y坐标赋值给深度像素</li><li>然后用 Sobel 算子来计算顶点map沿x和y方向的方向导数：<span class="math inline">\(G_x=S_x*M_v,G_y=S_y*M_v,\)</span></li><li>像素点 <span class="math inline">\(p=(x_p,y_p)\)</span>的法线就可以直接计算出来：<span class="math inline">\(N^{(x_py_p)}=G_x(p)\times G_y(p)\)</span></li></ol></li><li>最后用一个 Unet 把 normal map Upsample到两倍大小</li></ol><h3 id="loss-functions">Loss functions</h3><ol type="1"><li><p>3D占据空间的重建模误差，用Binary Cross-Entropy (BCE)表示： <span class="math display">\[L_V = -\frac{1}{|\hat V_o|}\sum_{x,y,z} \gamma \hat V_o^{(xyz)} logV_o^{(xyz)}+(1-\gamma)(1-\hat V_o^{(xyz)})log(1-V_o^{(xyz)})\]</span> <span class="math inline">\(\hat V_o\)</span>是占据空间真实值，<span class="math inline">\(V_o^{(xyz)}\)</span> <span class="math inline">\(\hat V_o^{(xyz)}\)</span>都是在坐标<span class="math inline">\((x,y,z)\)</span>的体素点，<span class="math inline">\(\gamma\)</span> 是一个权重</p></li><li><p>2D剪影（silhouette）的重建模误差，是一个多视角重投影loss： <span class="math display">\[L_{FS} = -\frac{1}{|\hat S_{fv}|}\sum_{x,y} \hat S_{fv}^{(xy)}log S_{fv}^{(xy)}+(1-\hat S_{fv}^{(xy)})log(1-S_{fv}^{(xy)})\]</span> <span class="math inline">\(L_{FS}\)</span>是前视角（front-view）的剪影重投影loss，<span class="math inline">\(S_{fv}\)</span>是<span class="math inline">\(V_o\)</span>的剪影重投影，<span class="math inline">\(\hat S_{fv}\)</span>是相应的真实投影，$ S_{fv}^{(xy)}$ <span class="math inline">\(\hat S_{fv}^{(xy)}\)</span>是在坐标<span class="math inline">\((x,y)\)</span>的相应像素值。</p><p>侧面视角的loss <span class="math inline">\(L_{SS}\)</span>的定义如出一辙。</p></li><li><p>法线图的精炼误差，用余弦距离： <span class="math display">\[L_N = \frac{1}{|\hat N|}\sum_{x,y} 1-\frac{&lt;N^{(xy)},\hat N^{(xy)}&gt;}{|N^{(xy)}|\cdot |\hat N^{(xy)}|}\]</span></p></li><li><p>总loss： <span class="math display">\[L = L_V +\lambda_{FS}L_{FS} +\lambda_{SS}L_{SS}+\lambda_{N}L_{N}\]</span></p></li></ol><h3 id="thuman数据集">THuman数据集</h3><ol type="1"><li>用DoubleFusion方法捕捉3D人体mesh模型</li><li>数据集中包括7000个数据项，每一项有：表面包含了材质的mesh模型 + RGBD图像 + 对应的SMPL模型</li></ol><h3 id="实验">实验</h3><figure><img src="https://s2.loli.net/2021/12/19/acYy6ftBGLN7FRV.png" alt="image-20211217121419341" /><figcaption>image-20211217121419341</figcaption></figure><p>跟这几个比：HMR BodyNet SiCloPe</p><p>效果好极了</p><h3 id="讨论">讨论</h3><p>limitation：</p><ol type="1"><li>必须依赖HMR和SMPLify来为输入图像估计一个SMPL模型</li><li>不可见的地方被过度平滑了</li><li>手只能表示成一团，面部表情也不能刻画</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_DeepHuman_3D_Human_Reconstruction_From_a_Single_Image_ICCV_2019_paper.html&quot; class=&quot;uri&quot;&gt;https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_DeepHuman_3D_Human_Reconstruction_From_a_Single_Image_ICCV_2019_paper.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, Yebin Liu&lt;/p&gt;
&lt;p&gt;发表： ICCV2019&lt;/p&gt;
&lt;p&gt;Code： &lt;a href=&quot;https://github.com/ZhengZerong/DeepHuman&quot; class=&quot;uri&quot;&gt;https://github.com/ZhengZerong/DeepHuman&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="Deep Learning" scheme="https://jyzhu.top/tags/Deep-Learning/"/>
    
    <category term="3D Reconstruction" scheme="https://jyzhu.top/tags/3D-Reconstruction/"/>
    
    <category term="Paper Reading" scheme="https://jyzhu.top/tags/Paper-Reading/"/>
    
  </entry>
  
  <entry>
    <title>读：论视觉知识</title>
    <link href="https://jyzhu.top/%E8%AF%BB%EF%BC%9A%E8%AE%BA%E8%A7%86%E8%A7%89%E7%9F%A5%E8%AF%86/"/>
    <id>https://jyzhu.top/%E8%AF%BB%EF%BC%9A%E8%AE%BA%E8%A7%86%E8%A7%89%E7%9F%A5%E8%AF%86/</id>
    <published>2021-12-15T06:25:17.000Z</published>
    <updated>2021-12-15T06:29:43.189Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://link.springer.com/article/10.1631%2FFITEE.1910001</p><p>作者：潘云鹤</p><p>发表： Frontiers of Information Technology &amp; Electronic Engineering</p><p>链接： https://link.springer.com/article/10.1631%2FFITEE.1910001</p><hr /><blockquote><p>这篇是潘院士的前瞻性思考，虽然比较短，但还是认真读一读。学长分享给我，大概是可以对我们的研究起到一些指导性启迪吧。</p></blockquote><h2 id="why">Why：</h2><h2 id="what">What：</h2><ol type="1"><li>提出“视觉知识”概念</li><li>视觉概念-&gt;视觉命题-&gt;视觉叙事</li></ol><p>读前疑问：</p><h2 id="how">How：</h2>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：https://link.springer.com/article/10.1631%2FFITEE.1910001&lt;/p&gt;
&lt;p&gt;作者：潘云鹤&lt;/p&gt;
&lt;p&gt;发表： Frontiers of Information Technology &amp;amp; Electronic Engineering&lt;/p&gt;
&lt;p&gt;链接： https://link.springer.com/article/10.1631%2FFITEE.1910001&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="Paper Reading" scheme="https://jyzhu.top/tags/Paper-Reading/"/>
    
  </entry>
  
  <entry>
    <title>Check My Evangelion Theme for Typora!!</title>
    <link href="https://jyzhu.top/Check-My-Evangelion-Typora-Theme/"/>
    <id>https://jyzhu.top/Check-My-Evangelion-Typora-Theme/</id>
    <published>2021-12-14T15:13:21.000Z</published>
    <updated>2021-12-18T18:27:38.729Z</updated>
    
    <content type="html"><![CDATA[<p>Heeeeyyyyy!!! I customized a theme called <code>Evangelion</code> for <strong><a href="https://typora.io/">Typora</a> Markdown Editor</strong>, and it was accepted! Do check it on the <a href="https://theme.typora.io/theme/Evangelion/">Typora - Theme official page</a>:</p><figure><img src="https://s2.loli.net/2021/12/14/1AX7Bkra6loxWcR.png" alt="The first one is mine" /><figcaption>The first one is mine</figcaption></figure><figure><img src="https://s2.loli.net/2021/12/14/kNT6uZ2MhrDPSHx.png" alt="Offical page" /><figcaption>Offical page</figcaption></figure><p>And the theme's homepage is here: <a href="https://github.com/viridityzhu/Evangelion-typora-theme" class="uri">https://github.com/viridityzhu/Evangelion-typora-theme</a></p><p>Get downloads: <img src="https://img.shields.io/github/downloads/viridityzhu/Evangelion-typora-theme/total.svg" /></p><p>(People just download, but don't give me a star T^T)</p><h1 id="evangelion-typora-theme">Evangelion-typora-theme</h1><p>An eye-friendly dark theme for <a href="https://typora.io/">Typora</a> Markdown Editor, based on the color scheme of Neon Genesis Evangelion.</p><figure><img src="https://theme.typora.io/media/theme/evangelion/eva0.png" alt="Image of Evangelion found from Google" /><figcaption>Image of Evangelion found from Google</figcaption></figure><h2 id="feature">Feature</h2><ul><li>Supports <strong>English</strong> and <strong>中文</strong>.</li><li>Eye-friendly <strong>dark mode</strong>.</li><li>Code color scheme imported from <a href="https://codemirror.net/theme/material-ocean.css">material-ocean</a>.</li><li><em>I love Evangelion Unit-01!</em></li></ul><h2 id="screenshots">Screenshots</h2><figure><img src="https://theme.typora.io/media/theme/evangelion/eva1.png" alt="套娃现场" /><figcaption>套娃现场</figcaption></figure><div><p float="left"><img src="https://theme.typora.io/media/theme/evangelion/eva2.png" width="49%" style="display:inline !important"/> <img src="https://theme.typora.io/media/theme/evangelion/eva3.png" width="49%" style="display:inline !important"/></p></div><h2 id="install">Install</h2><ul><li>Download the <code>Eva.css</code> file: see <a href="https://github.com/viridityzhu/Evangelion-typora-theme/releases">releases</a>.</li><li>Copy and paste the file in to the <strong>Typora Theme Folder</strong>:<ul><li><code>Preferences…</code> &gt; <code>Appearence</code> &gt; <code>Open Theme Folder</code>.</li></ul></li><li>Restart Typora, then select <code>Theme</code> &gt; <code>Eva</code> to apply.</li></ul><p>安装：</p><ul><li>下载<code>Eva.css</code>文件: <a href="https://github.com/viridityzhu/Evangelion-typora-theme/releases">releases</a></li><li>将该文件粘贴到<strong>Typora主题文件夹</strong>中：<ul><li><code>偏好设置</code> &gt; <code>外观</code> &gt; <code>打开主题文件夹</code></li></ul></li><li>重启Typora，然后选择<code>主题</code> &gt; <code>Eva</code>，主题即可生效。</li></ul><h2 id="reference">Reference</h2><p>I take the color palette of <a href="https://en.wikipedia.org/wiki/Neon_Genesis_Evangelion">Neon Genesis Evangelion</a>.</p><p>And this theme code is inspired by both <a href="https://github.com/imageslr/typora-theme-bear">bear-dracula</a> and <a href="https://github.com/Y1chenYao/typora-mint-theme">mint-dark</a>. I love these 2 themes, too!</p><p><em>Designed and tested on macOS. Not fully tested, but should work for Windows/Linux.</em></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Heeeeyyyyy!!! I customized a theme called &lt;code&gt;Evangelion&lt;/code&gt; for &lt;strong&gt;&lt;a href=&quot;https://typora.io/&quot;&gt;Typora&lt;/a&gt; Markdown Editor&lt;/strong&gt;, and it was accepted! Do check it on the &lt;a href=&quot;https://theme.typora.io/theme/Evangelion/&quot;&gt;Typora - Theme official page&lt;/a&gt;:&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2021/12/14/1AX7Bkra6loxWcR.png&quot; alt=&quot;The first one is mine&quot;&gt;&lt;figcaption&gt;The first one is mine&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2021/12/14/kNT6uZ2MhrDPSHx.png&quot; alt=&quot;Offical page&quot;&gt;&lt;figcaption&gt;Offical page&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;And the theme&#39;s homepage is here: &lt;a href=&quot;https://github.com/viridityzhu/Evangelion-typora-theme&quot; class=&quot;uri&quot;&gt;https://github.com/viridityzhu/Evangelion-typora-theme&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Typora" scheme="https://jyzhu.top/tags/Typora/"/>
    
    <category term="Theme" scheme="https://jyzhu.top/tags/Theme/"/>
    
    <category term="Evangelion" scheme="https://jyzhu.top/tags/Evangelion/"/>
    
  </entry>
  
  <entry>
    <title>阅读总结与别无所指</title>
    <link href="https://jyzhu.top/On-Reading-in-November/"/>
    <id>https://jyzhu.top/On-Reading-in-November/</id>
    <published>2021-12-01T16:01:41.000Z</published>
    <updated>2021-12-14T15:57:21.220Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/12/02/Hf8gyxVdShTbzCO.png" alt="我的微信读书11月阅读时长" style="zoom: 33%;" /></p><p>11月平均每日阅读43分钟，最高一天阅读4小时，总共21个小时。这意味着，这30天里，我大约有一整天的人生活在书里的世界。幸甚至哉！</p><p>这些时间最大头还是花在了<a href="https://jyzhu.top/2021/11/27/After-Reading-The-Wind-Up-Bird-Chronicle-by-Murakami-Haruki/">《奇鸟行状录》里（不太值）</a>，其次应该是钱钟书的《宋诗选注》和黑格尔的《小逻辑》里。看上去好奇怪的组合，不过不然。这其实很符合我的阅读习惯，一般是不同类型的书同时阅读，每次依当天的状态和心情去择一本翻开。实际上，想读《小逻辑》的时候，和想读《奇鸟行状录》的时候，整个人的状态是完全不一样的。据我观察，当我精神状态极佳、心情大好的时候，最想读哲学类的书；情感泛滥（高涨或者低落）的时候，最想读诗；感到疲乏的时候，想读小说来消遣。我倒也根据这么一点小习性，大言不惭地自觉“随性”，除了读书随心所欲以外，生活中大大小小的事上，心态也大抵类似。（主要是小事，大事实际上还是有点优柔寡断的，还想改呢。）</p><p>不过呢，这个月挑来读的这几本书，跟我之前的阅读惯性相比还是比较突兀的。一方面，小说上，我已经挺久没读日本小说了。虽然以前也喜欢村上春树，但这次是带着一些“怀旧”的心情去读他的。根据我的规划，在这个人生阶段，我是想读一些陀氏和海明威的，再往后也想涉猎一些不太熟悉但十分严肃的作家。</p><p>另一方面，这是我头一次迈入哲学原著的门🚪，精挑细选之后，才决定从黑格尔读起：人们说黑格尔是最后一个拥有完整的大哲学体系的哲学家；人们还说要想真的了解哲学，首先应该好好了解黑格尔。另外，黑格尔的辩论法，是想了解后世大多数哲学家，绕不过的一个话题。至于选择《小逻辑》，纯粹是贪图它比较短小精悍，但也完整包含了黑格尔整个逻辑学的思想，想来比较容易上手……然而读起来也不枉“晦涩”一词。我读得确实缓慢，但只要是一眼小小的泉眼，也总会源头活水慢慢来的。</p><p>宋诗嘛，是头脑一热想读古诗试试。之前喜欢的是现代诗，对古诗颇感敬畏。读起来其实很有趣，我以前对古诗的印象，是觉得所有诗都主题雷同，用的意象翻来覆去那几个。但捧起读来，才能真的体会到个中妙处。就是那么一板一眼的格律，那么受限的意象，那么几个主题，然每首诗都有实在的新意。我发现我渐渐开始喜欢起写景的诗来。</p><p>在新加坡这永无止境的夏季中，我心之所向的莫过于这样的场景了——</p><blockquote><p>嘉果浮沉酒半醺，床头书册乱纷纷。</p><p>北轩凉吹开疏竹，卧看青天行白云。</p><p>-- 苏舜钦《暑中闲咏》</p></blockquote><p>另外，读到一个有趣的诗人李觏，钱钟书说他写诗喜欢用独特的意象和“特立独行”的字。听上去这个评价并不是特别褒义，我倒是意外很喜欢。</p><blockquote><p>人言落日是天涯，望极天涯不见家；</p><p>已恨碧山相阻隔，碧山还被暮云遮！</p><p>-- 李觏《乡思》</p></blockquote><p>这种蕴含奇思的诗，可能美感会被削弱，但是灵气却是独具，是我所偏爱！</p><p>还有一位跟他评价相近的诗人王令，他的这首《暑旱苦热》是我目前印象最深的。天哪，一首诗难道不就是要体现诗人“手提天下”这样的意致纵横吗！</p><blockquote><p>清风无力屠得热，落日着翅飞上山。</p><p>人固已惧江海竭，天岂不惜河汉干？</p><p>昆仑之高有积雪，蓬莱之远常遗寒。</p><p><strong>不能手提天下往，何忍身去游其间？</strong></p><p>-- 王令《暑旱苦热》</p></blockquote><hr /><p>我人生的阅读量高峰，集中在高中三年，不长不短。大学以来，整个人“务实”了许多，大多数时间分给了小时候心目中“没有意义”的琐碎生活。这是有意义的。不过因为记忆能力的缺憾，我整个就好像一个虽然读写速度快，但容量很低的内存（我也不想混入这么奇怪的计算机比喻啊喂(#`O′)）……见什么东西，理解很快，忘得也很快。今天刚跟小熊凄惨地对了一下去年看的JOJO第四季的人物关系，发现我的记忆已经完全模糊混乱了。所以嘛，大部分读过的书，到最近一两年，几乎已经面目模糊了。我在想，有的书与我的关系可能已经衰退到了 听到书名时知道这本我读过 的地步，虽然我发誓我读每本书的时候都很认真（还很慢）。</p><p>以前在知乎上刷到“读过的书还是会忘记，那么读书到底有什么意义”这类问题，还会饶有兴致地点开认真看；现在看到倒是觉得看得明白了。要我回答的话，我认为这个问题本身是不成立的。问读书对人的意义在哪，不是和问“郊游”对人的意义在哪一样嘛。我能理解那种功利性的想法，把读书和学一门特长类比，希望带来一些价值；但我想这类想法都涉嫌自指，即一种逻辑上的悖论，是不成立的。（其实我很不喜欢“特长”这个词，因为它是功利性的；“兴趣”这个词才本质嘛。）</p><p>生活的目的（而不是意义）就是生活，再往后一步别无所指。我每日学习，阅读，喝酒（不是真的每天喝酒），晒太阳，这些事情的目的是什么呢？就是为了学习，阅读，喝酒，晒太阳呀！（去码头整点薯条笑话就是最准确的表达。）</p><p>想起来上次妈妈分享的一个有关性格、品质的心理学测试，测出来好多好多个评价维度，其中我最低的两项是什么“希望”“活力”，给我整挺乐的。那些问题问出来，按我的想法，选的确实都是最显得悲观的答案；但是这好像没法评价我这类荒诞主义<strong>信仰</strong>啊（瞎造了一个词）。虽然听上去很没有活力，但恰恰是超级有活力的！只不过望穿人生舞台的布景，直面整个荒诞而已，这不是对生活真正的热爱吗？</p><p>跟我心境最相近的毫无疑问是苏轼这一首词，我的最爱，没有之一：</p><blockquote><p>落日绣帘卷，亭下水连空。知君为我新作，窗户湿青红。</p><p>长记平山堂上，欹枕江南烟雨，杳杳没孤鸿。认得醉翁语，“山色有无中”。</p><p>一千顷，都镜净，倒碧峰。<strong>忽然浪起，掀舞一叶白头翁。</strong></p><p>堪笑兰台公子，未解庄生天籁，刚道有雌雄。<strong>一点浩然气，千里快哉风。</strong></p><p>-- 苏轼《水调歌头·黄州快哉亭赠张偓佺》</p></blockquote><p>我觉得人生诗意（或者一般语境中的意义），不说全部，至少一半都在这一句中了吧：“忽然浪起，掀舞一叶白头翁。”哈哈哈哈，解释不明白。</p><p>我其实最近才承认自己热爱生活。之前也确实中了常规思路的桎梏，觉得自己是消极悲观的。可是我方才自觉是极热爱生活的。就好像热爱一些自由度高的沙盒游戏，或者决策系统很复杂的策略游戏一样，我也热爱兼具这两种特性的生活，热爱生活的这两种特性。不过由于一些理科生的科学主义<strong>信仰</strong>，我倒是保有这样一种偏颇的理想：好想通过建模和参数来解释甚至预测整个世界啊哈哈哈。</p><p>最后再自己跟自己（因为大概没有别人会看吧）解释一下前文几处加粗的“<strong>信仰</strong>”二字的额外含义。其实只是一种疑心，既怀疑我对这些思想的理解之正确性，也怀疑这些思想本身的正确性。进一步来说，一方面意味着我仍然没有把这里写的任何东西当真，认为这些都有可能是错误的，并且可能会被之后的我推翻。另一方面，也意味着正如破除对“意义”的幻想一样，我当然也破除了对所有思想的幻想。再赘述一句，这类幻想的破除并不是消极的，而是勇敢的。是对应那句：</p><blockquote><p>我要的不是岸，我要海浪翻卷。</p></blockquote><p>写得也太零碎了，但能藉由阅读这个主题，把我最近这些细碎的想法一股脑说出来，倒也畅快。只是行文不得不说是坑坑洼洼，离我想达到的表达水平差远了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/12/02/Hf8gyxVdShTbzCO.png&quot; alt=&quot;我的微信读书11月阅读时长&quot; style=&quot;zoom: 33%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;11月平均每日阅读43分钟，最高一天阅读4小时，总共21个小时。这意味着，这30天里，我大约有一整天的人生活在书里的世界。幸甚至哉！&lt;/p&gt;
&lt;p&gt;这些时间最大头还是花在了&lt;a href=&quot;https://jyzhu.top/2021/11/27/After-Reading-The-Wind-Up-Bird-Chronicle-by-Murakami-Haruki/&quot;&gt;《奇鸟行状录》里（不太值）&lt;/a&gt;，其次应该是钱钟书的《宋诗选注》和黑格尔的《小逻辑》里。看上去好奇怪的组合，不过不然。这其实很符合我的阅读习惯，一般是不同类型的书同时阅读，每次依当天的状态和心情去择一本翻开。实际上，想读《小逻辑》的时候，和想读《奇鸟行状录》的时候，整个人的状态是完全不一样的。据我观察，当我精神状态极佳、心情大好的时候，最想读哲学类的书；情感泛滥（高涨或者低落）的时候，最想读诗；感到疲乏的时候，想读小说来消遣。我倒也根据这么一点小习性，大言不惭地自觉“随性”，除了读书随心所欲以外，生活中大大小小的事上，心态也大抵类似。（主要是小事，大事实际上还是有点优柔寡断的，还想改呢。）&lt;/p&gt;
&lt;p&gt;不过呢，这个月挑来读的这几本书，跟我之前的阅读惯性相比还是比较突兀的。一方面，小说上，我已经挺久没读日本小说了。虽然以前也喜欢村上春树，但这次是带着一些“怀旧”的心情去读他的。根据我的规划，在这个人生阶段，我是想读一些陀氏和海明威的，再往后也想涉猎一些不太熟悉但十分严肃的作家。&lt;/p&gt;</summary>
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>After Reading The Wind-Up Bird Chronicle by Murakami Haruki</title>
    <link href="https://jyzhu.top/After-Reading-The-Wind-Up-Bird-Chronicle-by-Murakami-Haruki/"/>
    <id>https://jyzhu.top/After-Reading-The-Wind-Up-Bird-Chronicle-by-Murakami-Haruki/</id>
    <published>2021-11-27T14:29:05.000Z</published>
    <updated>2021-11-27T14:45:26.379Z</updated>
    
    <content type="html"><![CDATA[<h2 id="读后感">读后感</h2><blockquote><p>在<em>微信读书</em>上花了整整22个小时读了<a href="https://en.wikipedia.org/wiki/The_Wind-Up_Bird_Chronicle">《奇鸟行状录》</a>，感受不佳，读完的时候甚至感到愤懑，胡说一通。</p></blockquote><p>读完我竟然有生气的感觉。尤不喜欢这个隐喻大乱炖式结尾。早知道这本小说通篇暗喻，一件事也没直接地、好端端地讲明白，我是断不会选择阅读的。生气大概是对我这二十多个小时感到不值。</p><p>我从来不讨厌隐喻，过去也从村上的《海边的卡夫卡》《挪威的森林》之类长篇里收获了村上式魔幻现实的趣味，也因此是喜欢村上春树的。</p><p>但是，不一样的地方在于，本书是所谓「村上春树转型之作」，开始触及更严肃、更宏大的议题，我原本也是对此颇有期待的。可是用他特有的轻飘飘的方式来探讨爱情、人生我尚能接受，照样探讨暴力、战场，我只有不理解。（剥皮那一段写得倒是很沉重，也很好。）</p><p>我认为隐喻该是为了更清楚、更深刻的表达而存在的，而这本书里的大量隐喻根本无益于表达。本书最大的一个主题，即暴力会以各种形式对人造成损害，完全是有更直截了当的方式可以表达清楚的，我一直在期待更直白深入的探讨。可是整本书被细枝末节的、含义不明的隐喻占据了篇幅，以悬疑的情节、跌宕的故事走向来吸引人，我只觉得是挂羊头卖狗肉。</p><p>无语凝噎。</p><h2 id="读中感">读中感</h2><p>这段时间在读村上春树的《奇鸟行状录》。我感觉还行，但是有一点不喜欢的地方。当然是会读完的，都读了三分之二了；但是容我在此吐吐槽吧。不喜欢的地方就是那种神神叨叨，老是说梦境与现实有联系、无法区分啊，老是说主角有什么样什么样的命运，然后遇到一些世外奇人，身上发生一些象征意义的变化，然后命运就哦豁转向了。命运之前是什么方向，后来又是什么方向呢？主角确实有感觉，但又道不明，不可说。总结起来就是：神神叨叨。看别人的想法的时候，有一个人提到村上说话总是有一股刻意的淡定、简短感，他又让笔下所有人都这样说话，导致所有人都有一股村上感，说的都不大像人话，也不一定符合人物性格了。太贴切了！</p><p>我想这本书读罢，我大概会搁置村上春树的作品们了。大概是又长大些了吧，阅读口味总之是变化了。下一本肯定会读陀氏的。</p><hr /><p>或许是我举一反三能力有限…有很多寓言性质的东西，我并没有找出来与之对应的现实意义的道理，并没有完全理解村上想表达的含义。</p><p>妻子有暗处的无人察觉的一面，妻子在某种超现实的意义上受困；作为与妻相爱的丈夫，冈田需要从一开始失去妻子的迷茫和失落中走出来，坚持自己，把这当成使生活停滞的磨难，想办法救回妻子，让生活步入正轨，给自己所处的世界重新拧上发条……可是现实生活—至少是我的现实生活，不是这么运转的呀，没有这样的形式呀？</p><p>一个地道的人，做出的事总是不那么地道、很令人意外；一个人渐渐地就滑入了一个与常人的世界不同的、仿佛停滞了的世界。这是怎样的一种人生困境呢？</p><p>书中冈田无法接受妻子和情人跑了的事实，很努力地思考、寻找答案，然后通过一些超现实的隐喻和境遇，还真的发现了妻子遇到了只有自己才能挽救的困境这一事实。可是现实生活中，更多时候，发生这种不遂人意的事，更大概率那就是事实吧，把自己放在井底枯想，只会想疯了，不会想通了吧？</p><h2 id="傻里傻气的雨蛙女儿从笠原may身上唯一感怀到的">傻里傻气的雨蛙女儿——从笠原May身上唯一感怀到的</h2><blockquote><p>最近我暗暗觉得好笑：人们这样从早到晚忙得不亦乐乎有点怪。没这样想过？怎么说好呢，我在这里的工作，只不过按头头如此这般的吩咐如此这般地干罢了，丝毫用不着动脑，等于说脑浆那东西上工前放在寄存柜里下工时再随手拿回。一天七小时对着操作台一个劲儿往发罩上栽头发，然后在食堂吃饭进浴室洗澡，接下去当然就得像一般人那样睡觉。一天二十四小时可自由支配的时间实在少得可怜，而且“自由时间”也由于人困马乏而多用来打瞌睡或怔怔发呆，几乎谈不上用心想点什么。当然周末不用做工，却又要集中洗衣服搞卫生。有时还要上街，一忽儿就过去了。一次曾下决心写写日记，但简直没什么好写，只一周就扔一边去了。日复一日千篇一律嘛！ 尽管这样，尽管这样，对于自己如此成为工作的一部分我还是半点厌恶情绪都没有，别扭感什么的也没有。或者不如说由于这样蚂蚁式地一门心思劳动，我甚至觉得渐渐靠近了“本来的自己”。怎么说呢，说倒说不好，总之好像是由于不思考自己而反倒接近自己的核心。我所说的“有点怪”就是这个意思。</p></blockquote><p>唉，我还没有长大（或者说还没有和解）的一点是什么呢，就是无论自己做什么，都还在担心没有在做「正确」的事情，担心虚耗时间，担心一头栽进什么洞里似的。</p><blockquote><p>那两人居然相信世界是如同单元住宅那样始终一贯如此这般的，以为只要以始终一贯的方法干下去，一切终将水到渠成，所以他们也才为我的倒行逆施而困惑而伤心而气恼。</p></blockquote><p>其实笠原may就是加缪笔下的荒诞人，觉察了世界如同舞台布景，一旦坍塌背后什么也不是。这里她所谓雨蛙一般的父母，就是从来没有过从自己扮演的角色中抽离出来，从来没有停下来反思过的平凡人罢</p><h2 id="有关拧发条鸟的比喻又及为什么要翻译成奇鸟行状录呢">有关「拧发条鸟」的比喻；又及，为什么要翻译成「奇鸟行状录」呢？</h2><blockquote><p>&quot;拧发条鸟是实际存在的鸟。什么样我不知道，我也没亲眼见过，只听过叫声。拧发条鸟落在那边树枝上一点一点拧世界发条，‘吱吱吱吱’拧个不停。如果它不拧发条，世界就不动了。但这点谁也不晓得，世上所有的人都以为一座远为堂皇和复杂的巨大装置在稳稳驱动世界，其实不然，是拧发条鸟飞到各个地方，每到一处就一点点拧动小发条来驱动世界。发条很简单，和发条玩具上的差不多，只消拧发条即可，但那发条唯独拧发条鸟方能看到。&quot;</p></blockquote><p>这个比喻没有打动我。我眼中的世界比这个精巧多了。不过比喻本身还是很精彩的</p><blockquote><p>但是，无论是不是偶然的一致，在肉桂的故事中“拧发条鸟”这一存在都不可漠视。人们在它那只有特殊人方可听见的鸣声引导下走向无可回避的毁灭。在那里，一如兽医自始至终感觉的那样，所谓人的自由意志等等是无能为力的。他们像被上紧背部发条而置于桌面的偶人，只能从事别无选择余地的行为，只能朝别无选择余地的方向前进。处于听到鸟鸣范围内的人们，几乎人人都遭受剧烈磨损以至消失。大部分人死掉了，他们直接从桌边滚到了地下。</p></blockquote><p>这才理解到村上比喻的准确。「拧发条」是被动而不是主动，是别无选择、剧烈磨损、走向毁灭。</p>]]></content>
    
    
    <summary type="html">我认为隐喻该是为了更清楚、更深刻的表达而存在的，而这本书里的大量隐喻根本无益于表达。</summary>
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/categories/thoughts/"/>
    
    
    <category term="Literature" scheme="https://jyzhu.top/tags/Literature/"/>
    
  </entry>
  
  <entry>
    <title>Houses with open doors</title>
    <link href="https://jyzhu.top/Houses-with-open-doors/"/>
    <id>https://jyzhu.top/Houses-with-open-doors/</id>
    <published>2021-11-27T14:23:00.000Z</published>
    <updated>2021-11-27T14:23:41.552Z</updated>
    
    <content type="html"><![CDATA[<p>互联网冲浪对于我来说最浪漫的事情之一，就是顺着一篇搜来的技术文档，随手点开博客主页时，发现博客中记满了朴素但真挚的生活感想。有哲思，有小故事，有少年得意的成就记叙，也有人到中年的感怀伤时……每个人的博客都是独一无二的，网站名字和独特装饰极力彰显着个性；但所有博客又有一个共同的特点，就是毫无保留的文字和期待被造访的心情。这就好像在漠然的大城市里，却有一些不起眼的小屋子，真诚地敞着所有的门窗，等待像我这样的，可能会来，也可能不会来的旅人。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;互联网冲浪对于我来说最浪漫的事情之一，就是顺着一篇搜来的技术文档，随手点开博客主页时，发现博客中记满了朴素但真挚的生活感想。有哲思，有小故事，有少年得意的成就记叙，也有人到中年的感怀伤时……每个人的博客都是独一无二的，网站名字和独特装饰极力彰显着个性；但所有博客又有一个共同</summary>
      
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/categories/thoughts/"/>
    
    
  </entry>
  
</feed>
