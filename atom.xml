<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tianke Youke</title>
  
  <subtitle>A Base for Secreting and Running at Night</subtitle>
  <link href="https://jyzhu.top/atom.xml" rel="self"/>
  
  <link href="https://jyzhu.top/"/>
  <updated>2023-01-26T10:54:39.666Z</updated>
  <id>https://jyzhu.top/</id>
  
  <author>
    <name>Jiayin Zhu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>齐次坐标系</title>
    <link href="https://jyzhu.top/homogeneous-coordinates/"/>
    <id>https://jyzhu.top/homogeneous-coordinates/</id>
    <published>2023-01-26T10:49:19.000Z</published>
    <updated>2023-01-26T10:54:39.666Z</updated>
    
    <content type="html"><![CDATA[<h1 id="齐次坐标系">齐次坐标系</h1><p>之前不理解为什么要用一个和从小到大学的笛卡尔坐标系不同的齐次坐标系来表示东西，并且弄得很复杂；学了各种公式也很糊涂。现在终于明白了</p><h2 id="齐次坐标系的现实意义">齐次坐标系的现实意义</h2><p>就是用来表示现实世界中我们眼睛看到的样子：两条平行线在无限远处能相交。 <embed src="https://pic4.zhimg.com/80/v2-dc3c0223dd24d2084fcda13b2d0e60f3_1440w.webp" /></p><h2 id="齐次坐标系的本质">齐次坐标系的本质：</h2><p>就是用N+1维来代表N维坐标。</p><p>也就是说，原本二维空间的点<span class="math inline">\((X,Y)\)</span>，增加一个维度，用<span class="math inline">\((x,y,w)\)</span>来表示。把齐次坐标转换成笛卡尔坐标是很简单的，对前两个维度分别除以最后一个维度的值，就好了，即 <span class="math display">\[X=\frac x w,\  Y=\frac y w\\ (X,Y)=(\frac x w,\frac y w)\]</span> <embed src="https://pic2.zhimg.com/80/v2-da28eed57fda0fc6a06b7122be5f2a1d_1440w.webp" /></p><p>这样做就可以表示两条平行线在远处能相交了！why？</p><p>要解释这个，需要先解释一个齐次坐标系的特点：规模不变性（也是叫homogeneous这个名字的原因）。也就是说，对任意非零的k，<span class="math inline">\((x,y,w)\)</span>和<span class="math inline">\((kx,ky,kw)\)</span>都表示二维空间中同一个点<span class="math inline">\((\frac x w,\frac y w)\)</span>。（因为<span class="math inline">\(\frac{kx}{kw}=\frac xw\)</span>嘛。）</p><p>首先，用原本笛卡尔坐标系中的表示方法，无限远处的点会被表示成<span class="math inline">\((\infin,\infin)\)</span>，从而失去意义。但是我们发现用齐次坐标，我们就有了一个方法明确表示无限远处的任意点，即，<span class="math inline">\((x,y,0)\)</span>。（为什么？因为把它转换回笛卡尔坐标，会得到<span class="math inline">\((\frac x 0,\frac y 0)=(\infin,\infin)\)</span>）。</p><p>现在，用初中所学，联立两条直线的方程，得到的解是两条直线的交点。假如有两条平行线<span class="math inline">\(Ax+By+C=0\)</span>和<span class="math inline">\(Ax+By+D=0\)</span>，求交点，则 <span class="math display">\[\left\{\matrix{Ax+By+C=0 \\Ax+By+D=0}\right.\]</span> 在笛卡尔坐标系中，可知唯一解是<span class="math inline">\(C=D\)</span>，即两条线为同一条直线。</p><p>但是，如果把它换成齐次坐标，得到 <span class="math display">\[\left\{\matrix{A\frac x w+B\frac y w+C=0\\ A\frac x w + B\frac y w +D=0}\right.\]</span></p><p><span class="math display">\[\left\{\matrix{Ax+By+Cw=0\\Ax+By+Dw=0}\right.\]</span></p><p>当<span class="math inline">\(w=0\)</span>，上式变成<span class="math inline">\(Ax+By=0\)</span>，得到解<span class="math inline">\((x,-\frac {A}Bx,0)\)</span>。其实这里的x和y是什么不重要，重要的是w=0，意味着这是个无限远处的点。也就是说，两条平行线在无限远处相交了！甚至能明确求出交点！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;齐次坐标系&quot;&gt;齐次坐标系&lt;/h1&gt;
&lt;p&gt;之前不理解为什么要用一个和从小到大学的笛卡尔坐标系不同的齐次坐标系来表示东西，并且弄得很复杂；学了各种公式也很糊涂。现在终于明白了&lt;/p&gt;
&lt;h2 id=&quot;齐次坐标系的现实意义&quot;&gt;齐次坐标系的现实意义&lt;/h2&gt;
&lt;p&gt;就是</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/tags/3D-Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to NeRF</title>
    <link href="https://jyzhu.top/Introduction-to-NeRF/"/>
    <id>https://jyzhu.top/Introduction-to-NeRF/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:21.514Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h1 id="introduction-to-nerf">1. Introduction to NeRF</h1><h2 id="what-is-nerf">What is NeRF</h2><blockquote><p>Reference: Original NeRF paper; an online ariticle</p></blockquote><p>在已知视角下对场景进行一系列的捕获 (包括拍摄到的图像，以及每张图像对应的内外参)，合成新视角下的图像。</p><p>NeRF 想做这样一件事，不需要中间三维重建的过程，仅根据位姿内参和图像，直接合成新视角下的图像。为此 NeRF 引入了辐射场的概念，这在图形学中是非常重要的概念，在此我们给出渲染方程的定义：</p><p><embed src="https://pic1.zhimg.com/80/v2-1a80de23a422688b739f36828affb8ec_1440w.webp" /></p><p><embed src="https://pic4.zhimg.com/80/v2-c469e4968a3e6cf8ec7a81f816de4f87_1440w.webp" /></p><p>那么辐射和颜色是什么关系呢？简单讲就是，光就是电磁辐射，或者说是振荡的电磁场，光又有波长和频率，<span class="math inline">\(波长\times 频率=光速\)</span>，光的颜色是由频率决定的，大多数光是不可见的，人眼可见的光谱称为可见光谱，对应的频率就是我们认为的颜色：</p><p><embed src="https://pic1.zhimg.com/80/v2-381aa740f21b7eba1f896fd98dcc1308_1440w.webp" /></p><p><embed src="https://pic1.zhimg.com/80/v2-51bd3710b9f891c4c44fde12545e4fd4_1440w.webp" /></p><h3 id="sdf---signed-distance-function">SDF - Signed Distance Function</h3><p>SDF是一种计算图形学中定义距离的函数。SDF定义了空间中的点到隐式曲面的距离，该点在曲面内外决定了其SDF的正负性。</p><p>相较于其他像点云（point cloud）、体素（voxel）、面云（mesh）那样的经典3D模型表示方法，SDF有固定的数学方程，更关注物体的表面信息，具有可控的计算成本。</p><h2 id="features-of-nerf">Features of NeRF</h2><ul><li>Representation can be discrete or continuous. but the discrete representation will be a big one if you have more dimensions, e.g., 3 dim.<ul><li>Actually the Plenoxels try to use 3D grids to store the fields. Fast, however, too much memory.</li></ul></li><li>Neural Field has advantages:<ol type="1"><li>Compactness 紧致:</li><li>Regularization: nn itself as inductive bias makes it easy to learn</li><li>Domain Agonostic: cheap to add a dimension</li></ol></li><li>also problems<ul><li>Editability / Manipulability</li><li>Computational Complexity</li><li>Spectral Bias</li></ul></li></ul><h2 id="problem-formulation">Problem Formulation</h2><ul><li>Input: multiview images</li><li>Output: 3D Geometry and appearance</li><li>Objective:</li></ul><p><span class="math display">\[\arg \min_x\|y-F(x)\|+\lambda P(x)\]</span></p><p>y is multiview images, F is forward mapping, x is the desired 3D reconstruction.</p><p>F can be differentiable, then you can supervise this.</p><ul><li>nn本身就是某种constraints，你就不需要加太多handicraft constraints</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&quot;introduction-to-nerf&quot;&gt;1. Introduc</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Learning NeRF</title>
    <link href="https://jyzhu.top/Learning-NeRF/"/>
    <id>https://jyzhu.top/Learning-NeRF/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:37.635Z</updated>
    
    <content type="html"><![CDATA[<h1 id="learning-nerf">Learning NeRF</h1><p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="reading-list">Reading List</h2><h3 id="classical">Classical</h3><ul><li><p>Mildenhall <em>et al.</em> introduced NeRF at ECCV 2020 in the now seminal <a href="https://www.matthewtancik.com/nerf">Neural Radiance Field paper</a>.</p><p>This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images.</p></li><li><p><a href="https://m-niemeyer.github.io/project-pages/giraffe/index.html">GIRAFFE</a>: Compositional Generative Neural Feature Fields</p></li></ul><h3 id="survey">Survey</h3><ul><li><a href="https://arxiv.org/abs/2004.03805">Apr 2020 - State of the Art on Neural Rendering</a></li></ul><h3 id="cvpr">2021CVPR</h3><p>2021年CVPR还有许多相关的精彩工作发表。例如，提升网络的泛化性：</p><ul><li><a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>：将每个像素的特征向量而非像素本身作为输入，允许网络在不同场景的多视图图像上进行训练，学习场景先验，然后测试时直接接收一个或几个视图为输入合成新视图。</li><li><a href="https://ibrnet.github.io/">IBRNet</a>：学习一个适用于多种场景的通用视图插值函数，从而不用为每个新的场景都新学习一个模型才能渲染；且网络结构上用了另一个时髦的东西 Transformer。</li><li><a href="https://apchenstu.github.io/mvsnerf/">MVSNeRF</a>：训练一个具有泛化性能的先验网络，在推理的时候只用3张输入图片就重建一个新的场景。</li></ul><p>针对动态场景的NeRF:</p><ul><li><a href="https://nerfies.github.io/">Nerfies</a>：多使用了一个多层感知机来拟合形变的SE(3) field，从而建模帧间场景形变。Nerfies: Deformable Neural Radiance Fields</li><li><a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a>：多使用了一个多层感知机来拟合场景形变的displacement。</li><li><a href="https://link.zhihu.com/?target=http%3A//www.cs.cornell.edu/~zl548/NSFF/">Neural Scene Flow Fields</a>：多提出了一个scene flow fields来描述时序的场景形变。</li></ul><p>其他创新点：</p><ul><li><a href="https://kai-46.github.io/PhySG-website/">PhySG</a>：用球状高斯函数模拟BRDF（高级着色的上古神器）和环境光照，针对更复杂的光照环境，能处理非朗伯表面的反射。</li><li><a href="https://nex-mpi.github.io/">NeX</a>：用MPI（Multi-Plane Image ）代替NeRF的RGBσ作为网络的输出。</li></ul><h3 id="cvpr-1">2022 CVPR</h3><p><a href="https://ajayj.com/dreamfields">Zero-Shot Text-Guided Object Generation with <strong>Dream Fields</strong></a></p><h2 id="useful-references"><strong>Useful References:</strong></h2><blockquote><p><a href="https://markboss.me/post/nerf_at_eccv22/?continueFlag=55ed0f6189bcd6ca987e08764bcbe945">NeRF at ECCV22 - Mark Boss</a></p><p><a href="https://markboss.me/post/nerf_at_neurips22/">NeRF at NeurIPS 2022 - Mark Boss</a></p><p><a href="https://dellaert.github.io/NeRF22/">NeRF at CVPR 2022 - Frank Dellaert</a></p><p><a href="https://youtu.be/PeRRp1cFuH4">CVPR 2022 Tutorial on Neural Fields in Computer Vision</a></p></blockquote><p>Bigger to learn:</p><ul><li>[ ] Above NeRF: neural rendering</li><li>[ ] Related theories in graphics and computer vision</li><li>[ ] NeRF的一作Ben Mildenhall在SIGGRAPH 2021 Course <a href="https://www.youtube.com/watch%3Fv%3Dotly9jcZ0Jg">Advances in Neural Rendering</a>中从概率的角度推导了NeRF的体渲染公式。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;learning-nerf&quot;&gt;Learning NeRF&lt;/h1&gt;
&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Manipulate Neural Fields</title>
    <link href="https://jyzhu.top/Manipulate-Neural-Fields/"/>
    <id>https://jyzhu.top/Manipulate-Neural-Fields/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:11:38.197Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="manipulate-neural-fields">2.5. Manipulate Neural Fields</h2><p>Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.</p><figure><img src="https://s2.loli.net/2023/01/12/okLGyeFmMvifhZW.png" alt="image-20221212211525928" /><figcaption>image-20221212211525928</figcaption></figure><p>You can either edit the input coordinates, or edit the parameters <span class="math inline">\(\theta\)</span>.</p><p>On the other axis, you can edit through an explicit geometry, or an implicit neural fields.</p><figure><img src="https://s2.loli.net/2023/01/12/S7HWcQPh1FtdJaw.png" alt="image-20221212213802209" /><figcaption>image-20221212213802209</figcaption></figure><p>The following examples 落在不同的象限。</p><h3 id="editing-the-input-via-explicit-geometry-left-up">Editing the input via Explicit geometry (left-up)</h3><ul><li><p>You can represent each object using a separated neural field (local frame), and then compose them together in different ways.</p></li><li><p>If you want to manipulate not only spatially, but also <strong>temporaly</strong>, it is also possible. You can add a time coordinate as the input of the neural field network, and transform the time input.</p></li><li><p>You can also manipulate (especially human body) via <strong>skeleton</strong>.</p><figure><img src="https://s2.loli.net/2023/01/12/y4bGulHpfOwWkqN.png" alt="image-20221212212838893" /><figcaption>image-20221212212838893</figcaption></figure><ul><li><p><strong>Beyond human</strong>, we can also first estimate different moving parts of an object, to form some skeleton structure, and then do the same.</p><figure><img src="https://s2.loli.net/2023/01/12/SBzGy3rnUaqLFI8.png" alt="Noguchi etal, CVPR22" /><figcaption>Noguchi etal, CVPR22</figcaption></figure></li></ul></li><li><p>Beyond rigid, we can also manipulate via <strong>mesh</strong>. coz we have plenty of manipulation tools on mesh. The deformation on mesh can be re-mapped as the deformation on the input coordinate</p><figure><img src="https://s2.loli.net/2023/01/12/UbFu74iCQ15mK3B.png" alt="image-20221212213601773" /><figcaption>image-20221212213601773</figcaption></figure></li></ul><h3 id="editing-the-input-via-neural-flow-fields-left-down">Editing the input via Neural Flow Fields (left-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/zxFElDIuSnPioJ7.png" alt="image-20230104183222294" /><figcaption>image-20230104183222294</figcaption></figure><p>We use the <span class="math inline">\(f_{i\rightarrow j}\)</span> to edit the <span class="math inline">\(r_{i\rightarrow j}\)</span> to represent one ray into another one.</p><p>We need to define the consistency here, so that the network can learn through forward and backward:</p><figure><img src="https://s2.loli.net/2023/01/12/VS1K3rQxHXYPRIg.png" alt="image-20230104183453487" /><figcaption>image-20230104183453487</figcaption></figure><h3 id="editing-network-parameters-via-explicit-geometry-right-up">Editing network parameters via Explicit geometry (right-up)</h3><p>The knowledge is already in the network. So instead of editing the inputs, we can directly edit the network parameters for generating new things.</p><figure><img src="https://s2.loli.net/2023/01/12/w2XEyYn5qbh41OB.png" alt="image-20230104185014312" /><figcaption>image-20230104185014312</figcaption></figure><ul><li>This proposed solution makes use of an encoder. The encoder learns to represent the rotated input as a high-dimensional latent code Z, with the same rotation R, in 3-dim space. The the following network use the latent code to generate the <span class="math inline">\(f_\theta\)</span></li></ul><figure><img src="https://s2.loli.net/2023/01/12/ruER6MJPpljSZiX.png" alt="image-20230104185544623" /><figcaption>image-20230104185544623</figcaption></figure><ul><li>In this work, the key idea is to map the high-resolutional object and the similar but lower resolutional object into the same latent space. Then, you can easily manipulate the lower resolutional object, and it should also affect the higher resolutional one. Then, the shared latent space are put into the following neural field network, which outputs high resolutional results.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/HL61itcqsIaEThX.png" alt="image-20230104202425695" /><figcaption>image-20230104202425695</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/y7eCcKDmdUY4VOu.png" alt="image-20230104202625346" /><figcaption>image-20230104202625346</figcaption></figure><ul><li>This work (Yang et al. NeurlPS'21) about shape editing is &quot;super important&quot; but the speaker does not have enough time... Basically it shows that the tools that we use to manipulate a mesh can also be used on a neural field, where we can keep some of the network parameters to make sure the basic shape of the object the same, and then the magical thing is the &quot;curvature manipulation&quot; item. Given the neural field is differentiable, this can be achieved.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/2lTvenQixfRm8Po.png" alt="image-20230104203311551" /><figcaption>image-20230104203311551</figcaption></figure><ul><li>Obeying the points (a.k.a generalization). It makes sure the manipulation done on the input points are reconstructed.</li></ul><h3 id="editing-network-parameters-via-neural-fields-right-down">Editing network parameters via Neural Fields (right-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/5Ohb7ExW4osc1n2.png" alt="image-20230104204330741" /><figcaption>image-20230104204330741</figcaption></figure><ul><li>This work constructs a reasonable latent space of the object, then do interpolation of different objects.</li><li>Beyond geometry, we can also manipulate <strong>color</strong></li></ul><figure><img src="https://s2.loli.net/2023/01/12/vM1QwkT4BJqGNIR.png" alt="image-20230104204738067" /><figcaption>image-20230104204738067</figcaption></figure><p>It decomposes the network into shape and color networks, and we can edit each independently.</p><figure><img src="https://s2.loli.net/2023/01/12/38HNsE9GnF1pydQ.png" alt="image-20230104204937204" /><figcaption>image-20230104204937204</figcaption></figure><ul><li>This is the stylization work. It mainly depends on a different loss function, which does not search for the exact feature of the vgg, but somehow the nearest neighbor.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;manipulate-neural-fields&quot;&gt;2.5. Ma</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Differentiable Forward Maps</title>
    <link href="https://jyzhu.top/NeRF-Differentiable-Forward-Maps/"/>
    <id>https://jyzhu.top/NeRF-Differentiable-Forward-Maps/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:34.505Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="differentiable-forward-maps">2.3. Differentiable Forward Maps</h2><figure><img src="https://s2.loli.net/2023/01/12/xp4avlLcJDI9R2k.png" alt="image-20221208175453557" /><figcaption>image-20221208175453557</figcaption></figure><h3 id="differentiable-rendering">Differentiable rendering</h3><figure><img src="https://s2.loli.net/2023/01/12/1Ng8wz2KP4oTiVH.png" alt="image-20221208181457315" /><figcaption>image-20221208181457315</figcaption></figure><p>Volume rendering can render fogs. Sphere rendering only render the solid surface, and needs ground truth supervision.? Neural renderer combines the two.</p><h3 id="differentiability-of-the-rendering-function-itself">Differentiability of the rendering function itself</h3><ul><li>BRDF Shading? details later.</li></ul><h3 id="differentiation-itself">Differentiation itself</h3><p>Design a neural network with higher order derivatives constraints and therefore directly use its derivative.</p><figure><img src="https://s2.loli.net/2023/01/12/Gi6IaAkhvlBxpoe.png" alt="image-20221208182302568" /><figcaption>image-20221208182302568</figcaption></figure><p>For example the Eikonal equation forces the neural network has a derivative as 1. Adding the eikonal loss then promises the neural network valid.</p><p>Generally, this kind of problems are: the solutions are constrained by its partial derivatives.</p><h3 id="special-identity-operator">Special: Identity Operator</h3><p><span class="math display">\[\text{Reconstruction} \rightarrow \hat 1()\rightarrow \text{Sensor domain}\\\text{Reconstruction} == \text{Sensor domain}\]</span></p><p>Q&amp;A:</p><ul><li>Can we obtain a neural network in just one forward, without optimization?</li><li>Can we design special forward maps for specific downstream tasks, eg., classification? Absolutely yes. We can design it to represent a compact representation as the sensor domain. The key idea is to get a differentiable function to map your specific recon and sensor domain.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;differentiable-forward-maps&quot;&gt;2.3.</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Hybrid representations</title>
    <link href="https://jyzhu.top/NeRF-Hybrid-representations/"/>
    <id>https://jyzhu.top/NeRF-Hybrid-representations/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:55.420Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="hybrid-representations">2.2. Hybrid representations</h2><h3 id="tradeoffs-of-choosing-a-proper-representation">Tradeoffs of choosing a proper representation</h3><figure><img src="https://s2.loli.net/2023/01/12/NyvS91xlLJA8KWR.png" alt="image-20221208172055153" /><figcaption>image-20221208172055153</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/sDg8FHQcjrGRW1b.png" alt="image-20221208172209556" /><figcaption>image-20221208172209556</figcaption></figure><p>You may choose one proper representation depending on your own application</p><h3 id="grid">1. Grid</h3><figure><img src="https://s2.loli.net/2023/01/12/ZpYEMbXvRdeOqiy.png" alt="image-20221205195659841" /><figcaption>image-20221205195659841</figcaption></figure><p>Input is too huge. Then you need too huge neural network. So, this grid interpolation acts like a &quot;position encoding&quot;, which encodes the low dimensional features into high dims.</p><figure><img src="https://s2.loli.net/2023/01/12/lMi3tK9NPgcWUaI.png" alt="image-20221208162026398" /><figcaption>image-20221208162026398</figcaption></figure><p>NeRFusion CVPR22: online!</p><h3 id="point-cloud">2. point cloud</h3><figure><img src="https://s2.loli.net/2023/01/12/zhYHQnFsgxBLCfM.png" alt="image-20221208162541770" /><figcaption>image-20221208162541770</figcaption></figure><p>Cons:</p><ol type="1"><li>To access local points, you need to specifically design the data structure. Otherwise, it is O(n)!</li><li>Choose different kernels to retrieve nearby points' features. Oftentimes you assume it is local kernel.</li></ol><p><img src="https://s2.loli.net/2023/01/12/37EbFsANOCc9voX.png" alt="image-20221208163050867" style="zoom:50%;" /></p><h3 id="mesh">3. Mesh</h3><p>Unstructed grids. Compared with point clouds, meshes have connectivity info.</p><figure><img src="https://s2.loli.net/2023/01/12/Dwir9hsm3VgZjQk.png" alt="image-20221208163526289" /><figcaption>image-20221208163526289</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/nPMw7T3hqRAU2iv.png" alt="image-20221208163746237" /><figcaption>image-20221208163746237</figcaption></figure><h3 id="multiplanar-images">4. Multiplanar Images</h3><p>Something like project a 3D grid into an axis to get levels of planes.</p><figure><img src="https://s2.loli.net/2023/01/12/CmhFDT5NoiMAOnv.png" alt="image-20221208164038729" /><figcaption>image-20221208164038729</figcaption></figure><p>Pros:</p><ol type="1"><li>Compact</li><li>Very efficient because the hardware and software designs are accelerated to these 2D operations, like bi-linear operations.</li></ol><p>Cons:</p><ol type="1"><li>Resolution bias on plane axis: coz it is discrete betweens planes.</li></ol><p>This is not very wise in my opinion. It is just a temporary tradeoff given nowadays' technologies. Coz everything will be 3D in the future.</p><p><img src="https://s2.loli.net/2023/01/12/UDO6HlWAp3y7qF1.png" alt="image-20221208165534056" />Generate 2D images from different camera views (perhaps). Key point is the tri-plane representation of 3D features.</p><h3 id="multiresolution-grids">5. Multiresolution grids</h3><figure><img src="https://s2.loli.net/2023/01/12/TlmkK1NDj2dAtUp.png" alt="image-20221208165714329" /><figcaption>image-20221208165714329</figcaption></figure><p>Pros:</p><ol start="2" type="1"><li>Stable coz you indeed need both low and high resolution info</li></ol><h3 id="hash-grids">6. Hash grids</h3><p><img src="https://s2.loli.net/2023/01/12/NZtH7wfxpSPVkGe.png" alt="image-20221208170131069" /> <span class="math display">\[[x,y,z]\text{ coordinates}\rightarrow \text{Hash function()} \rightarrow \text{Fixed size codebook}\]</span> Pros:</p><ol type="1"><li>No matter how big is the original data, you can use a fixed size codebook as the input feature.</li><li>Can be online!</li></ol><p>Cons:</p><ol type="1"><li>May still need large codebooks</li><li>Features not spatially local. I don't think the hash grid is a good idea if this drawback exists. But isn't there a simple way to generate features with local info remaining?</li></ol><h3 id="codebook-grids">7. Codebook grids</h3><figure><img src="https://s2.loli.net/2023/01/12/pnBJGxM6jNHD2v5.png" alt="image-20221208170955887" /><figcaption>image-20221208170955887</figcaption></figure><p>Instead of storing features of points in grids, store a (index to a) code in a codebook. The size of the codebook is fixed, so the overall size can be controlled as much smaller.</p><p>cons:</p><ol type="1"><li>To make the indexing operation differentiable, the computing complexity rises here.</li><li>Using hash is to get rid of the complex data structure, but the indices bring it back.</li></ol><h3 id="bounding-volume-hierarchies">8. Bounding Volume Hierarchies</h3><figure><img src="https://s2.loli.net/2023/01/12/LQ6fzh21OltTJxq.png" alt="image-20221208171806113" /><figcaption>image-20221208171806113</figcaption></figure><p>Commonly used method in computer graphics</p><h3 id="others-voxel">9. Others (voxel)</h3><figure><img src="https://s2.loli.net/2023/01/12/5sE1MYpOLB4mPCF.png" alt="image-20221208173124734" /><figcaption>image-20221208173124734</figcaption></figure><ul><li>For dynamic nerfs, is there any better hybrid representation? Sure.</li><li>Is there any explicit bias of these hybird representations that we can discover and then design regularization? Sure.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;hybrid-representations&quot;&gt;2.2. Hybr</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Network Architecture</title>
    <link href="https://jyzhu.top/NeRF-Network-Architecture/"/>
    <id>https://jyzhu.top/NeRF-Network-Architecture/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:07.993Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="network-architecture">2.1. Network Architecture</h2><h3 id="input-encoding">1. Input Encoding</h3><p>Similar as NLP, they use position encodings. Like Sinusoid functions. I also remember an encoding method which takes into consider of the 光线的散射</p><h3 id="activation-functions">2. Activation functions</h3><p>ReLU is not perfect for this task. Because it 不能解决对高阶导有constraints的函数。</p><p>SIREN is a replacement.</p><h3 id="symmetry-invariance-equivariance">3. Symmetry, Invariance &amp; Equivariance</h3><figure><img src="https://s2.loli.net/2023/01/12/tynBhWCulMINrU3.png" alt="image-20221205193423558" /><figcaption>image-20221205193423558</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/Rjx5h2kfY34JZct.png" alt="image-20221205193614341" /><figcaption>image-20221205193614341</figcaption></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;network-architecture&quot;&gt;2.1. Networ</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Notebook</title>
    <link href="https://jyzhu.top/NeRF-Notebook/"/>
    <id>https://jyzhu.top/NeRF-Notebook/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:36:59.994Z</updated>
    
    <content type="html"><![CDATA[<p>I am currently burying myself into the sea of NeRF. I plan to archive my learning notes here. I am still a beginner, so the notes absolutely contain errors, and are not finished yet.</p><h1 id="contents">Contents</h1><p><a href="https://jyzhu.top/Learning-NeRF/">Learning NeRF</a>: Reading list, learning references, and plans</p><p><strong>Notes of CVPR22' Tutorial:</strong></p><p><a href="https://jyzhu.top/Introduction-to-NeRF/">1. Introduction to NeRF</a>: What is NeRF and its features</p><p>2. Techniques</p><p>​ <a href="https://jyzhu.top/NeRF-Network-Architecture/">2.1. Network Architecture</a></p><p>​ <a href="https://jyzhu.top/NeRF-Hybrid-representations/">2.2. Hybrid representations</a></p><p>​ <a href="https://jyzhu.top/NeRF-Differentiable-Forward-Maps/">2.3. Differentiable Forward Maps</a></p><p>​ <a href="https://jyzhu.top/Prior-based-reconstruction-of-neural-fields/">2.4. Prior-based reconstruction of neural fields</a></p><p>​ <a href="https://jyzhu.top/Manipulate-Neural-Fields/">2.5. Manipulate Neural Fields</a></p><p>3. Applications</p><p><em>TBC</em></p><p><strong>Notes of paper reading</strong>:</p><p><a href="https://jyzhu.top/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/">Reading NeuMan: Neural Human Radiance Field from a Single Video</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;I am currently burying myself into the sea of NeRF. I plan to archive my learning notes here. I am still a beginner, so the notes absolut</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Prior based reconstruction of neural fields</title>
    <link href="https://jyzhu.top/Prior-based-reconstruction-of-neural-fields/"/>
    <id>https://jyzhu.top/Prior-based-reconstruction-of-neural-fields/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:12.867Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="prior-based-reconstruction-of-neural-fields">2.4. Prior-based reconstruction of neural fields</h2><p>Sounds like a one-shot task: instead of fitting and optimizing a neural field each for one scene; let's learn a prior distribution of neural field. Then, given a specific scene, it adjusts the neural field in just one forward.</p><figure><img src="https://s2.loli.net/2023/01/12/rMOwKRJBuedFm1n.png" alt="image-20221211234430727" /><figcaption>image-20221211234430727</figcaption></figure><h3 id="how-does-the-latent-code-look-like">How does the latent code look like?</h3><figure><img src="https://s2.loli.net/2023/01/12/A41EOlYCBXrUxto.png" alt="image-20221211234923290" /><figcaption>image-20221211234923290</figcaption></figure><ul><li>Global: not local. A small latent code represents a neural field<ul><li>main limitation: can only represent very simple (single) object. coz if you have multiple objects in a scene, the degree of freedom grows non-linearly.</li><li><strong>How about giving the natural language descriptions as conditions???</strong></li></ul></li><li>Local: you get different latent codes considering the locality where you are. So, you have a prior 3D data structure to store the latent codes.<ul><li>3D point clouds -&gt; grids -&gt; triplanes interpolation</li></ul></li></ul><blockquote><p>Convolutional Occupancy Networks</p></blockquote><h3 id="autodecoder-instead-of-encoder-decoder">Autodecoder instead of Encoder-decoder</h3><figure><img src="https://s2.loli.net/2023/01/12/zr2GnpBXeZDJUqa.png" alt="image-20221212005012783" /><figcaption>image-20221212005012783</figcaption></figure><ul><li><p>Encoder is a 2D CNN structure.</p></li><li><p>But while using autodecoder, the backpropogate through the forward map (i.e., the neural renderer) will give the 3D structural information to the latent codes directly. <span class="math display">\[\text{latent code }\hat z=\arg \min_z \|\text{Render(}\Phi)-g.t.\|\]</span> <img src="https://s2.loli.net/2023/01/12/FQjMgibmNtIP4ca.png" alt="image-20221212004946040" /></p></li></ul><p><strong>Instead of trying to build the encoder, sometimes just use the backpropogation through the forward map is helpful.</strong></p><h3 id="light-field-networks----dont-need-to-render-anymore">Light field networks -- Don't need to render anymore</h3><figure><img src="https://s2.loli.net/2023/01/12/FVP15fmBYz8rEoe.png" alt="image-20221212005908926" /><figcaption>image-20221212005908926</figcaption></figure><p>Instead of learning a NeRF that you use a neural renderer to generate all points along a ray; you can learn a network to directly give you a color along a ray. So you do not use a 3d coordinate as the query, instead, use a ray.</p><p>But this do not work in complicated task yet.</p><figure><img src="https://s2.loli.net/2023/01/12/mAFx6pzqaIuUMin.png" alt="image-20221212010316991" /><figcaption>image-20221212010316991</figcaption></figure><h3 id="outlook">Outlook</h3><ul><li>You don't need to use 600 images of a scene to reconstruct it. Synthesis images?</li><li>Open minds: other ways to skip the expensive forward map? (e.g., the light field)</li><li>Understanding the scene like humans do: disentangle different objects</li><li>Local conditioning methods? Regular grids are easy to tackle with, but it's harder for point clouds / factorized representations</li><li>Transformers: seems like local conditioning</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;prior-based-reconstruction-of-neu</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>The difference between RNN&#39;s output and h_n</title>
    <link href="https://jyzhu.top/The-difference-between-RNN-s-output-and-h-n/"/>
    <id>https://jyzhu.top/The-difference-between-RNN-s-output-and-h-n/</id>
    <published>2022-10-22T08:56:44.000Z</published>
    <updated>2022-10-22T09:12:01.123Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Reference: <a href="https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm" class="uri">https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm</a></p></blockquote><p>I was so confused when doing a homework on implementing the Luong Attention, because it tells that the decoder is a RNN, which takes <span class="math inline">\(y_{t-1}\)</span> and <span class="math inline">\(s_{t-1}\)</span> as input, and outputs <span class="math inline">\(s_t\)</span>, i.e., <span class="math inline">\(s_t = RNN(y_{t-1}, s_{t-1})\)</span>.</p><p>But the pytorch implementation of RNN is: <span class="math inline">\(outputs, hidden\_last = RNN(inputs, hidden\_init)\)</span>, which takes in a sequence of elements, computes in serials, and outputs a sequence also.</p><p>I was confused about what is the <span class="math inline">\(s_t\)</span>. Is it the <span class="math inline">\(outputs\)</span>, or the <span class="math inline">\(hidden\_states\)</span>?</p><p>This is the very helpful picture:</p><p><img src="https://i.stack.imgur.com/SjnTl.png" /></p><p>The <span class="math inline">\(output\)</span> here is the <span class="math inline">\(hidden\_states\)</span> of the last layer among all elements in the sequence (time steps), while the <span class="math inline">\(h_n,c_n = hidden\_last\)</span> is the <span class="math inline">\(hidden\_states\)</span> of the last time step among all layers.</p><p>The former is the <span class="math inline">\(H\)</span>, hidden state collection, which can be used in subsequent calculations, like attentions or scores; and the latter is the hidden state that can be directly used in the next iteration.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a href=&quot;https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-l</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Deep Learning" scheme="https://jyzhu.top/tags/Deep-Learning/"/>
    
    <category term="Neural Network" scheme="https://jyzhu.top/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Reading 3D Photography using Context-aware Layered Depth Inpainting</title>
    <link href="https://jyzhu.top/Reading-3D-Photography-using-Context-aware-Layered-Depth-Inpainting/"/>
    <id>https://jyzhu.top/Reading-3D-Photography-using-Context-aware-Layered-Depth-Inpainting/</id>
    <published>2022-10-07T00:35:43.000Z</published>
    <updated>2022-10-22T09:15:26.167Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://shihmengli.github.io/3D-Photo-Inpainting" class="uri">https://shihmengli.github.io/3D-Photo-Inpainting</a></p><p>作者：<a href="https://shihmengli.github.io/">Meng-Li Shih</a>, <a href="https://lemonatsu.github.io/">Shih-Yang Su</a>, <a href="https://johanneskopf.de/">Johannes Kopf</a>, <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a></p><p>发表： CVPR2020</p><p>链接： <a href="https://github.com/vt-vl-lab/3d-photo-inpainting" class="uri">https://github.com/vt-vl-lab/3d-photo-inpainting</a></p><hr /><h2 id="why">Why：</h2><p>之前的照片3D化的方法，会在视角变动后出现的新像素区域 填充很模糊的背景；这个方法主要是用inpainting的方法提高新背景生成的效果</p><figure><img src="https://s2.loli.net/2022/10/22/dfjuy9vNrBak1oP.png" alt="image-20221007084129382" /><figcaption>image-20221007084129382</figcaption></figure><h2 id="what">What：</h2><ol type="1"><li>任务是3D photography，图像3D化，把一张2D+深度信息的RGB-D图像转化成3D风格的图像。</li><li>现在的多镜头智能手机拍的照片都能提供深度信息。没有的话，也能用其他模型预测深度。</li><li>用分层深度图像（Layered Depth Image）来表示图像：能显式地表示像素点之间的连通性。和普通的2D图像相比，可以把像素点分成多层来表示，同一个坐标处可以有重合的不同层次的像素点。</li><li>提出一个基于学习的 inpainting 方法填充重叠区域的像素，让3D图像视角变化的时候出现的新背景效果很好。</li></ol><h2 id="how">How：</h2><p>是一个很清晰的流程：</p><ol type="1"><li><p>输入为单张RGB-D图像。D为depth，一般多镜头智能手机拍摄的照片都能提供深度信息；没有的话就用其他模型预测深度，比如MegaDepth, MiDas, and Kinect depth sensor</p></li><li><p>将输入图像转化成分层深度图像（Layered Depth Image）。LDI中的每个像素点保存颜色和深度信息，以及上下左右四个方向的邻居像素点。同一个坐标处可以有重合的不同深度的像素点。</p></li><li><p>图像预处理：检测深度不连贯的边缘</p><figure><img src="https://s2.loli.net/2022/10/22/6tjCgUHTpVyIAFN.png" alt="image-20221007090910332" /><figcaption>image-20221007090910332</figcaption></figure><p>用filter把深度边缘过滤得更锐利，然后清理一些不连贯的边缘，最后根据连通性划分不同的深度边（如图2 （f）中，不同颜色表示不同深度边）。</p></li><li><p>对于每一个深度边，把LDI图中的像素点切割开，并在背景层扩展一些像素点，对扩展区域进行生成</p><figure><img src="https://s2.loli.net/2022/10/22/Ap62PxdYEDKzBkJ.png" alt="image-20221007091217942" /><figcaption>image-20221007091217942</figcaption></figure><ol type="1"><li><p>找到一个深度边，把两层的像素点切割开</p></li><li><p>对于背景层，用flood-fill like算法迭代地选取一定的已知区域作为context region，以及扩展一定的未知区域作为synthesis region</p></li><li><p>利用已知context region 生成未知synthesis region 的深度和颜色：采用基于学习的inpainting方法</p><figure><img src="https://s2.loli.net/2022/10/22/h8uGZm4XAEcL5SP.png" alt="image-20221007091716266" /><figcaption>image-20221007091716266</figcaption></figure><p>这个方法中，最关键的就是在预测color和depth之前，先预测了一下depth edges，然后把这个edges信息加进去，可以更好地预测color和depth。</p></li><li><p>将生成完毕的像素融合回LDI图像</p></li></ol></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;论文地址：&lt;a href=&quot;https://shihmengli.github.io/3D-Photo-Inpainting&quot; class=&quot;uri&quot;&gt;https://shihmengli.github.io/3D-Photo-Inpainting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="3DCV" scheme="https://jyzhu.top/tags/3DCV/"/>
    
  </entry>
  
  <entry>
    <title>Reading SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos</title>
    <link href="https://jyzhu.top/Reading-SmoothNet-A-Plug-and-Play-Network-for-Refining-Human-Poses-in-Videos/"/>
    <id>https://jyzhu.top/Reading-SmoothNet-A-Plug-and-Play-Network-for-Refining-Human-Poses-in-Videos/</id>
    <published>2022-09-09T06:25:03.000Z</published>
    <updated>2022-10-22T09:16:13.032Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://arxiv.org/abs/2112.13715</p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng%2C+A">Ailing Zeng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+L">Lei Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ju%2C+X">Xuan Ju</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J">Jiefeng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+J">Jianyi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+Q">Qiang Xu</a></p><p>发表： ECCV 2022</p><p>链接： <a href="https://github.com/cure-lab/SmoothNet" class="uri">https://github.com/cure-lab/SmoothNet</a></p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p></blockquote><h2 id="why">Why：</h2><ol type="1"><li>从视频估计人体姿势时，抖动是个问题</li><li>除了轻微抖动以外，有一些long- term抖动，还有因为重叠、姿势少见等原因造成的估测困难</li></ol><h2 id="what">What：</h2><figure><img src="https://s2.loli.net/2022/10/22/fRIB2t9hw8na6my.png" alt="image-20220909143919787" /><figcaption>image-20220909143919787</figcaption></figure><ol type="1"><li>一个仅基于时序的精炼网络，以其他网络的姿势估计结果作为输入。</li><li>有监督的</li><li>采用滑动窗口，基于TCN</li><li>并不是常见的那种基于学习的方法，即采用时间-空间模型来同时优化逐帧的准确率和时序的平滑性。这个方法通过学习每一个关节在长时间范围的运动特征（而不是关节之间的关系），来自然地建模身体运动中的平滑特征。</li><li>由于它仅仅需要时序信息，所以可以泛化到很多种任务上，包括2D和3D的姿势估计、body recovery等</li></ol><h2 id="how">How：</h2><ol type="1"><li><p>根据持续时长，将抖动归类为sudden jitter和long- term jitter两种。为了解决long- term的抖动问题，现有那些方法都不大行。</p><p>根据程度，又可以将抖动分为小抖动和大抖动。小抖动一般由于不可避免的误差，或者标注上的误差；大抖动则是由于图像质量差、姿势少、重叠严重等。</p><figure><img src="https://s2.loli.net/2022/10/22/pNa7bcVqzi9lGZw.png" alt="image-20220909143902334" /><figcaption>image-20220909143902334</figcaption></figure></li><li><p>将误差归类为相邻帧之间的抖动造成的误差（jitter error）和模型估计结果与真实结果之间的偏差（bias error）这两种。现有那些方法并没有将这两类误差解耦</p></li><li><p>提出了basic smoothnet和正经smoothnet。</p><ol type="1"><li><figure><img src="https://s2.loli.net/2022/10/22/Zf6lpahbHrRG7kF.png" alt="image-20220909154626864" /><figcaption>image-20220909154626864</figcaption></figure><p>Basic smoothnet，FCN是backbone。通过长度为T的滑窗，每次传入T帧图像，包含C个channels。</p><figure><img src="https://s2.loli.net/2022/10/22/hRcX2MCx8SnHZfz.png" alt="image-20220909155504588" /><figcaption>image-20220909155504588</figcaption></figure><p>权重<span class="math inline">\(w_t^l\)</span>和偏差<span class="math inline">\(b^l\)</span>是第<span class="math inline">\(t_{th}\)</span>帧的，并且在不同的channel之间是共享的。</p></li><li><figure><img src="https://s2.loli.net/2022/10/22/uCJgF8M6tG5S1PB.png" alt="image-20220909155701901" /><figcaption>image-20220909155701901</figcaption></figure><p>完整的motion- aware smoothnet就是加上了速度和加速度两个模块。</p><p>因为jitter的一个衡量方式就是加速度，所以把加速度直观地显示在模型中是一个很显然的方式。给定预测出的姿势<span class="math inline">\(\hat Y\)</span>，速度就是两帧之间相减，得到 <span class="math display">\[\hat V_{i,t} = \hat Y_{i,t} − \hat Y_{i,t−1}\]</span> 加速度就是速度之间的差： <span class="math display">\[\hat A_{i,t} = \hat V_{i,t} − \hat V_{i,t−1}\]</span></p></li></ol></li><li><p>loss就是两个：</p><ol type="1"><li><p>ground truth pose和估计pose之间的误差： <span class="math display">\[L_{pose} = \frac{1}{T\times C} \sum_{t=0}^T \sum_{i=0}^C |\hat G_{i,t} − Y_{i,t}|,\]</span></p></li><li><p>ground truth 加速度和估计加速度之间的误差： <span class="math display">\[L_{acc} = \frac{1}{(T-2)\times C} \sum_{t=0}^T \sum_{i=0}^C |\hat G&#39;&#39;_{i,t} − A_{i,t}|,\]</span></p></li></ol></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;论文地址：https://arxiv.org/abs/2112.13715&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zeng%2C+A&quot;&gt;Ailing Zeng&lt;/</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Reading NeuMan: Neural Human Radiance Field from a Single Video</title>
    <link href="https://jyzhu.top/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/"/>
    <id>https://jyzhu.top/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/</id>
    <published>2022-08-24T04:25:25.000Z</published>
    <updated>2022-09-01T04:57:08.650Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://arxiv.org/abs/2203.12575</p><p>作者：Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag</p><p>发表： ECCV22</p><p>开源代码： https://github.com/apple/ml-neuman</p><h2 id="why">Why：</h2><ol type="1"><li><p>人体的渲染和新姿势生成在增强现实的应用中很重要</p></li><li><p>NeRF的出现让新视角生成任务取得很大进步</p></li><li><p>但是现有工作都没有实现：根据单段wild视频，生成新的人物和新的场景</p><figure><img src="https://s2.loli.net/2022/08/24/NdDrEhojmBLCJz9.png" alt="image-20220824123146868" /><figcaption>image-20220824123146868</figcaption></figure></li></ol><h2 id="what">What：</h2><p>读前疑问：</p><ol type="1"><li>NeRF和人体SMPL模型是怎么有机统一的🤔</li></ol><h2 id="how">How：</h2><ol type="1"><li><p>输入是一段wild视频，moving camera的。用现存方法估计人体姿势、人体形状、人体mask（Mask-RCNN）、相机pose、sparse scene model、depth maps</p></li><li><p>然后训练两个 NeRF 模型，一个用于人体，一个用于由 Mask-RCNN 估计的分割mask引导的背景。 此外，通过将来自多视图重建和单目深度回归的深度估计融合在一起来规范场景 NeRF 模型</p></li><li><p>关于NeRF：(参考：<a href="https://zhuanlan.zhihu.com/p/360365941">zhihu</a>）</p><ol type="1"><li><p>NeRF是用神经辐射场建模一个场景，好处是可以生成新视角的图像。针对一个静态场景，需要提供大量相机参数已知的图片。基于这些图片训练好的神经网络，即可以从任意角度渲染出图片结果了。</p></li><li><p>它用MLP，把一个3d场景隐式地编码进神经网络里。输入为3d空间中一个点的坐标<span class="math inline">\(\bold x = (x,y,z)\)</span>和相机视角 <span class="math inline">\(\bold d = (\theta, \phi)\)</span>，输出为该点对应的体素的密度opacity，以及颜色<span class="math inline">\(\bold c = (r,g,b)\)</span>。公式就是 <span class="math display">\[f(\bold{x},\bold{v})=(\bold c, \sigma)\]</span></p></li><li><p>在具体的实现中， x 首先输入到MLP网络中，并输出 σ 和中间特征，中间特征和 d 再输入到额外的全连接层中并预测颜色。因此，体素密度只和空间位置有关，而颜色则与空间位置以及观察的视角都有关系。基于view dependent 的颜色预测，能够得到不同视角下不同的光照效果。</p></li><li><p>NeRF 函数得到的是一个3D空间点的颜色和密度信息，但当用一个相机去对这个场景成像时，所得到的2D 图像上的一个像素实际上对应了一条从相机出发的射线上的所有连续空间点。后续就有各种各样高效的方式来进行可微渲染了，本质上都是从这条射线上采样，获得平均的颜色信息。</p></li></ol></li></ol><h3 id="人体模型nerfsmpl">人体模型：NeRF+SMPL</h3><p>我主要关注的就是人体模型这部分了。总体来说，做法就是：</p><p>首先生成人体NeRF模型，然后用ROMP生成逐帧的人体SMPL模型，然后定义一个canonical的人体模型（主要是去掉姿势这个变量，变成大字型人体），根据像素点在SMPL模型上对应的位置，再对应到canonical模型上，学到人体的外貌。（其实训练中NeRF和SMPL模型是一起学的，没有分得那么开的先后顺序。）</p><figure><img src="https://s2.loli.net/2022/08/24/JSycDH34UlMBsEW.png" alt="image-20220824180150642" /><figcaption>image-20220824180150642</figcaption></figure><p>具体来说：</p><ol type="1"><li><p>对于某一帧图像，用ROMP估计人体的SMPL模型，但采取了一些改良：</p><ol type="1"><li>利用densepose估计人体的silhouette，以及MMPose估计人体的2D joints；根据这些结果优化SMPL参数</li></ol></li><li><p>把刚刚得到的SMPL模型warp成一个canonical的大字型人体模型，这个warp变换称为<span class="math inline">\(\mathcal T\)</span></p></li><li><p>怎么把图像中的像素点对应到canonical的大字型人体模型上呢？</p><ol type="1"><li><p>首先生成人体NeRF模型</p></li><li><p>对于空间中的每个点<span class="math inline">\(\bold x_f=\bold r_f(t)\)</span> （这里的f是第f帧图像），它都可以由一条射线<span class="math inline">\(\bold r\)</span>上对应的像素点渲染而来；那么对这个点直接应用前面的变换<span class="math inline">\(\mathcal{T}\)</span>，就得到它在canonical空间中对应的点了，<span class="math inline">\(\bold x&#39;_f = \mathcal{T}_{\theta_f}(\bold x_f)\)</span></p></li><li><p>但是因为SMPL的估计不是很准确，这个变换<span class="math inline">\(\mathcal{T}\)</span>也不是很准确，所以这里提出来，通过在训练中同时优化SMPL模型 <span class="math inline">\(\theta_f\)</span>和人体NeRF模型的方式，可以提升效果。</p></li><li><p>还有，还加了一个MLP改错网络<span class="math inline">\(\mathcal{E}\)</span>改正warping的误差。最终结果就是： <span class="math display">\[\bold {\tilde x&#39;_f} = \mathcal{T}_{\theta_f}(\bold x_f) + \mathcal{E}(\bold x_f, f)\]</span></p></li><li><p>此时相机视角也需要校正：对于射线ray上的第i个样本点， <span class="math display">\[\bold d(t_i)&#39;_f = \bold {\hat x}&#39;_f(t_i) - \bold {\hat x}&#39;_f(t_{i-1})\]</span></p></li></ol></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;论文地址：https://arxiv.org/abs/2203.12575&lt;/p&gt;
&lt;p&gt;作者：Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="3D Generation" scheme="https://jyzhu.top/tags/3D-Generation/"/>
    
  </entry>
  
  <entry>
    <title>It seems impossible to access USB devices in Docker on MacOS</title>
    <link href="https://jyzhu.top/It-seems-impossible-to-access-USB-devices-in-Docker-on-MacOS/"/>
    <id>https://jyzhu.top/It-seems-impossible-to-access-USB-devices-in-Docker-on-MacOS/</id>
    <published>2022-05-16T11:59:12.000Z</published>
    <updated>2022-09-01T04:57:56.460Z</updated>
    
    <content type="html"><![CDATA[<p>According to <a href="https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd" class="uri">https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd</a>, it seems impossible to forward USB to a Docker container on MacOS, coz the Docker is running in a virtual environment via <a href="https://github.com/docker/for-mac/issues/900">hyperkit</a>.</p><p>First of all, ports of the host (i.e., MacOS) cannot be directly accessed by any virtual environment (i.e., Docker) on it. So, &quot;you first have to expose it to the virtual machine where Docker is running&quot;. However, Docker is running on hyperkit, which doesn't support usb forwarding.</p><p>The author provided another way to do it, that is to use<code>docker-machine</code>, which uses a Virtualbox VM to host the <code>dockerd</code> daemon, to replace the original docker. Then... why bother still using Docker, instead of just using Virtualbox to run the seperated environment?</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;According to &lt;a href=&quot;https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd&quot; class=&quot;uri&quot;&gt;https://dev.to/rubberduck/using-usb-with-</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Docker" scheme="https://jyzhu.top/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Reading Video-to-Video Synthesis</title>
    <link href="https://jyzhu.top/Reading-Video-to-Video-Synthesis/"/>
    <id>https://jyzhu.top/Reading-Video-to-Video-Synthesis/</id>
    <published>2022-05-13T09:36:36.000Z</published>
    <updated>2022-09-01T04:59:24.160Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf</p><p>作者：<a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p><p>发表： NeurIPS 2018</p><p>Project：https://tcwang0509.github.io/vid2vid/</p><p>Github：https://github.com/NVIDIA/vid2vid</p><hr /><blockquote><p><strong>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</strong></p><p>这篇论文的背景是当时已经有比较好的pic2pic生成模型了。要让vid2vid也work的话，最重要的应该就是帧与帧之间consistency的问题。所以我会想在将pic2pic生成模型应用在video的基础上，对帧之间加上consistency loss。但直接这样肯定效率很低，因为一个视频中帧与帧之间肯定会包含大量冗余信息嘛，应该还需要想办法让帧之间信息共享，这样模型只需要预测后一帧与前一帧不同的地方，减少运算。例如，用一个小网络预测图像中静态与动态的部分，不知道可不可行。</p><p><strong>看完文章后</strong>：我感觉我在大体思路上把握准了，主要矛盾确实如此。但</p><ol type="1"><li>作者没有用consistency loss，而是用gan的思路，设计了一个条件视频鉴别器 <span class="math inline">\(D_V\)</span>，鉴别视频在时序上的动态是否真实自然。</li><li>我没有optical flow，光流，这方面的知识储备；作者利用一个网络预测optical flow，就可以直接根据前一帧图像得到后一帧图像中对应的像素点了，而且这样的结果能够很consistent。对于前一帧图像中没有对应的像素，再用一个补洞网络补洞。这样就解决了效率问题。</li><li>作者还利用了特征嵌入方法，实现了多模态视频的生成，这是我第一次了解到的方法，感觉很有趣。</li></ol></blockquote><h2 id="why">Why：</h2><ol type="1"><li>图像水平上的生成被研究得很好，但是视频上的此前却比较少；图像生成的成果如果直接放在视频上的话，效果不太妙，因为帧与帧之间缺乏连贯性。所以是需要一些temporal上的改进的</li></ol><h2 id="what">What：</h2><ol type="1"><li>利用GAN和一些时间-空间对抗目标（spatio-temporal adverbial objective），来实现video to video的生成。</li><li>用不同类型的输入来生成新的照片写实风格的视频，效果很好。</li><li>是一个全新定义的vid2vid任务，主要新点在于：输入的vid并不是完整的视频帧，而是一些可以操控的语义信息，例如segmentation masks, sketches, and poses</li></ol><p>读前疑问：</p><ol type="1"><li>在此之前没有比较好的vid2vid，这篇论文是在什么条件下实现了很好的vid2vid效果呢，比如其他方面的技术革新？我觉得主要是利用了gan，一个图像鉴别器+一个视频鉴别器相配合，取得很好的生成效果。除此之外我觉得optical flow用在这里也很好，效率高而且生成效果连贯（但是不知道新不新）；另外特征嵌入方法用在这里也很好，实现了根据语义信息来生成新视频，而且可以是多模态的视频</li><li>摘要里强调的时间-空间对抗目标（spatio-temporal adverbial objective）到底是什么？我感觉正文里好像没有再特别强调时间-空间这一对目标了……根据我自己的理解的话，主要就是那一对鉴别器：时间上--视频鉴别器鉴别在时序上的动态是否真实自然，空间上--图像鉴别器鉴别一张图像在空间上是否真实自然。</li></ol><h2 id="how">How：</h2><h3 id="定义vid2vid任务">定义vid2vid任务</h3><ol type="1"><li><p>定义输入的语义信息序列为<span class="math inline">\(s\)</span>，对应的真实视频序列为<span class="math inline">\(x\)</span>，模型生成的视频序列为<span class="math inline">\(\tilde x\)</span>，则模型的目标是在给定<span class="math inline">\(s\)</span>的条件下，让<span class="math inline">\(\tilde x\)</span>的条件分布拟合<span class="math inline">\(x\)</span>的条件分布</p></li><li><figure><img src="https://s2.loli.net/2022/09/01/VfJE42QpgUB5zxt.png" alt="image-20220524143313609" /><figcaption>image-20220524143313609</figcaption></figure><p>D是discriminator，G是generator。整个任务就变成了一个最大最小优化问题，论文主要通过设计网络和时空优化目标来解决这个问题。</p></li><li><p>为了简化问题，做了一个Markov假设：当前第t帧生成的视频<span class="math inline">\(\tilde x_t\)</span>，由且仅由第t帧输入<span class="math inline">\(s_t\)</span> + 前L帧输入<span class="math inline">\(s_{t-L}^{t-1}\)</span> + 前L帧生成的视频<span class="math inline">\(\tilde x_{t-L}^{t-1}\)</span>这三个因素决定。<img src="https://s2.loli.net/2022/09/01/Q9MPRD5otda3mAq.png" alt="image-20220524144030781" /></p><p>实验里，L取了个不大不小的2.建立了一个神经网络F，递归地逐帧生成视频。</p></li></ol><h3 id="网络架构">网络架构</h3><ol type="1"><li><p>网络F定义如下：</p><figure><img src="https://s2.loli.net/2022/09/01/5OA4ptSUjI9Mn7J.png" alt="image-20220524144722968" /><figcaption>image-20220524144722968</figcaption></figure><p>给定<span class="math inline">\((\tilde x_{t-L}^{t-1}, s_{t}^{t-1})\)</span>作为输入。对于与上一帧图像有关联的像素点，网络会利用optical flow来warp（扭曲？）上一帧像素点，得到这一帧新的像素点。对应等式的前半部分。还有一些像素是上一帧图像里没有的，这时候就需要生成来填充。对应等式的后半部分。具体来说：</p><ul><li>用一个optical flow预测网络W来估计从上一帧到这一帧的optical flow <span class="math inline">\(\tilde w_{t-1}\)</span>.</li><li>用一个生成器H来生成需要填充的像素<span class="math inline">\(\tilde h_t\)</span>.</li><li>用一个mask预测网络M来生成mask <span class="math inline">\(\tilde m_t\)</span>. 这个mask不是非0即1的，而是包含了0-1之间的连续值。这样做是为了更好地融合W和H生成的结果。比如说，在 zoom in 的情况下，一个物体逐渐靠近，那么它会逐帧放大。如果仅仅利用optical flow的扭曲结果，那么这个物体就会变得模糊。因此，还需要生成器来填充一些细节。有了soft mask，warp的像素和生成的像素就可以融合。</li></ul></li><li><p>用了coarse-to-fine的方法来生成高分辨率的视频</p></li><li><p>用了2个不同的discriminator来减轻gan训练中的mode collapse问题（模式倒塌，即生成的结果是很逼真，但是多样性不足）。</p><ol type="1"><li>条件图像鉴别器 <span class="math inline">\(D_I\)</span>，顾名思义，鉴别每一帧图像是否真实的</li><li>条件视频鉴别器 <span class="math inline">\(D_V\)</span>，鉴别视频在时序上的动态是否真实自然。给定optical flow，鉴别K个连续的帧</li></ol></li></ol><h3 id="losses">losses</h3><p><span class="math display">\[\mathop{min}\limits_{F} ( \mathop{max}\limits_{D_I}\mathcal{L}_I (F, D_I ) + \mathop{max}\limits_{D_V}  \mathcal{L}_V (F, D_V )) + λ_W L_W (F ),\]</span></p><ol type="1"><li><span class="math inline">\(\mathcal{L}_I\)</span> 是条件图像鉴别器 <span class="math inline">\(D_I\)</span>的gan loss：<img src="https://s2.loli.net/2022/09/01/J1mVkCnzWYZiaR9.png" alt="image-20220530194307049" />，其中，<span class="math inline">\(\phi_I\)</span> 就是从第1～T帧中随机取1帧的操作</li><li><span class="math inline">\(\mathcal{L}_V\)</span>是条件视频鉴别器 <span class="math inline">\(D_V\)</span>的 gan loss：<img src="https://s2.loli.net/2022/09/01/rq6yiaRSHUnEYzg.png" alt="image-20220530194419594" />，和图像的如出一辙，<span class="math inline">\(\phi_V\)</span> 就是从第1～T帧中随机取连续K帧的操作</li><li><span class="math inline">\(L_W\)</span> 是flow estimation loss：<img src="https://s2.loli.net/2022/09/01/AuZf1zlcihE3KsU.png" alt="image-20220530194534485" />，包括两部分，1. 真实flow和估计flow的端点误差 2. 把前一帧扭曲到后一帧的warp loss</li></ol><h3 id="前景-背景先验">前景-背景先验</h3><p>通过语义分割，给模型提供了一个前景、背景的先验信息，同时把补洞网络拆分成了两个：</p><ol type="1"><li>背景补洞网络：这个很容易，因为整个大背景其实可以由optical flow很准确地预测出来，补洞网络只需要补一点点从画面外面刚进来的部分</li><li>前景补洞网络：这个比较难，因为前景物品往往占比不大，但是又动作幅度很大，前景补洞网络需要从零开始生成很多东西</li></ol><p>通过用户实验，证明大部分人觉得有这个前景-背景先验之后，效果更好。</p><h3 id="多模态生成">多模态生成</h3><p>网络F是一个单模态映射函数，这意味着输入一个视频，它也只能生成一个视频。那怎样让它根据同一个输入视频，输出多个不同的视频呢？这里采用了特征嵌入方法（feature embedding scheme）。</p><ol type="1"><li>输入源视频的同时，也输入instance级别的语义分割mask <span class="math inline">\(s_t\)</span></li><li>训练一个图像编码器E，它把每一帧真实图像<span class="math inline">\(x_t\)</span>编码成d维的特征map（论文中d取3）。然后用一个instance-wise的平均池化，来让同一个物体的所有像素分享共同的特征向量。得到的这个instance-wise平均池化后的图像特征map称为<span class="math inline">\(z_t\)</span></li><li>这个<span class="math inline">\(z_t\)</span>，加上mask <span class="math inline">\(s_t\)</span>，再被输入到网络F</li><li>以上是训练的过程。训练结束后，对每个类型的对象的特征向量的高斯分布拟合一个混合高斯分布。</li><li>测试的时候，利用每个物体所对应的类型的分布，可以sample特征向量，进而生成新视频了。给出不同的特征向量，F就能生成不同的视频了</li></ol><figure><img src="https://tcwang0509.github.io/vid2vid/paper_gifs/cityscapes_change_styles.gif" alt="img" /><figcaption>img</figcaption></figure><h3 id="实现的细节">实现的细节</h3><ol type="1"><li>coarse-to-fine的训练：512 × 256, 1024 × 512, and 2048 × 1024 resolutions，这三种分辨率，先从低的开始训练起，逐渐增加到高的。</li><li>mask预测网络M和flow预测网络W共享权重，只有输出层不一样。</li><li>图像鉴别器是一个多尺度PatchGAN</li><li>除了空间上的多尺度，视频鉴别器还会考虑多个帧率，即时间上的多尺度，确保短期和长期都能consistent</li><li>2k分辨率，8个v100 gpus，训练10天……</li><li>datasets：<ol type="1"><li>Cityscapes：训练DeepLabV3网络来获得所有的语义分割mask，用FlowNet2的结果作为optical flow的ground truth，用Mask R- CNN的结果作为instance- level 语义mask的gt</li><li>Apolloscape</li><li>Face video dataset：FaceForensics dataset里的真实视频</li><li>Dance video dataset：从YouTube下载的跳舞视频💃</li></ol></li></ol><h3 id="结果">结果</h3><p>图像生成模型pix2pixHD和视频风格迁移模型COVST作为baseline。FID（论文定义的视频变种）跟baseline比略好，但human preference score（论文定义的由人来打分，评估哪个视频更真实）高很多。</p><p>通过更改语义mask，可以控制生成视频中的物体种类；通过更改特征向量，可以控制生成视频中的物体外观；在未来视频预测上也有很好的性能。</p><h3 id="局限性">局限性</h3><ol type="1"><li>缺少一个物体内部的具体信息，生成转弯的车的时候效果比较差。论文提出或许可以通过增加3D信息作为输入来解决</li><li>在整个视频中，一个物体的外观有时候还是前后不一致</li><li>偶然情况下，一辆车的颜色可能会逐渐发生变化</li><li>当通过更改语义信息来操纵视频生成的时候，例如把树改成房子，偶然会出现一部分变成房子，另一部分树变了形的情况（？是这个意思吗）。这或许可以通过采用更粗糙的语义标签的方式解决，因为这样模型就不会对标签形状过于敏感</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;论文地址：https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://tcwang0509.github.io/&quot;&gt;Ting-Chun Wang&lt;/a&gt;, &lt;a href</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="Video Generation" scheme="https://jyzhu.top/tags/Video-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Reading Few-shot Video-to-Video Synthesis</title>
    <link href="https://jyzhu.top/Reading-Few-shot-Video-to-Video-Synthesis/"/>
    <id>https://jyzhu.top/Reading-Few-shot-Video-to-Video-Synthesis/</id>
    <published>2022-05-13T08:49:57.000Z</published>
    <updated>2022-09-01T05:00:11.103Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：http://arxiv.org/abs/1910.12713</p><p>作者：<a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, Andrew Tao, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p><p>发表： NeurIPS 2019</p><p>Project： https://nvlabs.github.io/few-shot-vid2vid</p><p>Github：https://github.com/NVLabs/few-shot-vid2vid</p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p><p>首先这个任务选题对我来说很新，我之前都没有意识到过这方面的问题。如果告诉我有这样的问题，需要去解决的话，我的直观的想法会受到这篇论文作者的上一篇中提到的 特征嵌入方法 所影响：会想也通过将一类物体的特征编码起来，然后通过学习不同个体的特征编码，来实现不同风格的视频生成。</p></blockquote><h2 id="why">Why：</h2><p>当今vid2vid方法的两个局限性：</p><ol type="1"><li>需要大量数据，尤其是需要生成的这个人的视频数据</li><li>泛化能力有限，比如说只能在训练集中包含的人上生成新的pose-to-human视频，不能泛化到训练集中不存在的人上</li></ol><p>所以这篇论文就是想解决这两个问题。</p><h2 id="what">What：</h2><ol type="1"><li><p>任务是Video-to-video synthesis，即利用输入的语义视频（例如人的姿势、街景），生成写实的视频。例如说，人体姿势生成的任务，就是首先收集一个人做大量不同动作的视频，作为训练集；然后向模型中输入动作序列，让模型生成该人做该动作的视频。再比如街景生成，也是以大量街景视频作为训练集，然后向模型中输入语义mask序列，让它生成风格类似的全新街景。</p></li><li><p>这篇论文提出了一个网络，其中包括一个网络权重生成模块（novel network weight generation module）和attention机制</p></li><li><p>这个方法的创新点在于，只需要在测试时，向模型提供少量的在训练集中没出现过的新的人物的图像，它就能生成这个新的人的视频。</p><figure><img src="https://s2.loli.net/2022/08/10/IUKDziV4AtOwsxu.png" alt="image-20220513171420180" /><figcaption>image-20220513171420180</figcaption></figure><p>上图中，左边是现存方法，它们基本上对于每个人，都需要在单独的训练集上训练。右边是这篇论文提出的方法，只需要训练一次，然后输入一些示范图像，就可以泛化到新的人上。</p></li></ol><p>读前疑问：</p><ol type="1"><li>说是利用少量的新的人物的示范图像，生成网络权重。意思是以原本的vid2vid网络的权重作为输出？为什么？我的更直观的想法是，直接用一个新网络，学习新人物的图像，然后把output给concat或者加进旧网络的output中……另外，直接作用于网络权重上，在我的粗浅理解中，会不会造成信息的损失呢？还是说本质上没差？ related work里提到这类网络属于adaptive network，跟常规网络相比有不同的inductive bias（想想也是），有对应的应用任务。或许我之后再了解一下这块。</li><li>标题中的few-shot是什么意思，就是指更少的data、更高的泛化性吗？这是一类任务吧，从少量标注的样本中学习的意思。这个确实就是啊，只需要一点点示范图像，就可以生成图中这个人/物的新video。</li><li>作者的上篇论文是利用gan，这篇又用上了attention，为什么作出这样本质的改变呢？</li></ol><h2 id="how">How：</h2><ol type="1"><li>视频生成任务可以分成3类：<ol type="1"><li>unconditional synthesis：随机生成视频片段</li><li>future video prediction</li><li>vid2vid：把语义输入转变成现实风的视频。这篇论文就是属于这个任务，不过它聚焦的点在于few shot，即通过在测试的时候输入少量图像，让生成的视频可以泛化到没见过的domain上</li></ol></li><li><p>vid2vid是前一篇工作的内容啦。reference：<a href="jyzhu.top/reading-video-to-video-synthesis"><em>Reading vid2vid</em></a></p></li><li><p>few-shot本质上就是多加了个网络E，用来生成原补洞网络H的权重。至于原本还有两个网络W和M，他们都不需要改动，因为他们都是基于上一帧生成的图像进行变形的，代表一种运动，而和视频本质的内容没有关系。</p></li><li><p>精髓一图：</p><figure><img src="https://s2.loli.net/2022/08/10/T6is35cjyHlvaNe.png" alt="image-20220810193845710" /><figcaption>image-20220810193845710</figcaption></figure></li><li><p>用最新的SOTA语义图像生成模型SPADE代替了上一篇论文中的网络H。SPADE包含several spatial modulation branches and a main image synthesis branch。不过网络E只更新SPADE模型中的spatial modulation branches的权重，因为1这样量比较小，2这样可以避免一个直接从input image到output image的短路（我尚没有深究原因）。</p></li><li><p>权重生成模块E。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;论文地址：http://arxiv.org/abs/1910.12713&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://tcwang0509.github.io/&quot;&gt;Ting-Chun Wang&lt;/a&gt;, &lt;a href=&quot;http://mingyuliu.net/</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="Video" scheme="https://jyzhu.top/tags/Video/"/>
    
  </entry>
  
  <entry>
    <title>Live demo of CodeTyping via Pythonanywhere</title>
    <link href="https://jyzhu.top/live-demo-of-code-typing-via-pythonanywhere/"/>
    <id>https://jyzhu.top/live-demo-of-code-typing-via-pythonanywhere/</id>
    <published>2022-04-07T15:24:15.000Z</published>
    <updated>2022-04-07T15:34:41.597Z</updated>
    
    <content type="html"><![CDATA[<figure><img src="https://user-images.githubusercontent.com/39082096/149967763-a9bb56c5-6411-4d86-90d3-f1e22845e2a8.png" alt="image" /><figcaption>image</figcaption></figure><p><a href="http://jyzhu.pythonanywhere.com/">Code Typing Practice</a> (or source code on Github: <a href="https://github.com/viridityzhu/code-typing">here</a>) is a tiny web page that I wrote for fun last semester, which is for me myself to practice code typing. It is a naive Django web app (with bugs🤪). BUT! I find a service to deploy it lively today. That's what is worth noting down now.</p><span id="more"></span><h2 id="note">Note</h2><p>Initially I tried to deploy this demo on Vercel.com. But it was too troublesome coz it does not support Django by default. Thankfully, I found <a href="https://www.pythonanywhere.com/">pythonanywhere</a>, on which each user can deploy one web app without payment. What's the best is that it is really easy to deploy: it provides access to Bash console.</p><p>Two things to be noted:</p><ol type="1"><li>Every 3 months, I have to login into the pythonanywhere to extend my web app, otherwise it will be killed.</li><li>The bug cost most of my time is that in the <code>view.py</code> I had used relative path to the static code files. However, I should use absolute path, with adding <code>BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))</code> ahead.</li></ol><h2 id="todo">TODO</h2><p>Actually, after learned the MERN frame this semester, I am now aware of how naive this project is. However, I love Python, so it doesn't matter if i still regard Django as a hobby🤨. Who knows... I haven't even spent my time on that course project...</p><p>Now that the live demo is achieved, I might think of polishing this little project a bit.</p><ul><li>[ ] Fix bugs. Though i've already forgotten what those bugs are...</li><li>[ ] Replace the stupid code snippets...</li><li>[ ] Add the feature to compute time cost and typing speed. Also, save typing records.</li><li>[ ] Explicitly support other kinds of typing materials, and also support uploading customize materials.</li></ul>]]></content>
    
    
    <summary type="html">&lt;figure&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/39082096/149967763-a9bb56c5-6411-4d86-90d3-f1e22845e2a8.png&quot; alt=&quot;image&quot; /&gt;&lt;figcaption&gt;image&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;http://jyzhu.pythonanywhere.com/&quot;&gt;Code Typing Practice&lt;/a&gt; (or source code on Github: &lt;a href=&quot;https://github.com/viridityzhu/code-typing&quot;&gt;here&lt;/a&gt;) is a tiny web page that I wrote for fun last semester, which is for me myself to practice code typing. It is a naive Django web app (with bugs🤪). BUT! I find a service to deploy it lively today. That&#39;s what is worth noting down now.&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Django" scheme="https://jyzhu.top/tags/Django/"/>
    
    <category term="Python" scheme="https://jyzhu.top/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>三月的人生主题是复杂性</title>
    <link href="https://jyzhu.top/complexity-again/"/>
    <id>https://jyzhu.top/complexity-again/</id>
    <published>2022-03-14T07:26:36.000Z</published>
    <updated>2022-03-14T07:48:22.442Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我放弃理解很多东西</p><p>我开始拥抱</p><p>惊人的复杂性</p></blockquote><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="450" src="//music.163.com/outchain/player?type=0&amp;id=7326559292&amp;auto=1&amp;height=430"></iframe><p>这两天列了一个名叫「精神状况」的歌单，顾名思义，遴选了大抵是最能有效表达我近来精神状况的9首歌（不出所料以摇滚为主）。奇妙的是，一方面我自诩听歌时对歌词甚是重视，另一方面最爱的歌竟然歌词含量不足50%。想想现实，或许是我放弃理解很多东西（用理性），开始拥抱复杂性了吧（用感性？）。</p><p><img src="https://s2.loli.net/2022/03/14/W5bxdr9w3egqRGF.jpg" style="zoom: 33%;" /></p><hr /><h3 id="最近随想">最近随想：</h3><ul><li>「活着很累，在漫长的生命中，积累的痛苦与折磨会变得绵长。」所以人长大了开始喝酒，一遍遍喝酒，只是因为积压的痛苦无法消解吧。不过这样的好处就是会有一天不再像年轻时候那样怕死了</li><li>我很悲戚，这个世界的悲戚底色近年也逐渐显露，看得浅的人也能举目就看见灰黑色了。很无望，年复一年埋头活着，一抬头就搞不清楚在盼什么。想到父母渐老，大小毛病接连不断；就连我自己都开始显露一些身体不好的迹象，就实在是难过。想到我的生活，海外留学，专业学术，与父母的生活，柴米油盐，家长里短，我们的世界是割裂的，我整个人也感到一种割裂，待在父母身边或者远方，都很悲哀。这种悲哀甚至只是稍纵即逝的，它哪怕能长存一些，我也能对生活稍多些把握。只是时间总会过得很快，可预见的未来还会面临变更，伴随更深、更无力的悲哀，例如作为一个成年人需承担的一个家庭的压力，例如重要的人的衰老、疾病、死亡。</li><li>我有时候对人生很随便，糟糕境遇的发生会被钝感而忽略，或者很快视若无睹，例如疫情和疫情后的世界。只是每一件事，也不至于对我全无影响，我的荒诞感随着它们一层一层加深。</li><li>昨天去外公墓上挂社了，和妈妈、外婆一起。外婆跟外公说很多话，问他这里风景好不好，有没有去哪里钓鱼。妈妈偷偷抹了几次眼泪。我最没用，眼泪大颗大颗滴在纸钱上，都不好烧了。可是我看见墓碑上外公的名字，他笑容灿烂的彩色照片，只是很想念很想念。妈妈让我作揖的时候可以心里跟外公说一些话，告诉他不要担心、请他保佑。可是我一边作揖一边心里只有一个声音：尕公啊 尕公诶[泪]</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;我放弃理解很多东西&lt;/p&gt;
&lt;p&gt;我开始拥抱&lt;/p&gt;
&lt;p&gt;惊人的复杂性&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; </summary>
      
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>复杂性</title>
    <link href="https://jyzhu.top/complexity/"/>
    <id>https://jyzhu.top/complexity/</id>
    <published>2022-03-14T07:22:53.000Z</published>
    <updated>2022-03-14T07:26:15.368Z</updated>
    
    <content type="html"><![CDATA[<p>我放弃理解很多东西</p><p>我开始拥抱</p><p>惊人的复杂性</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我放弃理解很多东西&lt;/p&gt;
&lt;p&gt;我开始拥抱&lt;/p&gt;
&lt;p&gt;惊人的复杂性&lt;/p&gt;
</summary>
      
    
    
    
    <category term="poems" scheme="https://jyzhu.top/categories/poems/"/>
    
    
  </entry>
  
  <entry>
    <title>Reading Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation</title>
    <link href="https://jyzhu.top/Reading-Pixel2meshPP/"/>
    <id>https://jyzhu.top/Reading-Pixel2meshPP/</id>
    <published>2022-02-06T09:59:29.000Z</published>
    <updated>2022-03-14T07:49:35.086Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://arxiv.org/abs/1908.01491</p><p>作者：Chao Wen, Yinda Zhang, Zhuwen Li, Yanwei Fu</p><p>发表： ICCV 2019</p><p>链接： https://arxiv.org/abs/1908.01491</p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p></blockquote><h2 id="why">Why：</h2><ol type="1"><li>单视角图像3D重建模的效果不够好，尤其是背面，而且泛化能力也差。</li><li>所以增加多个视角的图像：更多视觉信息，且有已经定义得很好的传统方法。</li><li>但是传统方法需要更大量的图像；这时候深度学习模型可以隐式编码视角间的关联，就派上了用场。</li><li>很有用，但是欠研究。</li></ol><h2 id="what">What：</h2><ol type="1"><li>利用多视角图像，固定相机pose参数，利用GCN，从粗糙逐渐精细地变形，生成3D mesh重建模。</li><li>采样mesh模型顶点周围的区域，利用perceptual feature推理出对mesh的形变。</li><li>对于不同种类的物体泛化能力很好。</li></ol><p>读前疑问：</p><ol type="1"><li>似乎就是给pixel2mesh加上一层壳，应用在多视角图像上🤔那么这里的创新点本质在哪儿呢？</li><li>GCN用处大吗，为什么用它？</li><li>采样mesh顶点周围的区域，是什么意思？</li><li>多视角3D重建，应用真的广泛吗？</li></ol><h2 id="how">How：</h2><ol type="1"><li>多视角变形网络 Multi-View Deformation Network (MDN)</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;论文地址：https://arxiv.org/abs/1908.01491&lt;/p&gt;
&lt;p&gt;作者：Chao Wen, Yinda Zhang, Zhuwen Li, Yanwei Fu&lt;/p&gt;
&lt;p&gt;发表： ICCV 2019&lt;/p&gt;
&lt;p&gt;链接： https://arxiv</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/tags/Computer-Vision/"/>
    
    <category term="Deep Learning" scheme="https://jyzhu.top/tags/Deep-Learning/"/>
    
    <category term="3D Reconstruction" scheme="https://jyzhu.top/tags/3D-Reconstruction/"/>
    
  </entry>
  
</feed>
