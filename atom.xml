<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tianke Youke</title>
  
  <subtitle>A Base for Secreting and Running at Night</subtitle>
  <link href="https://jyzhu.top/blog/atom.xml" rel="self"/>
  
  <link href="https://jyzhu.top/blog/"/>
  <updated>2023-07-03T18:11:44.610Z</updated>
  <id>https://jyzhu.top/blog/</id>
  
  <author>
    <name>Jiayin Zhu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>奶奶</title>
    <link href="https://jyzhu.top/blog/Grandma/"/>
    <id>https://jyzhu.top/blog/Grandma/</id>
    <published>2023-07-03T18:08:23.000Z</published>
    <updated>2023-07-03T18:11:44.610Z</updated>
    
    <content type="html"><![CDATA[<p>奶奶身体还好，但心态不好。健忘、固执，自闭，悲观。她几乎没有什么盼头了。她觉得活得失败，活得不好。她觉得如今脸上无光。她说：</p><p>我最痛恨别人，半熟不熟的人见面，跟你打招呼，问你家里近况怎么样。我很生气，简直想骂回去。可是你骂了吧，人家觉得你是神经病。可是我要怎么回答呢？我这两个儿子，一个工作都丢了，一个身体又那样不好。</p><p>我命苦啊。我从小家里穷，也没人管我。13岁就出去工作养活自己了。跟了你爷爷，过的都是苦日子。他一个月三十四块五角钱，二十块钱给他爸妈，雷打不动的。家里饭都吃不起了，也要给。剩下十四块五，十块钱给我，他留四块五。抽最差的烟，走得那么早。</p><p>我要管家里所有事。他什么也不管啊，一日三餐，他妈，两个儿子，去河边洗衣服。我还要上班。我每天都好累。</p><p>好不容易，日子稍微好一点了。他又走了！我那时简直恨他。</p><p>别看我现在这样，现在是我最轻松的日子。共产党给我发钱。所以我说，感谢共产党。我拿自己的钱，过自己的日子！就是记性不好。人活到连自理能力也没有了，还有什么意思。跟你爸说好了，到时候我死在这间屋子了，就火化。我都看得开。</p><p>死！我听到就眼泪直湍湍地流。奶奶说，不说这个了。她也抹眼泪。</p><p>奶奶一个月领三千多退休工资，自己省吃俭用，只花得了一千多。剩的，存在那，每年寒暑假我和妹妹去看她，她发给我们。</p><p>我说爸爸，你帮我把钱退给她。</p><p>爸爸说你拿着。她自己花不完，这钱不给你们，她拿着有什么用了？给了她安心。</p><p>我走的时候，她送出屋子，送到楼梯口。</p><p>我走远了，爸爸说，你回头再给奶奶挥挥手，你看她在阳台上看你呢。</p><p>我转头看到一簇花白的头发，远远的，很小。</p><p>能不能不要有离别呢？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;奶奶身体还好，但心态不好。健忘、固执，自闭，悲观。她几乎没有什么盼头了。她觉得活得失败，活得不好。她觉得如今脸上无光。她说：&lt;/p&gt;
&lt;p&gt;我最痛恨别人，半熟不熟的人见面，跟你打招呼，问你家里近况怎么样。我很生气，简直想骂回去。可是你骂了吧，人家觉得你是神经病。可是我要怎么回答呢？我这两个儿子，一个工作都丢了，一个身体又那样不好。&lt;/p&gt;
&lt;p&gt;我命苦啊。我从小家里穷，也没人管我。13岁就出去工作养活自己了。跟了你爷爷，过的都是苦日子。他一个月三十四块五角钱，二十块钱给他爸妈，雷打不动的。家里饭都吃不起了，也要给。剩下十四块五，十块钱给我，他留四块五。抽最差的烟，走得那么早。&lt;/p&gt;
&lt;p&gt;我要管家里所有事。他什么也不管啊，一日三餐，他妈，两个儿子，去河边洗衣服。我还要上班。我每天都好累。&lt;/p&gt;</summary>
    
    
    
    <category term="thoughts" scheme="https://jyzhu.top/blog/categories/thoughts/"/>
    
    
  </entry>
  
  <entry>
    <title>Solutions to Common Problems in Pytorch3D Rendering</title>
    <link href="https://jyzhu.top/blog/Solutions-to-Common-Problems-in-Pytorch3D-Render/"/>
    <id>https://jyzhu.top/blog/Solutions-to-Common-Problems-in-Pytorch3D-Render/</id>
    <published>2023-04-23T06:22:24.000Z</published>
    <updated>2023-04-23T06:42:33.231Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch3d-rendering-的一些疑难杂症">Pytorch3D Rendering 的一些疑难杂症</h1><p>有哪些？</p><ol type="1"><li>有了相机内参 K，而render又需要NDC坐标系，那要怎么定义相机？</li><li>图像的黄蓝色反了？</li><li>render 完的图像锯齿很严重？怎么抗锯齿（Antialiasing）？</li><li>皮肤表面反光太强，光滑得像镜面一样，怎样更自然？</li><li>怎么物体只剩半截，更远的部分似乎被截掉了？</li><li>没解决的问题：PBR（physical based rendering）</li></ol><span id="more"></span><h3 id="有了相机内参-k而render又需要ndc坐标系那要怎么定义相机">1. 有了相机内参 K，而render又需要NDC坐标系，那要怎么定义相机？</h3><p>这里的坑在于，camera本身支持任意坐标系，比如Freihand提供的是screen是224*224的相机坐标系。但是，render是默认NDC坐标系的！也就是normalized coordinate system，x和y是normalized到[-1,1]的。</p><p>一开始我直接把相机内参传给<code>PerspectiveCameras</code>，并且定义我的相机screen是224*224，像这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras = PerspectiveCameras(K=ks, image_size=((<span class="number">224</span>,<span class="number">224</span>),))</span><br></pre></td></tr></table></figure><p>完全不报错，就是有问题：render 过后没东西在画面上。</p><h4 id="解决">解决：</h4><p>我最后在<a href="https://pytorch3d.org/docs/cameras">官方文档</a>找到不起眼的一句：</p><blockquote><p>The PyTorch3D renderer for both meshes and point clouds assumes that the camera transformed points, meaning the points passed as input to the rasterizer, are in PyTorch3D's NDC space.</p></blockquote><figure><img src="https://user-images.githubusercontent.com/669761/145090051-67b506d7-6d73-4826-a677-5873b7cb92ba.png" alt="（世界坐标系 -&gt; 相机坐标系 -&gt; ndc坐标系 -&gt; 图像坐标系）" /><figcaption>（世界坐标系 -&gt; 相机坐标系 -&gt; ndc坐标系 -&gt; 图像坐标系）</figcaption></figure><p>我一看，原来默认PerspectiveCameras是ndc坐标系的，<code>in_ndc = False</code> by default！</p><p>所以解决方法就是：</p><blockquote><p>Screen space camera parameters are common and for that case the user needs to set <code>in_ndc</code> to <code>False</code> and also provide the <code>image_size=(height, width)</code> of the screen, aka the image.</p></blockquote><p>那么加一个参数就好了，可是谁知道这问题困扰了我整整两三天：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cameras = PerspectiveCameras(K=ks, in_ndc=<span class="literal">False</span>, image_size=((<span class="number">224</span>,<span class="number">224</span>),))</span><br></pre></td></tr></table></figure><p>另外，我还找到了如下这个等价方法，是先把内参转到NDC坐标系，再传给<code>PerspectiveCameras</code>。（至于为什么探索到这个方法，在后面问题 3 里可以找到原因…）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ndc_fcl_prp</span>(<span class="params">Ks</span>):</span></span><br><span class="line">        ndc_fx = Ks[:, <span class="number">0</span>, <span class="number">0</span>] * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_fy = Ks[:, <span class="number">1</span>, <span class="number">1</span>] * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_px = - (Ks[:, <span class="number">0</span>, <span class="number">2</span>] - <span class="number">112.0</span>) * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        ndc_py = - (Ks[:, <span class="number">1</span>, <span class="number">2</span>] - <span class="number">112.0</span>) * <span class="number">2</span> / <span class="number">224.0</span></span><br><span class="line">        focal_length = torch.stack([ndc_fx, ndc_fy], dim=-<span class="number">1</span>)</span><br><span class="line">        principal_point = torch.stack([ndc_px, ndc_py], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> focal_length, principal_point</span><br><span class="line"></span><br><span class="line">fcl, prp = get_ndc_fcl_prp(Ks)</span><br><span class="line">cameras = PerspectiveCameras(focal_length=-fcl, principal_point=prp)</span><br></pre></td></tr></table></figure><p>注意<code>focal_length=-fcl</code>，这个负号是为什么呢？这是另一个坑了哈哈哈哈。</p><p>答案是：pytorch3d坐标系的convention和我的相机不一样，它是+X指向左，+Y指向上，+Z指向图像平面外。这其中有个上下左右镜像的关系。</p><h3 id="图像的黄蓝色反了">2. 图像的黄蓝色反了？</h3><p>cv2的图像是BGR（老生常谈了），pytorch3d的是RGB。如果图像的黄蓝色相反了，基本就是这个问题，需要翻转一下，可以用torch的<code>clip(dim=(2,))</code></p><h3 id="render-完的图像锯齿很严重怎么抗锯齿antialiasing">3. render 完的图像锯齿很严重？怎么抗锯齿（Antialiasing）？</h3><p>锯齿就是说像下图这样，物体的边缘很尖锐，像素点粒粒分明！</p><figure><img src="https://s2.loli.net/2023/04/23/6gx5DEXJK9uMGwS.png" alt="rand_4_skin_rendered_bad" /><figcaption>rand_4_skin_rendered_bad</figcaption></figure><p>下面是我抗锯齿处理后的效果，可以看见边缘柔和了很多：</p><figure><img src="https://s2.loli.net/2023/04/23/LAchPHVfImRtkDs.png" alt="rand_4_skin_rendered" /><figcaption>rand_4_skin_rendered</figcaption></figure><p>（我真的搞了一周这个问题……看看我的心路历程：</p><ol type="1"><li>是不是 camera 没有用 NDC，而是直接用224x224的坐标系，导致投影过程有损失？所以我试了先转换成 NDC 坐标系的相机，再render。答案是，没有影响。</li><li>是不是 Shader 的参数设置得不对，比如 <code>blur_radius</code> 和 <code>faces_per_pixel</code> 应该调大一些？这其实是一个很直观的想法了，甚至一个有经验的学长看了之后都告诉我应该是这个问题。可是当我疯狂调大这两个参数，发现并没有改变这个问题。blur_radius 只会让物体内部的材质更模糊，但是边缘的锯齿完全没改变。faces_per_pixel更是无益，几乎不影响效果。</li><li>是不是图像尺寸太小了（224x224），只能达到这么个效果？我首先测试了调大图像尺寸，到<code>1024x1024</code>，发现锯齿边缘的确是不明显了！可是我又看了相机拍摄的原始图像，虽然是有点模糊，但是不至于这么大的锯齿呀，肯定还有别的问题。）</li></ol><h4 id="解决-1">解决：</h4><p>终于，在这个issue里找到同样的问题：https://github.com/facebookresearch/pytorch3d/issues/399</p><p>解决方案是：</p><blockquote><p>render at a higher resolution and then use average pooling to reduce back to the target resolution</p></blockquote><p>居然这么暴力……不过issue里面有很详细的解释，也能理解，这就是render原理之外需要考虑的事情，甚至算不上什么bug。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">aa_factor = <span class="number">3</span> <span class="comment"># Anti-aliasing factor</span></span><br><span class="line">raster_settings_soft = RasterizationSettings(</span><br><span class="line">        image_size=<span class="number">224</span> * aa_factor, </span><br><span class="line">    )</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">images = renderer(mesh)</span><br><span class="line">images = images.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># NHWC -&gt; NCHW</span></span><br><span class="line">images = F.avg_pool2d(images, kernel_size=aa_factor, stride=aa_factor)</span><br><span class="line">images = images.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># NCHW -&gt; NHWC</span></span><br></pre></td></tr></table></figure><h3 id="皮肤表面反光太强光滑得像镜面一样怎样更自然">4. 皮肤表面反光太强，光滑得像镜面一样，怎样更自然？</h3><p>一开始，皮肤 render 出来像这样，跟陶瓷似的，像话吗：</p><figure><img src="https://s2.loli.net/2023/04/23/8NWzr7txYKyqRk6.png" alt="rand_4_skin_rendered_bad2" /><figcaption>rand_4_skin_rendered_bad2</figcaption></figure><p>改进后，效果这样，自然多了：</p><figure><img src="https://s2.loli.net/2023/04/23/4qHNOs58ZhEI6Ry.png" alt="rand_4_skin_rendered_big" /><figcaption>rand_4_skin_rendered_big</figcaption></figure><h4 id="解决-2">解决：</h4><p>其实搞清楚材质相关的一些参数就好了。主要来说，这个反光是由这两个量决定的：</p><ol type="1"><li><code>specular_color</code>: specular reflectivity of the material，指定镜面反射颜色，在表面有光泽和镜面般的地方看到的颜色。</li><li><code>shininess</code>：定义材质中镜面反射高光的焦点。 值通常介于 0 到 1000 之间，较高的值会产生紧密、集中的高光。</li></ol><p>注意这里是改物体material的这些参数。虽然lighting也有这些参数定义，但这是关于光源的，和这个反光没有关系。</p><p>所以修改很简单：定义materials类，调整<code>specular_color</code>。默认是<code>1,1,1</code>，就是纯白色；调成<code>0.2,0.2,0.2</code>比较适合人的皮肤。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch3d.renderer <span class="keyword">import</span> Materials</span><br><span class="line"></span><br><span class="line">materials = Materials(</span><br><span class="line">    specular_color=((<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>),), <span class="comment"># 默认是1,1,1，就是纯白色；测试发现调成0.2,0.2,0.2比较适合人的皮肤。</span></span><br><span class="line">    shininess=<span class="number">30</span>, <span class="comment"># 默认值是 64，看上去高光稍微有点聚集了，改成30的话略自然，差别不太明显</span></span><br><span class="line">)</span><br><span class="line"> renderer_p3d = MeshRenderer(</span><br><span class="line">    rasterizer=MeshRasterizer(),</span><br><span class="line">    shader=HardPhongShader(</span><br><span class="line">        materials=materials,</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="怎么物体只剩半截更远的部分似乎被截掉了">5. 怎么物体只剩半截，更远的部分似乎被截掉了？</h3><p>还是一只手的模型，render 出来居然只有半个手背，距离相机更远的部分像是被截断了：</p><figure><img src="https://s2.loli.net/2023/04/23/imzqovVyWfnK3Fs.png" alt="rand_1_skin_rendered_half" /><figcaption>rand_1_skin_rendered_half</figcaption></figure><p>改进后，正常的效果应该是这样才对：</p><figure><img src="https://s2.loli.net/2023/04/23/gSJ5wm74RUjHODY.png" alt="rand_1_skin_rendered_full" /><figcaption>rand_1_skin_rendered_full</figcaption></figure><p>所以问题出在哪呢？的确是“更远的部分被截掉了”。我找到了<code>RasterizationSettings</code>里有这么一个相关的参数：</p><ul><li>z_clip_value: if not None, then triangles will be clipped (and possibly subdivided into smaller triangles) such that z &gt;= z_clip_value. This avoids camera projections that go to infinity as z-&gt;0. Default is None as clipping affects rasterization speed and should only be turned on if explicitly needed. See clip.py for all the extra computation that is required.</li></ul><p>可是问题不在这个参数上，因为它的默认值就是None，应该在后续都没有影响。</p><h4 id="解决-3">解决：</h4><p>经过仔细看源码，我发现问题出在<code>SoftPhongShader</code>……具体来说，在<code>shader.py</code> 第138-139行，<code>SoftPhongShader</code>的<code>forward</code>函数里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">znear = kwargs.get(<span class="string">&quot;znear&quot;</span>, <span class="built_in">getattr</span>(cameras, <span class="string">&quot;znear&quot;</span>, <span class="number">1.0</span>))</span><br><span class="line">zfar = kwargs.get(<span class="string">&quot;zfar&quot;</span>, <span class="built_in">getattr</span>(cameras, <span class="string">&quot;zfar&quot;</span>, <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><p>居然有一个默认的z范围[1,100]……………………所以其实是我的mesh的scale太大了，再加上相机的dist比较大，整个深度就超过zfar了。所以有两种方法，要么缩小一下mesh的尺度；要么不想改变原数据的话，在render的时候，把<code>znear</code> <code>zfar</code>参数额外传入，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">images = renderer(mesh, ..., znear=-<span class="number">2.0</span>, zfar=<span class="number">1000.0</span>)</span><br></pre></td></tr></table></figure><h3 id="没解决的问题pbrphysical-based-rendering">6. 没解决的问题：PBR（physical based rendering）</h3><p>我的数据中3D mesh的材质用了PBR（physical based rendering）。它提供三张贴图图像：diffuse map，specular map和normal map。</p><p>但是pytorch3d目前并不支持PBR inspired shading（see <a href="https://github.com/facebookresearch/pytorch3d/issues/174">issue</a>）。</p><p>所以目前我只能把diffuse map作为一般意义上的texture map，而忽略了specular map和normal map这两张图。</p><p>我不确定能不能自己实现这部分功能，比如自定义 <code>phong_shading</code>函数（参考<a href="https://github.com/facebookresearch/pytorch3d/issues/865">issue</a>）。但这有点超出我的能力范围和精力范围，所以暂时搁置了。如果能实现的话，PyTorch3D 似乎是欢迎contribution的（<a href="https://github.com/facebookresearch/pytorch3d/issues/174">issue</a>）</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;pytorch3d-rendering-的一些疑难杂症&quot;&gt;Pytorch3D Rendering 的一些疑难杂症&lt;/h1&gt;
&lt;p&gt;有哪些？&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;有了相机内参 K，而render又需要NDC坐标系，那要怎么定义相机？&lt;/li&gt;
&lt;li&gt;图像的黄蓝色反了？&lt;/li&gt;
&lt;li&gt;render 完的图像锯齿很严重？怎么抗锯齿（Antialiasing）？&lt;/li&gt;
&lt;li&gt;皮肤表面反光太强，光滑得像镜面一样，怎样更自然？&lt;/li&gt;
&lt;li&gt;怎么物体只剩半截，更远的部分似乎被截掉了？&lt;/li&gt;
&lt;li&gt;没解决的问题：PBR（physical based rendering）&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
    <category term="Pytorch3D" scheme="https://jyzhu.top/blog/tags/Pytorch3D/"/>
    
  </entry>
  
  <entry>
    <title>Camera projection with the pinhole model</title>
    <link href="https://jyzhu.top/blog/Camera-Model-Notes/"/>
    <id>https://jyzhu.top/blog/Camera-Model-Notes/</id>
    <published>2023-04-02T13:44:43.000Z</published>
    <updated>2023-04-02T15:47:48.447Z</updated>
    
    <content type="html"><![CDATA[<p>A camera is a mapping between the 3D world (object space) and a 2D image.</p><p>In general, the camera projection matrix P has 11 degrees of freedom: <span class="math display">\[P=K[R\ \ \ t]\]</span></p><table><thead><tr class="header"><th>Component</th><th># DOF</th><th>Elements</th><th>Known As</th></tr></thead><tbody><tr class="odd"><td>K</td><td>5</td><td><span class="math inline">\(f_x, f_y, s,p_x, p_y\)</span></td><td>Intrinsic Parameters; camera calibration matrix</td></tr><tr class="even"><td>R</td><td>3</td><td><span class="math inline">\(\alpha,\beta,\gamma\)</span></td><td>Extrinsic Parameters</td></tr><tr class="odd"><td>t (or <span class="math inline">\(\tilde{C}\)</span>)</td><td>3</td><td><span class="math inline">\((t_x,t_y,t_z)\)</span></td><td>Extrinsic Parameters</td></tr></tbody></table><p>3D world frame ----- R, t ----&gt; 3D camera frame ------ K -----&gt; 2D image</p><p>Explanation:</p><ul><li><p>P: Projective camera, maps 3D world points to 2D image points.</p></li><li><p>K: Camera calibration matrix, 3 x 3, <span class="math inline">\(x=K[I|0]X_{cam}\)</span>, given 3D points in camera coordinate frame <span class="math inline">\(X_{cam}\)</span>, we can project it into 2D points on image <span class="math inline">\(x\)</span>.</p><p><img src="https://s2.loli.net/2023/04/02/FEe9lM5t3TJKgyL.png" alt="K" style="zoom:50%;" /></p></li><li><p>R and t: Camera Rotation and Translation, rigid transformation. <span class="math inline">\(X_{cam}=( X,Y,Z,1)^T\)</span> is expressed in the camera coordinate frame. In general, 3D points are expressed in a different Euclidean coordinate frame, known as the <strong>world coordinate frame</strong>. The two frames are related via a rigid transformation (R, t).</p></li></ul><h3 id="some-other-terms-you-may-see">Some other terms you may see</h3><ul><li><p><strong>P</strong>: 3x4, homogeneous, camera projection matrix, <span class="math inline">\(P=diag(f,f,1)[I|0]\)</span>. P is K without considering <span class="math inline">\((x_{cam},y_{cam})\)</span> in the image. (In other words, it simplify <span class="math inline">\((p_x, p_y)=(0,0)\)</span>.</p><p><img src="https://s2.loli.net/2023/04/02/TlcW7QFhOAXK6sY.png" alt="P" style="zoom:50%;" /></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;A camera is a mapping between the 3D world (object space) and a 2D image.&lt;/p&gt;
&lt;p&gt;In general, the camera projection matrix P has 11 degrees of freedom: &lt;span class=&quot;math display&quot;&gt;\[
P=K[R\ \ \ t]
\]&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;# DOF&lt;/th&gt;
&lt;th&gt;Elements&lt;/th&gt;
&lt;th&gt;Known As&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;K&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;\(f_x, f_y, s,p_x, p_y\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Intrinsic Parameters; camera calibration matrix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;\(\alpha,\beta,\gamma\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Extrinsic Parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td&gt;t (or &lt;span class=&quot;math inline&quot;&gt;\(\tilde{C}\)&lt;/span&gt;)&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;&lt;span class=&quot;math inline&quot;&gt;\((t_x,t_y,t_z)\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;Extrinsic Parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;3D world frame ----- R, t ----&amp;gt; 3D camera frame ------ K -----&amp;gt; 2D image&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Configure Academic Page in Jekyll + Blog in Hexo together</title>
    <link href="https://jyzhu.top/blog/Configure-Academic-Page-in-Jekyll-Blog-in-Hexo-together/"/>
    <id>https://jyzhu.top/blog/Configure-Academic-Page-in-Jekyll-Blog-in-Hexo-together/</id>
    <published>2023-03-14T06:24:17.000Z</published>
    <updated>2023-04-02T16:12:37.901Z</updated>
    
    <content type="html"><![CDATA[<h2 id="difficult-situations">Difficult situations:</h2><ol type="1"><li>The Academic page is powered by <a href="http://jekyllrb.com/">Jekyll</a>, while the blog website is powered by Hexo.</li><li>And they are maintained in two separated repositories on Github.</li><li>Besides <code>[username].github.io</code>, I have a domain <code>jyzhu.top</code>, and want to use my custom domain.</li><li>All in all, I hope to visit the academic page is at <a href="jyzhu.top" class="uri">jyzhu.top</a>, while visit the blog is at <a href="jyzhu.top/blog" class="uri">jyzhu.top/blog</a>.</li></ol><h2 id="now-lets-configure.">Now let's configure.</h2><ol type="1"><li><p>Rename the blog repo as <code>blog</code>; rename the academic page repo as <code>[username].github.io</code>.</p></li><li><p>Edit the blog's Hexo config file:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">url:</span> <span class="string">https://jyzhu.top/blog</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/blog/</span></span><br></pre></td></tr></table></figure><p><em>While no need to move all the files into a subfolder <code>blog</code> of your repo.</em></p><p>The Jekyll config is simple. Nothing needs to specify.</p></li><li><p>Edit the Github repo settings. Set the academic repo's <strong>custom domain</strong> as <code>jyzhu.top</code>. A <code>CNAME</code> file will be automatically added in the root. Now obviously, the <code>jyzhu.top</code> successfully refers to the academic page.</p><p>Then you know what, everything is done! Because all other repos with github page turns on, are automatically mapped to subpaths of <code>[username].github.io</code> by Github. Then coz <code>[username].github.io</code> is mapped to <code>[url]</code>, everything will be there, including <code>[url]/blog</code> for the <code>blog</code> repo.</p></li></ol><h2 id="todo">TODO</h2><p>The <code>hexo-douban</code> plugin cannot render styles now. Need to fix.</p><p><em>Update</em>:</p><ol type="1"><li>updated hexo-douban to the latest version</li><li>edit the file path of <code>loading.gif</code> in the <code>index.js</code> of this plugin</li></ol><p>then it seems ok now.</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;difficult-situations&quot;&gt;Difficult situations:&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;The Academic page is powered by &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;, while the blog website is powered by Hexo.&lt;/li&gt;
&lt;li&gt;And they are maintained in two separated repositories on Github.&lt;/li&gt;
&lt;li&gt;Besides &lt;code&gt;[username].github.io&lt;/code&gt;, I have a domain &lt;code&gt;jyzhu.top&lt;/code&gt;, and want to use my custom domain.&lt;/li&gt;
&lt;li&gt;All in all, I hope to visit the academic page is at &lt;a href=&quot;jyzhu.top&quot; class=&quot;uri&quot;&gt;jyzhu.top&lt;/a&gt;, while visit the blog is at &lt;a href=&quot;jyzhu.top/blog&quot; class=&quot;uri&quot;&gt;jyzhu.top/blog&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;now-lets-configure.&quot;&gt;Now let&#39;s configure.&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;Rename the blog repo as &lt;code&gt;blog&lt;/code&gt;; rename the academic page repo as &lt;code&gt;[username].github.io&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edit the blog&#39;s Hexo config file:&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;url:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;https://jyzhu.top/blog&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;root:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;/blog/&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;While no need to move all the files into a subfolder &lt;code&gt;blog&lt;/code&gt; of your repo.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Jekyll config is simple. Nothing needs to specify.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Edit the Github repo settings. Set the academic repo&#39;s &lt;strong&gt;custom domain&lt;/strong&gt; as &lt;code&gt;jyzhu.top&lt;/code&gt;. A &lt;code&gt;CNAME&lt;/code&gt; file will be automatically added in the root. Now obviously, the &lt;code&gt;jyzhu.top&lt;/code&gt; successfully refers to the academic page.&lt;/p&gt;
&lt;p&gt;Then you know what, everything is done! Because all other repos with github page turns on, are automatically mapped to subpaths of &lt;code&gt;[username].github.io&lt;/code&gt; by Github. Then coz &lt;code&gt;[username].github.io&lt;/code&gt; is mapped to &lt;code&gt;[url]&lt;/code&gt;, everything will be there, including &lt;code&gt;[url]/blog&lt;/code&gt; for the &lt;code&gt;blog&lt;/code&gt; repo.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Blog" scheme="https://jyzhu.top/blog/tags/Blog/"/>
    
    <category term="Hexo" scheme="https://jyzhu.top/blog/tags/Hexo/"/>
    
    <category term="Jekyll" scheme="https://jyzhu.top/blog/tags/Jekyll/"/>
    
  </entry>
  
  <entry>
    <title>齐次坐标系</title>
    <link href="https://jyzhu.top/blog/homogeneous-coordinates/"/>
    <id>https://jyzhu.top/blog/homogeneous-coordinates/</id>
    <published>2023-01-26T10:49:19.000Z</published>
    <updated>2023-01-26T10:58:06.245Z</updated>
    
    <content type="html"><![CDATA[<h1 id="齐次坐标系">齐次坐标系</h1><p>之前不理解为什么要用一个和从小到大学的笛卡尔坐标系不同的齐次坐标系来表示东西，并且弄得很复杂；学了各种公式也很糊涂。现在终于明白了</p><h2 id="齐次坐标系的现实意义">齐次坐标系的现实意义</h2><p>就是用来表示现实世界中我们眼睛看到的样子：两条平行线在无限远处能相交。 <embed src="https://pic4.zhimg.com/80/v2-dc3c0223dd24d2084fcda13b2d0e60f3_1440w.webp" /></p><h2 id="齐次坐标系的本质">齐次坐标系的本质：</h2><p>就是用N+1维来代表N维坐标。</p><p>也就是说，原本二维空间的点<span class="math inline">\((X,Y)\)</span>，增加一个维度，用<span class="math inline">\((x,y,w)\)</span>来表示。把齐次坐标转换成笛卡尔坐标是很简单的，对前两个维度分别除以最后一个维度的值，就好了，即 <span class="math display">\[X=\frac x w,\  Y=\frac y w\\ (X,Y)=(\frac x w,\frac y w)\]</span> <embed src="https://pic2.zhimg.com/80/v2-da28eed57fda0fc6a06b7122be5f2a1d_1440w.webp" /></p><p>这样做就可以表示两条平行线在远处能相交了！why？</p><p>要解释这个，需要先解释一个齐次坐标系的特点：规模不变性（也是叫homogeneous这个名字的原因）。也就是说，对任意非零的k，<span class="math inline">\((x,y,w)\)</span>和<span class="math inline">\((kx,ky,kw)\)</span>都表示二维空间中同一个点<span class="math inline">\((\frac x w,\frac y w)\)</span>。（因为<span class="math inline">\(\frac{kx}{kw}=\frac xw\)</span>嘛。）</p><p>首先，用原本笛卡尔坐标系中的表示方法，无限远处的点会被表示成<span class="math inline">\((\infty,\infty)\)</span>，从而失去意义。但是我们发现用齐次坐标，我们就有了一个方法明确表示无限远处的任意点，即，<span class="math inline">\((x,y,0)\)</span>。（为什么？因为把它转换回笛卡尔坐标，会得到<span class="math inline">\((\frac x 0,\frac y 0)=(\infty,\infty)\)</span>）。</p><p>现在，用初中所学，联立两条直线的方程，得到的解是两条直线的交点。假如有两条平行线<span class="math inline">\(Ax+By+C=0\)</span>和<span class="math inline">\(Ax+By+D=0\)</span>，求交点，则 <span class="math display">\[\left\{\matrix{Ax+By+C=0 \\Ax+By+D=0}\right.\]</span> 在笛卡尔坐标系中，可知唯一解是<span class="math inline">\(C=D\)</span>，即两条线为同一条直线。</p><p>但是，如果把它换成齐次坐标，得到 <span class="math display">\[\left\{\matrix{A\frac x w+B\frac y w+C=0\\ A\frac x w + B\frac y w +D=0}\right.\]</span></p><p><span class="math display">\[\left\{\matrix{Ax+By+Cw=0\\Ax+By+Dw=0}\right.\]</span></p><p>当<span class="math inline">\(w=0\)</span>，上式变成<span class="math inline">\(Ax+By=0\)</span>，得到解<span class="math inline">\((x,-\frac {A}Bx,0)\)</span>。其实这里的x和y是什么不重要，重要的是w=0，意味着这是个无限远处的点。也就是说，两条平行线在无限远处相交了！甚至能明确求出交点！</p><blockquote><p>Reference:</p><p>http://www.songho.ca/math/homogeneous/homogeneous.html</p><p>https://zhuanlan.zhihu.com/p/373969867</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;齐次坐标系&quot;&gt;齐次坐标系&lt;/h1&gt;
&lt;p&gt;之前不理解为什么要用一个和从小到大学的笛卡尔坐标系不同的齐次坐标系来表示东西，并且弄得很复杂；学了各种公式也很糊涂。现在终于明白了&lt;/p&gt;
&lt;h2 id=&quot;齐次坐标系的现实意义&quot;&gt;齐次坐标系的现实意义&lt;/h2&gt;
&lt;p&gt;就是用来表示现实世界中我们眼睛看到的样子：两条平行线在无限远处能相交。 &lt;embed src=&quot;https://pic4.zhimg.com/80/v2-dc3c0223dd24d2084fcda13b2d0e60f3_1440w.webp&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3D Computer Vision" scheme="https://jyzhu.top/blog/tags/3D-Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to NeRF</title>
    <link href="https://jyzhu.top/blog/Introduction-to-NeRF/"/>
    <id>https://jyzhu.top/blog/Introduction-to-NeRF/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:21.514Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h1 id="introduction-to-nerf">1. Introduction to NeRF</h1><h2 id="what-is-nerf">What is NeRF</h2><blockquote><p>Reference: Original NeRF paper; an online ariticle</p></blockquote><p>在已知视角下对场景进行一系列的捕获 (包括拍摄到的图像，以及每张图像对应的内外参)，合成新视角下的图像。</p><p>NeRF 想做这样一件事，不需要中间三维重建的过程，仅根据位姿内参和图像，直接合成新视角下的图像。为此 NeRF 引入了辐射场的概念，这在图形学中是非常重要的概念，在此我们给出渲染方程的定义：</p><p><embed src="https://pic1.zhimg.com/80/v2-1a80de23a422688b739f36828affb8ec_1440w.webp" /></p><p><embed src="https://pic4.zhimg.com/80/v2-c469e4968a3e6cf8ec7a81f816de4f87_1440w.webp" /></p><p>那么辐射和颜色是什么关系呢？简单讲就是，光就是电磁辐射，或者说是振荡的电磁场，光又有波长和频率，<span class="math inline">\(波长\times 频率=光速\)</span>，光的颜色是由频率决定的，大多数光是不可见的，人眼可见的光谱称为可见光谱，对应的频率就是我们认为的颜色：</p><p><embed src="https://pic1.zhimg.com/80/v2-381aa740f21b7eba1f896fd98dcc1308_1440w.webp" /></p><p><embed src="https://pic1.zhimg.com/80/v2-51bd3710b9f891c4c44fde12545e4fd4_1440w.webp" /></p><h3 id="sdf---signed-distance-function">SDF - Signed Distance Function</h3><p>SDF是一种计算图形学中定义距离的函数。SDF定义了空间中的点到隐式曲面的距离，该点在曲面内外决定了其SDF的正负性。</p><p>相较于其他像点云（point cloud）、体素（voxel）、面云（mesh）那样的经典3D模型表示方法，SDF有固定的数学方程，更关注物体的表面信息，具有可控的计算成本。</p><h2 id="features-of-nerf">Features of NeRF</h2><ul><li>Representation can be discrete or continuous. but the discrete representation will be a big one if you have more dimensions, e.g., 3 dim.<ul><li>Actually the Plenoxels try to use 3D grids to store the fields. Fast, however, too much memory.</li></ul></li><li>Neural Field has advantages:<ol type="1"><li>Compactness 紧致:</li><li>Regularization: nn itself as inductive bias makes it easy to learn</li><li>Domain Agonostic: cheap to add a dimension</li></ol></li><li>also problems<ul><li>Editability / Manipulability</li><li>Computational Complexity</li><li>Spectral Bias</li></ul></li></ul><h2 id="problem-formulation">Problem Formulation</h2><ul><li>Input: multiview images</li><li>Output: 3D Geometry and appearance</li><li>Objective:</li></ul><p><span class="math display">\[\arg \min_x\|y-F(x)\|+\lambda P(x)\]</span></p><p>y is multiview images, F is forward mapping, x is the desired 3D reconstruction.</p><p>F can be differentiable, then you can supervise this.</p><ul><li>nn本身就是某种constraints，你就不需要加太多handicraft constraints</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&quot;introduction-to-nerf&quot;&gt;1. Introduction to NeRF&lt;/h1&gt;
&lt;h2 id=&quot;what-is-nerf&quot;&gt;What is NeRF&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Reference: Original NeRF paper; an online ariticle&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Learning NeRF</title>
    <link href="https://jyzhu.top/blog/Learning-NeRF/"/>
    <id>https://jyzhu.top/blog/Learning-NeRF/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:37.635Z</updated>
    
    <content type="html"><![CDATA[<h1 id="learning-nerf">Learning NeRF</h1><p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="reading-list">Reading List</h2><h3 id="classical">Classical</h3><ul><li><p>Mildenhall <em>et al.</em> introduced NeRF at ECCV 2020 in the now seminal <a href="https://www.matthewtancik.com/nerf">Neural Radiance Field paper</a>.</p><p>This is done by storing the density and radiance in a neural volumetric scene representation using MLPs and then rendering the volume to create new images.</p></li><li><p><a href="https://m-niemeyer.github.io/project-pages/giraffe/index.html">GIRAFFE</a>: Compositional Generative Neural Feature Fields</p></li></ul><h3 id="survey">Survey</h3><ul><li><a href="https://arxiv.org/abs/2004.03805">Apr 2020 - State of the Art on Neural Rendering</a></li></ul><h3 id="cvpr">2021CVPR</h3><p>2021年CVPR还有许多相关的精彩工作发表。例如，提升网络的泛化性：</p><ul><li><a href="https://alexyu.net/pixelnerf/">pixelNeRF</a>：将每个像素的特征向量而非像素本身作为输入，允许网络在不同场景的多视图图像上进行训练，学习场景先验，然后测试时直接接收一个或几个视图为输入合成新视图。</li><li><a href="https://ibrnet.github.io/">IBRNet</a>：学习一个适用于多种场景的通用视图插值函数，从而不用为每个新的场景都新学习一个模型才能渲染；且网络结构上用了另一个时髦的东西 Transformer。</li><li><a href="https://apchenstu.github.io/mvsnerf/">MVSNeRF</a>：训练一个具有泛化性能的先验网络，在推理的时候只用3张输入图片就重建一个新的场景。</li></ul><p>针对动态场景的NeRF:</p><ul><li><a href="https://nerfies.github.io/">Nerfies</a>：多使用了一个多层感知机来拟合形变的SE(3) field，从而建模帧间场景形变。Nerfies: Deformable Neural Radiance Fields</li><li><a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a>：多使用了一个多层感知机来拟合场景形变的displacement。</li><li><a href="https://link.zhihu.com/?target=http%3A//www.cs.cornell.edu/~zl548/NSFF/">Neural Scene Flow Fields</a>：多提出了一个scene flow fields来描述时序的场景形变。</li></ul><p>其他创新点：</p><ul><li><a href="https://kai-46.github.io/PhySG-website/">PhySG</a>：用球状高斯函数模拟BRDF（高级着色的上古神器）和环境光照，针对更复杂的光照环境，能处理非朗伯表面的反射。</li><li><a href="https://nex-mpi.github.io/">NeX</a>：用MPI（Multi-Plane Image ）代替NeRF的RGBσ作为网络的输出。</li></ul><h3 id="cvpr-1">2022 CVPR</h3><p><a href="https://ajayj.com/dreamfields">Zero-Shot Text-Guided Object Generation with <strong>Dream Fields</strong></a></p><h2 id="useful-references"><strong>Useful References:</strong></h2><blockquote><p><a href="https://markboss.me/post/nerf_at_eccv22/?continueFlag=55ed0f6189bcd6ca987e08764bcbe945">NeRF at ECCV22 - Mark Boss</a></p><p><a href="https://markboss.me/post/nerf_at_neurips22/">NeRF at NeurIPS 2022 - Mark Boss</a></p><p><a href="https://dellaert.github.io/NeRF22/">NeRF at CVPR 2022 - Frank Dellaert</a></p><p><a href="https://youtu.be/PeRRp1cFuH4">CVPR 2022 Tutorial on Neural Fields in Computer Vision</a></p></blockquote><p>Bigger to learn:</p><ul><li>[ ] Above NeRF: neural rendering</li><li>[ ] Related theories in graphics and computer vision</li><li>[ ] NeRF的一作Ben Mildenhall在SIGGRAPH 2021 Course <a href="https://www.youtube.com/watch%3Fv%3Dotly9jcZ0Jg">Advances in Neural Rendering</a>中从概率的角度推导了NeRF的体渲染公式。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;learning-nerf&quot;&gt;Learning NeRF&lt;/h1&gt;
&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;reading-list&quot;&gt;Reading List&lt;/h2&gt;
&lt;h3 id=&quot;classical&quot;&gt;Classical&lt;/h3&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Manipulate Neural Fields</title>
    <link href="https://jyzhu.top/blog/Manipulate-Neural-Fields/"/>
    <id>https://jyzhu.top/blog/Manipulate-Neural-Fields/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:11:38.197Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="manipulate-neural-fields">2.5. Manipulate Neural Fields</h2><p>Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.</p><figure><img src="https://s2.loli.net/2023/01/12/okLGyeFmMvifhZW.png" alt="image-20221212211525928" /><figcaption>image-20221212211525928</figcaption></figure><p>You can either edit the input coordinates, or edit the parameters <span class="math inline">\(\theta\)</span>.</p><p>On the other axis, you can edit through an explicit geometry, or an implicit neural fields.</p><figure><img src="https://s2.loli.net/2023/01/12/S7HWcQPh1FtdJaw.png" alt="image-20221212213802209" /><figcaption>image-20221212213802209</figcaption></figure><p>The following examples 落在不同的象限。</p><h3 id="editing-the-input-via-explicit-geometry-left-up">Editing the input via Explicit geometry (left-up)</h3><ul><li><p>You can represent each object using a separated neural field (local frame), and then compose them together in different ways.</p></li><li><p>If you want to manipulate not only spatially, but also <strong>temporaly</strong>, it is also possible. You can add a time coordinate as the input of the neural field network, and transform the time input.</p></li><li><p>You can also manipulate (especially human body) via <strong>skeleton</strong>.</p><figure><img src="https://s2.loli.net/2023/01/12/y4bGulHpfOwWkqN.png" alt="image-20221212212838893" /><figcaption>image-20221212212838893</figcaption></figure><ul><li><p><strong>Beyond human</strong>, we can also first estimate different moving parts of an object, to form some skeleton structure, and then do the same.</p><figure><img src="https://s2.loli.net/2023/01/12/SBzGy3rnUaqLFI8.png" alt="Noguchi etal, CVPR22" /><figcaption>Noguchi etal, CVPR22</figcaption></figure></li></ul></li><li><p>Beyond rigid, we can also manipulate via <strong>mesh</strong>. coz we have plenty of manipulation tools on mesh. The deformation on mesh can be re-mapped as the deformation on the input coordinate</p><figure><img src="https://s2.loli.net/2023/01/12/UbFu74iCQ15mK3B.png" alt="image-20221212213601773" /><figcaption>image-20221212213601773</figcaption></figure></li></ul><h3 id="editing-the-input-via-neural-flow-fields-left-down">Editing the input via Neural Flow Fields (left-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/zxFElDIuSnPioJ7.png" alt="image-20230104183222294" /><figcaption>image-20230104183222294</figcaption></figure><p>We use the <span class="math inline">\(f_{i\rightarrow j}\)</span> to edit the <span class="math inline">\(r_{i\rightarrow j}\)</span> to represent one ray into another one.</p><p>We need to define the consistency here, so that the network can learn through forward and backward:</p><figure><img src="https://s2.loli.net/2023/01/12/VS1K3rQxHXYPRIg.png" alt="image-20230104183453487" /><figcaption>image-20230104183453487</figcaption></figure><h3 id="editing-network-parameters-via-explicit-geometry-right-up">Editing network parameters via Explicit geometry (right-up)</h3><p>The knowledge is already in the network. So instead of editing the inputs, we can directly edit the network parameters for generating new things.</p><figure><img src="https://s2.loli.net/2023/01/12/w2XEyYn5qbh41OB.png" alt="image-20230104185014312" /><figcaption>image-20230104185014312</figcaption></figure><ul><li>This proposed solution makes use of an encoder. The encoder learns to represent the rotated input as a high-dimensional latent code Z, with the same rotation R, in 3-dim space. The the following network use the latent code to generate the <span class="math inline">\(f_\theta\)</span></li></ul><figure><img src="https://s2.loli.net/2023/01/12/ruER6MJPpljSZiX.png" alt="image-20230104185544623" /><figcaption>image-20230104185544623</figcaption></figure><ul><li>In this work, the key idea is to map the high-resolutional object and the similar but lower resolutional object into the same latent space. Then, you can easily manipulate the lower resolutional object, and it should also affect the higher resolutional one. Then, the shared latent space are put into the following neural field network, which outputs high resolutional results.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/HL61itcqsIaEThX.png" alt="image-20230104202425695" /><figcaption>image-20230104202425695</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/y7eCcKDmdUY4VOu.png" alt="image-20230104202625346" /><figcaption>image-20230104202625346</figcaption></figure><ul><li>This work (Yang et al. NeurlPS'21) about shape editing is &quot;super important&quot; but the speaker does not have enough time... Basically it shows that the tools that we use to manipulate a mesh can also be used on a neural field, where we can keep some of the network parameters to make sure the basic shape of the object the same, and then the magical thing is the &quot;curvature manipulation&quot; item. Given the neural field is differentiable, this can be achieved.</li></ul><figure><img src="https://s2.loli.net/2023/01/12/2lTvenQixfRm8Po.png" alt="image-20230104203311551" /><figcaption>image-20230104203311551</figcaption></figure><ul><li>Obeying the points (a.k.a generalization). It makes sure the manipulation done on the input points are reconstructed.</li></ul><h3 id="editing-network-parameters-via-neural-fields-right-down">Editing network parameters via Neural Fields (right-down)</h3><figure><img src="https://s2.loli.net/2023/01/12/5Ohb7ExW4osc1n2.png" alt="image-20230104204330741" /><figcaption>image-20230104204330741</figcaption></figure><ul><li>This work constructs a reasonable latent space of the object, then do interpolation of different objects.</li><li>Beyond geometry, we can also manipulate <strong>color</strong></li></ul><figure><img src="https://s2.loli.net/2023/01/12/vM1QwkT4BJqGNIR.png" alt="image-20230104204738067" /><figcaption>image-20230104204738067</figcaption></figure><p>It decomposes the network into shape and color networks, and we can edit each independently.</p><figure><img src="https://s2.loli.net/2023/01/12/38HNsE9GnF1pydQ.png" alt="image-20230104204937204" /><figcaption>image-20230104204937204</figcaption></figure><ul><li>This is the stylization work. It mainly depends on a different loss function, which does not search for the exact feature of the vgg, but somehow the nearest neighbor.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;manipulate-neural-fields&quot;&gt;2.5. Manipulate Neural Fields&lt;/h2&gt;
&lt;p&gt;Neural fields is ready to be a prime representation, similar as point clouds or meshes, that is able to be manipulated.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/okLGyeFmMvifhZW.png&quot; alt=&quot;image-20221212211525928&quot;&gt;&lt;figcaption&gt;image-20221212211525928&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Differentiable Forward Maps</title>
    <link href="https://jyzhu.top/blog/NeRF-Differentiable-Forward-Maps/"/>
    <id>https://jyzhu.top/blog/NeRF-Differentiable-Forward-Maps/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:34.505Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="differentiable-forward-maps">2.3. Differentiable Forward Maps</h2><figure><img src="https://s2.loli.net/2023/01/12/xp4avlLcJDI9R2k.png" alt="image-20221208175453557" /><figcaption>image-20221208175453557</figcaption></figure><h3 id="differentiable-rendering">Differentiable rendering</h3><figure><img src="https://s2.loli.net/2023/01/12/1Ng8wz2KP4oTiVH.png" alt="image-20221208181457315" /><figcaption>image-20221208181457315</figcaption></figure><p>Volume rendering can render fogs. Sphere rendering only render the solid surface, and needs ground truth supervision.? Neural renderer combines the two.</p><h3 id="differentiability-of-the-rendering-function-itself">Differentiability of the rendering function itself</h3><ul><li>BRDF Shading? details later.</li></ul><h3 id="differentiation-itself">Differentiation itself</h3><p>Design a neural network with higher order derivatives constraints and therefore directly use its derivative.</p><figure><img src="https://s2.loli.net/2023/01/12/Gi6IaAkhvlBxpoe.png" alt="image-20221208182302568" /><figcaption>image-20221208182302568</figcaption></figure><p>For example the Eikonal equation forces the neural network has a derivative as 1. Adding the eikonal loss then promises the neural network valid.</p><p>Generally, this kind of problems are: the solutions are constrained by its partial derivatives.</p><h3 id="special-identity-operator">Special: Identity Operator</h3><p><span class="math display">\[\text{Reconstruction} \rightarrow \hat 1()\rightarrow \text{Sensor domain}\\\text{Reconstruction} == \text{Sensor domain}\]</span></p><p>Q&amp;A:</p><ul><li>Can we obtain a neural network in just one forward, without optimization?</li><li>Can we design special forward maps for specific downstream tasks, eg., classification? Absolutely yes. We can design it to represent a compact representation as the sensor domain. The key idea is to get a differentiable function to map your specific recon and sensor domain.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;differentiable-forward-maps&quot;&gt;2.3. Differentiable Forward Maps&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/xp4avlLcJDI9R2k.png&quot; alt=&quot;image-20221208175453557&quot;&gt;&lt;figcaption&gt;image-20221208175453557&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;differentiable-rendering&quot;&gt;Differentiable rendering&lt;/h3&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Hybrid representations</title>
    <link href="https://jyzhu.top/blog/NeRF-Hybrid-representations/"/>
    <id>https://jyzhu.top/blog/NeRF-Hybrid-representations/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:55.420Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="hybrid-representations">2.2. Hybrid representations</h2><h3 id="tradeoffs-of-choosing-a-proper-representation">Tradeoffs of choosing a proper representation</h3><figure><img src="https://s2.loli.net/2023/01/12/NyvS91xlLJA8KWR.png" alt="image-20221208172055153" /><figcaption>image-20221208172055153</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/sDg8FHQcjrGRW1b.png" alt="image-20221208172209556" /><figcaption>image-20221208172209556</figcaption></figure><p>You may choose one proper representation depending on your own application</p><h3 id="grid">1. Grid</h3><figure><img src="https://s2.loli.net/2023/01/12/ZpYEMbXvRdeOqiy.png" alt="image-20221205195659841" /><figcaption>image-20221205195659841</figcaption></figure><p>Input is too huge. Then you need too huge neural network. So, this grid interpolation acts like a &quot;position encoding&quot;, which encodes the low dimensional features into high dims.</p><figure><img src="https://s2.loli.net/2023/01/12/lMi3tK9NPgcWUaI.png" alt="image-20221208162026398" /><figcaption>image-20221208162026398</figcaption></figure><p>NeRFusion CVPR22: online!</p><h3 id="point-cloud">2. point cloud</h3><figure><img src="https://s2.loli.net/2023/01/12/zhYHQnFsgxBLCfM.png" alt="image-20221208162541770" /><figcaption>image-20221208162541770</figcaption></figure><p>Cons:</p><ol type="1"><li>To access local points, you need to specifically design the data structure. Otherwise, it is O(n)!</li><li>Choose different kernels to retrieve nearby points' features. Oftentimes you assume it is local kernel.</li></ol><p><img src="https://s2.loli.net/2023/01/12/37EbFsANOCc9voX.png" alt="image-20221208163050867" style="zoom:50%;" /></p><h3 id="mesh">3. Mesh</h3><p>Unstructed grids. Compared with point clouds, meshes have connectivity info.</p><figure><img src="https://s2.loli.net/2023/01/12/Dwir9hsm3VgZjQk.png" alt="image-20221208163526289" /><figcaption>image-20221208163526289</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/nPMw7T3hqRAU2iv.png" alt="image-20221208163746237" /><figcaption>image-20221208163746237</figcaption></figure><h3 id="multiplanar-images">4. Multiplanar Images</h3><p>Something like project a 3D grid into an axis to get levels of planes.</p><figure><img src="https://s2.loli.net/2023/01/12/CmhFDT5NoiMAOnv.png" alt="image-20221208164038729" /><figcaption>image-20221208164038729</figcaption></figure><p>Pros:</p><ol type="1"><li>Compact</li><li>Very efficient because the hardware and software designs are accelerated to these 2D operations, like bi-linear operations.</li></ol><p>Cons:</p><ol type="1"><li>Resolution bias on plane axis: coz it is discrete betweens planes.</li></ol><p>This is not very wise in my opinion. It is just a temporary tradeoff given nowadays' technologies. Coz everything will be 3D in the future.</p><p><img src="https://s2.loli.net/2023/01/12/UDO6HlWAp3y7qF1.png" alt="image-20221208165534056" />Generate 2D images from different camera views (perhaps). Key point is the tri-plane representation of 3D features.</p><h3 id="multiresolution-grids">5. Multiresolution grids</h3><figure><img src="https://s2.loli.net/2023/01/12/TlmkK1NDj2dAtUp.png" alt="image-20221208165714329" /><figcaption>image-20221208165714329</figcaption></figure><p>Pros:</p><ol start="2" type="1"><li>Stable coz you indeed need both low and high resolution info</li></ol><h3 id="hash-grids">6. Hash grids</h3><p><img src="https://s2.loli.net/2023/01/12/NZtH7wfxpSPVkGe.png" alt="image-20221208170131069" /> <span class="math display">\[[x,y,z]\text{ coordinates}\rightarrow \text{Hash function()} \rightarrow \text{Fixed size codebook}\]</span> Pros:</p><ol type="1"><li>No matter how big is the original data, you can use a fixed size codebook as the input feature.</li><li>Can be online!</li></ol><p>Cons:</p><ol type="1"><li>May still need large codebooks</li><li>Features not spatially local. I don't think the hash grid is a good idea if this drawback exists. But isn't there a simple way to generate features with local info remaining?</li></ol><h3 id="codebook-grids">7. Codebook grids</h3><figure><img src="https://s2.loli.net/2023/01/12/pnBJGxM6jNHD2v5.png" alt="image-20221208170955887" /><figcaption>image-20221208170955887</figcaption></figure><p>Instead of storing features of points in grids, store a (index to a) code in a codebook. The size of the codebook is fixed, so the overall size can be controlled as much smaller.</p><p>cons:</p><ol type="1"><li>To make the indexing operation differentiable, the computing complexity rises here.</li><li>Using hash is to get rid of the complex data structure, but the indices bring it back.</li></ol><h3 id="bounding-volume-hierarchies">8. Bounding Volume Hierarchies</h3><figure><img src="https://s2.loli.net/2023/01/12/LQ6fzh21OltTJxq.png" alt="image-20221208171806113" /><figcaption>image-20221208171806113</figcaption></figure><p>Commonly used method in computer graphics</p><h3 id="others-voxel">9. Others (voxel)</h3><figure><img src="https://s2.loli.net/2023/01/12/5sE1MYpOLB4mPCF.png" alt="image-20221208173124734" /><figcaption>image-20221208173124734</figcaption></figure><ul><li>For dynamic nerfs, is there any better hybrid representation? Sure.</li><li>Is there any explicit bias of these hybird representations that we can discover and then design regularization? Sure.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;hybrid-representations&quot;&gt;2.2. Hybrid representations&lt;/h2&gt;
&lt;h3 id=&quot;tradeoffs-of-choosing-a-proper-representation&quot;&gt;Tradeoffs of choosing a proper representation&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/NyvS91xlLJA8KWR.png&quot; alt=&quot;image-20221208172055153&quot;&gt;&lt;figcaption&gt;image-20221208172055153&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Network Architecture</title>
    <link href="https://jyzhu.top/blog/NeRF-Network-Architecture/"/>
    <id>https://jyzhu.top/blog/NeRF-Network-Architecture/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:14:07.993Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="network-architecture">2.1. Network Architecture</h2><h3 id="input-encoding">1. Input Encoding</h3><p>Similar as NLP, they use position encodings. Like Sinusoid functions. I also remember an encoding method which takes into consider of the 光线的散射</p><h3 id="activation-functions">2. Activation functions</h3><p>ReLU is not perfect for this task. Because it 不能解决对高阶导有constraints的函数。</p><p>SIREN is a replacement.</p><h3 id="symmetry-invariance-equivariance">3. Symmetry, Invariance &amp; Equivariance</h3><figure><img src="https://s2.loli.net/2023/01/12/tynBhWCulMINrU3.png" alt="image-20221205193423558" /><figcaption>image-20221205193423558</figcaption></figure><figure><img src="https://s2.loli.net/2023/01/12/Rjx5h2kfY34JZct.png" alt="image-20221205193614341" /><figcaption>image-20221205193614341</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;network-architecture&quot;&gt;2.1. Network Architecture&lt;/h2&gt;
&lt;h3 id=&quot;input-encoding&quot;&gt;1. Input Encoding&lt;/h3&gt;
&lt;p&gt;Similar as NLP, they use position encodings. Like Sinusoid functions. I also remember an encoding method which takes into consider of the 光线的散射&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF Notebook</title>
    <link href="https://jyzhu.top/blog/NeRF-Notebook/"/>
    <id>https://jyzhu.top/blog/NeRF-Notebook/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:36:59.994Z</updated>
    
    <content type="html"><![CDATA[<p>I am currently burying myself into the sea of NeRF. I plan to archive my learning notes here. I am still a beginner, so the notes absolutely contain errors, and are not finished yet.</p><h1 id="contents">Contents</h1><p><a href="https://jyzhu.top/Learning-NeRF/">Learning NeRF</a>: Reading list, learning references, and plans</p><p><strong>Notes of CVPR22' Tutorial:</strong></p><p><a href="https://jyzhu.top/Introduction-to-NeRF/">1. Introduction to NeRF</a>: What is NeRF and its features</p><p>2. Techniques</p><p>​ <a href="https://jyzhu.top/NeRF-Network-Architecture/">2.1. Network Architecture</a></p><p>​ <a href="https://jyzhu.top/NeRF-Hybrid-representations/">2.2. Hybrid representations</a></p><p>​ <a href="https://jyzhu.top/NeRF-Differentiable-Forward-Maps/">2.3. Differentiable Forward Maps</a></p><p>​ <a href="https://jyzhu.top/Prior-based-reconstruction-of-neural-fields/">2.4. Prior-based reconstruction of neural fields</a></p><p>​ <a href="https://jyzhu.top/Manipulate-Neural-Fields/">2.5. Manipulate Neural Fields</a></p><p>3. Applications</p><p><em>TBC</em></p><p><strong>Notes of paper reading</strong>:</p><p><a href="https://jyzhu.top/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/">Reading NeuMan: Neural Human Radiance Field from a Single Video</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;I am currently burying myself into the sea of NeRF. I plan to archive my learning notes here. I am still a beginner, so the notes absolutely contain errors, and are not finished yet.&lt;/p&gt;
&lt;h1 id=&quot;contents&quot;&gt;Contents&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://jyzhu.top/Learning-NeRF/&quot;&gt;Learning NeRF&lt;/a&gt;: Reading list, learning references, and plans&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notes of CVPR22&#39; Tutorial:&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Prior based reconstruction of neural fields</title>
    <link href="https://jyzhu.top/blog/Prior-based-reconstruction-of-neural-fields/"/>
    <id>https://jyzhu.top/blog/Prior-based-reconstruction-of-neural-fields/</id>
    <published>2023-01-12T09:33:39.000Z</published>
    <updated>2023-01-12T10:13:12.867Z</updated>
    
    <content type="html"><![CDATA[<p>This is part of <a href="https://jyzhu.top/NeRF-Notebook/">my journey of learning NeRF</a>.</p><h2 id="prior-based-reconstruction-of-neural-fields">2.4. Prior-based reconstruction of neural fields</h2><p>Sounds like a one-shot task: instead of fitting and optimizing a neural field each for one scene; let's learn a prior distribution of neural field. Then, given a specific scene, it adjusts the neural field in just one forward.</p><figure><img src="https://s2.loli.net/2023/01/12/rMOwKRJBuedFm1n.png" alt="image-20221211234430727" /><figcaption>image-20221211234430727</figcaption></figure><h3 id="how-does-the-latent-code-look-like">How does the latent code look like?</h3><figure><img src="https://s2.loli.net/2023/01/12/A41EOlYCBXrUxto.png" alt="image-20221211234923290" /><figcaption>image-20221211234923290</figcaption></figure><ul><li>Global: not local. A small latent code represents a neural field<ul><li>main limitation: can only represent very simple (single) object. coz if you have multiple objects in a scene, the degree of freedom grows non-linearly.</li><li><strong>How about giving the natural language descriptions as conditions???</strong></li></ul></li><li>Local: you get different latent codes considering the locality where you are. So, you have a prior 3D data structure to store the latent codes.<ul><li>3D point clouds -&gt; grids -&gt; triplanes interpolation</li></ul></li></ul><blockquote><p>Convolutional Occupancy Networks</p></blockquote><h3 id="autodecoder-instead-of-encoder-decoder">Autodecoder instead of Encoder-decoder</h3><figure><img src="https://s2.loli.net/2023/01/12/zr2GnpBXeZDJUqa.png" alt="image-20221212005012783" /><figcaption>image-20221212005012783</figcaption></figure><ul><li><p>Encoder is a 2D CNN structure.</p></li><li><p>But while using autodecoder, the backpropogate through the forward map (i.e., the neural renderer) will give the 3D structural information to the latent codes directly. <span class="math display">\[\text{latent code }\hat z=\arg \min_z \|\text{Render(}\Phi)-g.t.\|\]</span> <img src="https://s2.loli.net/2023/01/12/FQjMgibmNtIP4ca.png" alt="image-20221212004946040" /></p></li></ul><p><strong>Instead of trying to build the encoder, sometimes just use the backpropogation through the forward map is helpful.</strong></p><h3 id="light-field-networks----dont-need-to-render-anymore">Light field networks -- Don't need to render anymore</h3><figure><img src="https://s2.loli.net/2023/01/12/FVP15fmBYz8rEoe.png" alt="image-20221212005908926" /><figcaption>image-20221212005908926</figcaption></figure><p>Instead of learning a NeRF that you use a neural renderer to generate all points along a ray; you can learn a network to directly give you a color along a ray. So you do not use a 3d coordinate as the query, instead, use a ray.</p><p>But this do not work in complicated task yet.</p><figure><img src="https://s2.loli.net/2023/01/12/mAFx6pzqaIuUMin.png" alt="image-20221212010316991" /><figcaption>image-20221212010316991</figcaption></figure><h3 id="outlook">Outlook</h3><ul><li>You don't need to use 600 images of a scene to reconstruct it. Synthesis images?</li><li>Open minds: other ways to skip the expensive forward map? (e.g., the light field)</li><li>Understanding the scene like humans do: disentangle different objects</li><li>Local conditioning methods? Regular grids are easy to tackle with, but it's harder for point clouds / factorized representations</li><li>Transformers: seems like local conditioning</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is part of &lt;a href=&quot;https://jyzhu.top/NeRF-Notebook/&quot;&gt;my journey of learning NeRF&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;prior-based-reconstruction-of-neural-fields&quot;&gt;2.4. Prior-based reconstruction of neural fields&lt;/h2&gt;
&lt;p&gt;Sounds like a one-shot task: instead of fitting and optimizing a neural field each for one scene; let&#39;s learn a prior distribution of neural field. Then, given a specific scene, it adjusts the neural field in just one forward.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://s2.loli.net/2023/01/12/rMOwKRJBuedFm1n.png&quot; alt=&quot;image-20221211234430727&quot;&gt;&lt;figcaption&gt;image-20221211234430727&lt;/figcaption&gt;
&lt;/figure&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="NeRF" scheme="https://jyzhu.top/blog/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>The difference between RNN&#39;s output and h_n</title>
    <link href="https://jyzhu.top/blog/The-difference-between-RNN-s-output-and-h-n/"/>
    <id>https://jyzhu.top/blog/The-difference-between-RNN-s-output-and-h-n/</id>
    <published>2022-10-22T08:56:44.000Z</published>
    <updated>2022-10-22T09:12:01.123Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Reference: <a href="https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm" class="uri">https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm</a></p></blockquote><p>I was so confused when doing a homework on implementing the Luong Attention, because it tells that the decoder is a RNN, which takes <span class="math inline">\(y_{t-1}\)</span> and <span class="math inline">\(s_{t-1}\)</span> as input, and outputs <span class="math inline">\(s_t\)</span>, i.e., <span class="math inline">\(s_t = RNN(y_{t-1}, s_{t-1})\)</span>.</p><p>But the pytorch implementation of RNN is: <span class="math inline">\(outputs, hidden\_last = RNN(inputs, hidden\_init)\)</span>, which takes in a sequence of elements, computes in serials, and outputs a sequence also.</p><p>I was confused about what is the <span class="math inline">\(s_t\)</span>. Is it the <span class="math inline">\(outputs\)</span>, or the <span class="math inline">\(hidden\_states\)</span>?</p><p>This is the very helpful picture:</p><p><img src="https://i.stack.imgur.com/SjnTl.png" /></p><p>The <span class="math inline">\(output\)</span> here is the <span class="math inline">\(hidden\_states\)</span> of the last layer among all elements in the sequence (time steps), while the <span class="math inline">\(h_n,c_n = hidden\_last\)</span> is the <span class="math inline">\(hidden\_states\)</span> of the last time step among all layers.</p><p>The former is the <span class="math inline">\(H\)</span>, hidden state collection, which can be used in subsequent calculations, like attentions or scores; and the latter is the hidden state that can be directly used in the next iteration.</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a href=&quot;https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm&quot; class=&quot;uri&quot;&gt;https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was so confused when doing a homework on implementing the Luong Attention, because it tells that the decoder is a RNN, which takes &lt;span class=&quot;math inline&quot;&gt;\(y_{t-1}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(s_{t-1}\)&lt;/span&gt; as input, and outputs &lt;span class=&quot;math inline&quot;&gt;\(s_t\)&lt;/span&gt;, i.e., &lt;span class=&quot;math inline&quot;&gt;\(s_t = RNN(y_{t-1}, s_{t-1})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But the pytorch implementation of RNN is: &lt;span class=&quot;math inline&quot;&gt;\(outputs, hidden\_last = RNN(inputs, hidden\_init)\)&lt;/span&gt;, which takes in a sequence of elements, computes in serials, and outputs a sequence also.&lt;/p&gt;
&lt;p&gt;I was confused about what is the &lt;span class=&quot;math inline&quot;&gt;\(s_t\)&lt;/span&gt;. Is it the &lt;span class=&quot;math inline&quot;&gt;\(outputs\)&lt;/span&gt;, or the &lt;span class=&quot;math inline&quot;&gt;\(hidden\_states\)&lt;/span&gt;?&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Deep Learning" scheme="https://jyzhu.top/blog/tags/Deep-Learning/"/>
    
    <category term="Neural Network" scheme="https://jyzhu.top/blog/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Reading 3D Photography using Context-aware Layered Depth Inpainting</title>
    <link href="https://jyzhu.top/blog/Reading-3D-Photography-using-Context-aware-Layered-Depth-Inpainting/"/>
    <id>https://jyzhu.top/blog/Reading-3D-Photography-using-Context-aware-Layered-Depth-Inpainting/</id>
    <published>2022-10-07T00:35:43.000Z</published>
    <updated>2022-10-22T09:15:26.167Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://shihmengli.github.io/3D-Photo-Inpainting" class="uri">https://shihmengli.github.io/3D-Photo-Inpainting</a></p><p>作者：<a href="https://shihmengli.github.io/">Meng-Li Shih</a>, <a href="https://lemonatsu.github.io/">Shih-Yang Su</a>, <a href="https://johanneskopf.de/">Johannes Kopf</a>, <a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a></p><p>发表： CVPR2020</p><p>链接： <a href="https://github.com/vt-vl-lab/3d-photo-inpainting" class="uri">https://github.com/vt-vl-lab/3d-photo-inpainting</a></p><hr /><h2 id="why">Why：</h2><p>之前的照片3D化的方法，会在视角变动后出现的新像素区域 填充很模糊的背景；这个方法主要是用inpainting的方法提高新背景生成的效果</p><figure><img src="https://s2.loli.net/2022/10/22/dfjuy9vNrBak1oP.png" alt="image-20221007084129382" /><figcaption>image-20221007084129382</figcaption></figure><h2 id="what">What：</h2><ol type="1"><li>任务是3D photography，图像3D化，把一张2D+深度信息的RGB-D图像转化成3D风格的图像。</li><li>现在的多镜头智能手机拍的照片都能提供深度信息。没有的话，也能用其他模型预测深度。</li><li>用分层深度图像（Layered Depth Image）来表示图像：能显式地表示像素点之间的连通性。和普通的2D图像相比，可以把像素点分成多层来表示，同一个坐标处可以有重合的不同层次的像素点。</li><li>提出一个基于学习的 inpainting 方法填充重叠区域的像素，让3D图像视角变化的时候出现的新背景效果很好。</li></ol><h2 id="how">How：</h2><p>是一个很清晰的流程：</p><ol type="1"><li><p>输入为单张RGB-D图像。D为depth，一般多镜头智能手机拍摄的照片都能提供深度信息；没有的话就用其他模型预测深度，比如MegaDepth, MiDas, and Kinect depth sensor</p></li><li><p>将输入图像转化成分层深度图像（Layered Depth Image）。LDI中的每个像素点保存颜色和深度信息，以及上下左右四个方向的邻居像素点。同一个坐标处可以有重合的不同深度的像素点。</p></li><li><p>图像预处理：检测深度不连贯的边缘</p><figure><img src="https://s2.loli.net/2022/10/22/6tjCgUHTpVyIAFN.png" alt="image-20221007090910332" /><figcaption>image-20221007090910332</figcaption></figure><p>用filter把深度边缘过滤得更锐利，然后清理一些不连贯的边缘，最后根据连通性划分不同的深度边（如图2 （f）中，不同颜色表示不同深度边）。</p></li><li><p>对于每一个深度边，把LDI图中的像素点切割开，并在背景层扩展一些像素点，对扩展区域进行生成</p><figure><img src="https://s2.loli.net/2022/10/22/Ap62PxdYEDKzBkJ.png" alt="image-20221007091217942" /><figcaption>image-20221007091217942</figcaption></figure><ol type="1"><li><p>找到一个深度边，把两层的像素点切割开</p></li><li><p>对于背景层，用flood-fill like算法迭代地选取一定的已知区域作为context region，以及扩展一定的未知区域作为synthesis region</p></li><li><p>利用已知context region 生成未知synthesis region 的深度和颜色：采用基于学习的inpainting方法</p><figure><img src="https://s2.loli.net/2022/10/22/h8uGZm4XAEcL5SP.png" alt="image-20221007091716266" /><figcaption>image-20221007091716266</figcaption></figure><p>这个方法中，最关键的就是在预测color和depth之前，先预测了一下depth edges，然后把这个edges信息加进去，可以更好地预测color和depth。</p></li><li><p>将生成完毕的像素融合回LDI图像</p></li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：&lt;a href=&quot;https://shihmengli.github.io/3D-Photo-Inpainting&quot; class=&quot;uri&quot;&gt;https://shihmengli.github.io/3D-Photo-Inpainting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://shihmengli.github.io/&quot;&gt;Meng-Li Shih&lt;/a&gt;, &lt;a href=&quot;https://lemonatsu.github.io/&quot;&gt;Shih-Yang Su&lt;/a&gt;, &lt;a href=&quot;https://johanneskopf.de/&quot;&gt;Johannes Kopf&lt;/a&gt;, &lt;a href=&quot;https://filebox.ece.vt.edu/~jbhuang/&quot;&gt;Jia-Bin Huang&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发表： CVPR2020&lt;/p&gt;
&lt;p&gt;链接： &lt;a href=&quot;https://github.com/vt-vl-lab/3d-photo-inpainting&quot; class=&quot;uri&quot;&gt;https://github.com/vt-vl-lab/3d-photo-inpainting&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3DCV" scheme="https://jyzhu.top/blog/tags/3DCV/"/>
    
  </entry>
  
  <entry>
    <title>Reading SmoothNet: A Plug-and-Play Network for Refining Human Poses in Videos</title>
    <link href="https://jyzhu.top/blog/Reading-SmoothNet-A-Plug-and-Play-Network-for-Refining-Human-Poses-in-Videos/"/>
    <id>https://jyzhu.top/blog/Reading-SmoothNet-A-Plug-and-Play-Network-for-Refining-Human-Poses-in-Videos/</id>
    <published>2022-09-09T06:25:03.000Z</published>
    <updated>2022-10-22T09:16:13.032Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://arxiv.org/abs/2112.13715</p><p>作者：<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng%2C+A">Ailing Zeng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang%2C+L">Lei Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ju%2C+X">Xuan Ju</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J">Jiefeng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+J">Jianyi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+Q">Qiang Xu</a></p><p>发表： ECCV 2022</p><p>链接： <a href="https://github.com/cure-lab/SmoothNet" class="uri">https://github.com/cure-lab/SmoothNet</a></p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p></blockquote><h2 id="why">Why：</h2><ol type="1"><li>从视频估计人体姿势时，抖动是个问题</li><li>除了轻微抖动以外，有一些long- term抖动，还有因为重叠、姿势少见等原因造成的估测困难</li></ol><h2 id="what">What：</h2><figure><img src="https://s2.loli.net/2022/10/22/fRIB2t9hw8na6my.png" alt="image-20220909143919787" /><figcaption>image-20220909143919787</figcaption></figure><ol type="1"><li>一个仅基于时序的精炼网络，以其他网络的姿势估计结果作为输入。</li><li>有监督的</li><li>采用滑动窗口，基于TCN</li><li>并不是常见的那种基于学习的方法，即采用时间-空间模型来同时优化逐帧的准确率和时序的平滑性。这个方法通过学习每一个关节在长时间范围的运动特征（而不是关节之间的关系），来自然地建模身体运动中的平滑特征。</li><li>由于它仅仅需要时序信息，所以可以泛化到很多种任务上，包括2D和3D的姿势估计、body recovery等</li></ol><h2 id="how">How：</h2><ol type="1"><li><p>根据持续时长，将抖动归类为sudden jitter和long- term jitter两种。为了解决long- term的抖动问题，现有那些方法都不大行。</p><p>根据程度，又可以将抖动分为小抖动和大抖动。小抖动一般由于不可避免的误差，或者标注上的误差；大抖动则是由于图像质量差、姿势少、重叠严重等。</p><figure><img src="https://s2.loli.net/2022/10/22/pNa7bcVqzi9lGZw.png" alt="image-20220909143902334" /><figcaption>image-20220909143902334</figcaption></figure></li><li><p>将误差归类为相邻帧之间的抖动造成的误差（jitter error）和模型估计结果与真实结果之间的偏差（bias error）这两种。现有那些方法并没有将这两类误差解耦</p></li><li><p>提出了basic smoothnet和正经smoothnet。</p><ol type="1"><li><figure><img src="https://s2.loli.net/2022/10/22/Zf6lpahbHrRG7kF.png" alt="image-20220909154626864" /><figcaption>image-20220909154626864</figcaption></figure><p>Basic smoothnet，FCN是backbone。通过长度为T的滑窗，每次传入T帧图像，包含C个channels。</p><figure><img src="https://s2.loli.net/2022/10/22/hRcX2MCx8SnHZfz.png" alt="image-20220909155504588" /><figcaption>image-20220909155504588</figcaption></figure><p>权重<span class="math inline">\(w_t^l\)</span>和偏差<span class="math inline">\(b^l\)</span>是第<span class="math inline">\(t_{th}\)</span>帧的，并且在不同的channel之间是共享的。</p></li><li><figure><img src="https://s2.loli.net/2022/10/22/uCJgF8M6tG5S1PB.png" alt="image-20220909155701901" /><figcaption>image-20220909155701901</figcaption></figure><p>完整的motion- aware smoothnet就是加上了速度和加速度两个模块。</p><p>因为jitter的一个衡量方式就是加速度，所以把加速度直观地显示在模型中是一个很显然的方式。给定预测出的姿势<span class="math inline">\(\hat Y\)</span>，速度就是两帧之间相减，得到 <span class="math display">\[\hat V_{i,t} = \hat Y_{i,t} − \hat Y_{i,t−1}\]</span> 加速度就是速度之间的差： <span class="math display">\[\hat A_{i,t} = \hat V_{i,t} − \hat V_{i,t−1}\]</span></p></li></ol></li><li><p>loss就是两个：</p><ol type="1"><li><p>ground truth pose和估计pose之间的误差： <span class="math display">\[L_{pose} = \frac{1}{T\times C} \sum_{t=0}^T \sum_{i=0}^C |\hat G_{i,t} − Y_{i,t}|,\]</span></p></li><li><p>ground truth 加速度和估计加速度之间的误差： <span class="math display">\[L_{acc} = \frac{1}{(T-2)\times C} \sum_{t=0}^T \sum_{i=0}^C |\hat G&#39;&#39;_{i,t} − A_{i,t}|,\]</span></p></li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：https://arxiv.org/abs/2112.13715&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zeng%2C+A&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang%2C+L&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ju%2C+X&quot;&gt;Xuan Ju&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Li%2C+J&quot;&gt;Jiefeng Li&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang%2C+J&quot;&gt;Jianyi Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu%2C+Q&quot;&gt;Qiang Xu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发表： ECCV 2022&lt;/p&gt;
&lt;p&gt;链接： &lt;a href=&quot;https://github.com/cure-lab/SmoothNet&quot; class=&quot;uri&quot;&gt;https://github.com/cure-lab/SmoothNet&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Reading NeuMan: Neural Human Radiance Field from a Single Video</title>
    <link href="https://jyzhu.top/blog/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/"/>
    <id>https://jyzhu.top/blog/Reading-NeuMan-Neural-Human-Radiance-Field-from-a-Single-Video/</id>
    <published>2022-08-24T04:25:25.000Z</published>
    <updated>2022-09-01T04:57:08.650Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://arxiv.org/abs/2203.12575</p><p>作者：Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag</p><p>发表： ECCV22</p><p>开源代码： https://github.com/apple/ml-neuman</p><h2 id="why">Why：</h2><ol type="1"><li><p>人体的渲染和新姿势生成在增强现实的应用中很重要</p></li><li><p>NeRF的出现让新视角生成任务取得很大进步</p></li><li><p>但是现有工作都没有实现：根据单段wild视频，生成新的人物和新的场景</p><figure><img src="https://s2.loli.net/2022/08/24/NdDrEhojmBLCJz9.png" alt="image-20220824123146868" /><figcaption>image-20220824123146868</figcaption></figure></li></ol><h2 id="what">What：</h2><p>读前疑问：</p><ol type="1"><li>NeRF和人体SMPL模型是怎么有机统一的🤔</li></ol><h2 id="how">How：</h2><ol type="1"><li><p>输入是一段wild视频，moving camera的。用现存方法估计人体姿势、人体形状、人体mask（Mask-RCNN）、相机pose、sparse scene model、depth maps</p></li><li><p>然后训练两个 NeRF 模型，一个用于人体，一个用于由 Mask-RCNN 估计的分割mask引导的背景。 此外，通过将来自多视图重建和单目深度回归的深度估计融合在一起来规范场景 NeRF 模型</p></li><li><p>关于NeRF：(参考：<a href="https://zhuanlan.zhihu.com/p/360365941">zhihu</a>）</p><ol type="1"><li><p>NeRF是用神经辐射场建模一个场景，好处是可以生成新视角的图像。针对一个静态场景，需要提供大量相机参数已知的图片。基于这些图片训练好的神经网络，即可以从任意角度渲染出图片结果了。</p></li><li><p>它用MLP，把一个3d场景隐式地编码进神经网络里。输入为3d空间中一个点的坐标<span class="math inline">\(\bold x = (x,y,z)\)</span>和相机视角 <span class="math inline">\(\bold d = (\theta, \phi)\)</span>，输出为该点对应的体素的密度opacity，以及颜色<span class="math inline">\(\bold c = (r,g,b)\)</span>。公式就是 <span class="math display">\[f(\bold{x},\bold{v})=(\bold c, \sigma)\]</span></p></li><li><p>在具体的实现中， x 首先输入到MLP网络中，并输出 σ 和中间特征，中间特征和 d 再输入到额外的全连接层中并预测颜色。因此，体素密度只和空间位置有关，而颜色则与空间位置以及观察的视角都有关系。基于view dependent 的颜色预测，能够得到不同视角下不同的光照效果。</p></li><li><p>NeRF 函数得到的是一个3D空间点的颜色和密度信息，但当用一个相机去对这个场景成像时，所得到的2D 图像上的一个像素实际上对应了一条从相机出发的射线上的所有连续空间点。后续就有各种各样高效的方式来进行可微渲染了，本质上都是从这条射线上采样，获得平均的颜色信息。</p></li></ol></li></ol><h3 id="人体模型nerfsmpl">人体模型：NeRF+SMPL</h3><p>我主要关注的就是人体模型这部分了。总体来说，做法就是：</p><p>首先生成人体NeRF模型，然后用ROMP生成逐帧的人体SMPL模型，然后定义一个canonical的人体模型（主要是去掉姿势这个变量，变成大字型人体），根据像素点在SMPL模型上对应的位置，再对应到canonical模型上，学到人体的外貌。（其实训练中NeRF和SMPL模型是一起学的，没有分得那么开的先后顺序。）</p><figure><img src="https://s2.loli.net/2022/08/24/JSycDH34UlMBsEW.png" alt="image-20220824180150642" /><figcaption>image-20220824180150642</figcaption></figure><p>具体来说：</p><ol type="1"><li><p>对于某一帧图像，用ROMP估计人体的SMPL模型，但采取了一些改良：</p><ol type="1"><li>利用densepose估计人体的silhouette，以及MMPose估计人体的2D joints；根据这些结果优化SMPL参数</li></ol></li><li><p>把刚刚得到的SMPL模型warp成一个canonical的大字型人体模型，这个warp变换称为<span class="math inline">\(\mathcal T\)</span></p></li><li><p>怎么把图像中的像素点对应到canonical的大字型人体模型上呢？</p><ol type="1"><li><p>首先生成人体NeRF模型</p></li><li><p>对于空间中的每个点<span class="math inline">\(\bold x_f=\bold r_f(t)\)</span> （这里的f是第f帧图像），它都可以由一条射线<span class="math inline">\(\bold r\)</span>上对应的像素点渲染而来；那么对这个点直接应用前面的变换<span class="math inline">\(\mathcal{T}\)</span>，就得到它在canonical空间中对应的点了，<span class="math inline">\(\bold x&#39;_f = \mathcal{T}_{\theta_f}(\bold x_f)\)</span></p></li><li><p>但是因为SMPL的估计不是很准确，这个变换<span class="math inline">\(\mathcal{T}\)</span>也不是很准确，所以这里提出来，通过在训练中同时优化SMPL模型 <span class="math inline">\(\theta_f\)</span>和人体NeRF模型的方式，可以提升效果。</p></li><li><p>还有，还加了一个MLP改错网络<span class="math inline">\(\mathcal{E}\)</span>改正warping的误差。最终结果就是： <span class="math display">\[\bold {\tilde x&#39;_f} = \mathcal{T}_{\theta_f}(\bold x_f) + \mathcal{E}(\bold x_f, f)\]</span></p></li><li><p>此时相机视角也需要校正：对于射线ray上的第i个样本点， <span class="math display">\[\bold d(t_i)&#39;_f = \bold {\hat x}&#39;_f(t_i) - \bold {\hat x}&#39;_f(t_{i-1})\]</span></p></li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：https://arxiv.org/abs/2203.12575&lt;/p&gt;
&lt;p&gt;作者：Jiang, Wei and Yi, Kwang Moo and Samei, Golnoosh and Tuzel, Oncel and Ranjan, Anurag&lt;/p&gt;
&lt;p&gt;发表： ECCV22&lt;/p&gt;
&lt;p&gt;开源代码： https://github.com/apple/ml-neuman&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="3D Generation" scheme="https://jyzhu.top/blog/tags/3D-Generation/"/>
    
  </entry>
  
  <entry>
    <title>It seems impossible to access USB devices in Docker on MacOS</title>
    <link href="https://jyzhu.top/blog/It-seems-impossible-to-access-USB-devices-in-Docker-on-MacOS/"/>
    <id>https://jyzhu.top/blog/It-seems-impossible-to-access-USB-devices-in-Docker-on-MacOS/</id>
    <published>2022-05-16T11:59:12.000Z</published>
    <updated>2022-09-01T04:57:56.460Z</updated>
    
    <content type="html"><![CDATA[<p>According to <a href="https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd" class="uri">https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd</a>, it seems impossible to forward USB to a Docker container on MacOS, coz the Docker is running in a virtual environment via <a href="https://github.com/docker/for-mac/issues/900">hyperkit</a>.</p><p>First of all, ports of the host (i.e., MacOS) cannot be directly accessed by any virtual environment (i.e., Docker) on it. So, &quot;you first have to expose it to the virtual machine where Docker is running&quot;. However, Docker is running on hyperkit, which doesn't support usb forwarding.</p><p>The author provided another way to do it, that is to use<code>docker-machine</code>, which uses a Virtualbox VM to host the <code>dockerd</code> daemon, to replace the original docker. Then... why bother still using Docker, instead of just using Virtualbox to run the seperated environment?</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;According to &lt;a href=&quot;https://dev.to/rubberduck/using-usb-with-docker-for-mac-3fdd&quot; class=&quot;uri&quot;&gt;https://dev.to/rubberduck/using-usb-with-</summary>
      
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Docker" scheme="https://jyzhu.top/blog/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Reading Video-to-Video Synthesis</title>
    <link href="https://jyzhu.top/blog/Reading-Video-to-Video-Synthesis/"/>
    <id>https://jyzhu.top/blog/Reading-Video-to-Video-Synthesis/</id>
    <published>2022-05-13T09:36:36.000Z</published>
    <updated>2022-09-01T04:59:24.160Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf</p><p>作者：<a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a href="http://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, Andrew Tao, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p><p>发表： NeurIPS 2018</p><p>Project：https://tcwang0509.github.io/vid2vid/</p><p>Github：https://github.com/NVIDIA/vid2vid</p><hr /><blockquote><p><strong>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</strong></p><p>这篇论文的背景是当时已经有比较好的pic2pic生成模型了。要让vid2vid也work的话，最重要的应该就是帧与帧之间consistency的问题。所以我会想在将pic2pic生成模型应用在video的基础上，对帧之间加上consistency loss。但直接这样肯定效率很低，因为一个视频中帧与帧之间肯定会包含大量冗余信息嘛，应该还需要想办法让帧之间信息共享，这样模型只需要预测后一帧与前一帧不同的地方，减少运算。例如，用一个小网络预测图像中静态与动态的部分，不知道可不可行。</p><p><strong>看完文章后</strong>：我感觉我在大体思路上把握准了，主要矛盾确实如此。但</p><ol type="1"><li>作者没有用consistency loss，而是用gan的思路，设计了一个条件视频鉴别器 <span class="math inline">\(D_V\)</span>，鉴别视频在时序上的动态是否真实自然。</li><li>我没有optical flow，光流，这方面的知识储备；作者利用一个网络预测optical flow，就可以直接根据前一帧图像得到后一帧图像中对应的像素点了，而且这样的结果能够很consistent。对于前一帧图像中没有对应的像素，再用一个补洞网络补洞。这样就解决了效率问题。</li><li>作者还利用了特征嵌入方法，实现了多模态视频的生成，这是我第一次了解到的方法，感觉很有趣。</li></ol></blockquote><h2 id="why">Why：</h2><ol type="1"><li>图像水平上的生成被研究得很好，但是视频上的此前却比较少；图像生成的成果如果直接放在视频上的话，效果不太妙，因为帧与帧之间缺乏连贯性。所以是需要一些temporal上的改进的</li></ol><h2 id="what">What：</h2><ol type="1"><li>利用GAN和一些时间-空间对抗目标（spatio-temporal adverbial objective），来实现video to video的生成。</li><li>用不同类型的输入来生成新的照片写实风格的视频，效果很好。</li><li>是一个全新定义的vid2vid任务，主要新点在于：输入的vid并不是完整的视频帧，而是一些可以操控的语义信息，例如segmentation masks, sketches, and poses</li></ol><p>读前疑问：</p><ol type="1"><li>在此之前没有比较好的vid2vid，这篇论文是在什么条件下实现了很好的vid2vid效果呢，比如其他方面的技术革新？我觉得主要是利用了gan，一个图像鉴别器+一个视频鉴别器相配合，取得很好的生成效果。除此之外我觉得optical flow用在这里也很好，效率高而且生成效果连贯（但是不知道新不新）；另外特征嵌入方法用在这里也很好，实现了根据语义信息来生成新视频，而且可以是多模态的视频</li><li>摘要里强调的时间-空间对抗目标（spatio-temporal adverbial objective）到底是什么？我感觉正文里好像没有再特别强调时间-空间这一对目标了……根据我自己的理解的话，主要就是那一对鉴别器：时间上--视频鉴别器鉴别在时序上的动态是否真实自然，空间上--图像鉴别器鉴别一张图像在空间上是否真实自然。</li></ol><h2 id="how">How：</h2><h3 id="定义vid2vid任务">定义vid2vid任务</h3><ol type="1"><li><p>定义输入的语义信息序列为<span class="math inline">\(s\)</span>，对应的真实视频序列为<span class="math inline">\(x\)</span>，模型生成的视频序列为<span class="math inline">\(\tilde x\)</span>，则模型的目标是在给定<span class="math inline">\(s\)</span>的条件下，让<span class="math inline">\(\tilde x\)</span>的条件分布拟合<span class="math inline">\(x\)</span>的条件分布</p></li><li><figure><img src="https://s2.loli.net/2022/09/01/VfJE42QpgUB5zxt.png" alt="image-20220524143313609" /><figcaption>image-20220524143313609</figcaption></figure><p>D是discriminator，G是generator。整个任务就变成了一个最大最小优化问题，论文主要通过设计网络和时空优化目标来解决这个问题。</p></li><li><p>为了简化问题，做了一个Markov假设：当前第t帧生成的视频<span class="math inline">\(\tilde x_t\)</span>，由且仅由第t帧输入<span class="math inline">\(s_t\)</span> + 前L帧输入<span class="math inline">\(s_{t-L}^{t-1}\)</span> + 前L帧生成的视频<span class="math inline">\(\tilde x_{t-L}^{t-1}\)</span>这三个因素决定。<img src="https://s2.loli.net/2022/09/01/Q9MPRD5otda3mAq.png" alt="image-20220524144030781" /></p><p>实验里，L取了个不大不小的2.建立了一个神经网络F，递归地逐帧生成视频。</p></li></ol><h3 id="网络架构">网络架构</h3><ol type="1"><li><p>网络F定义如下：</p><figure><img src="https://s2.loli.net/2022/09/01/5OA4ptSUjI9Mn7J.png" alt="image-20220524144722968" /><figcaption>image-20220524144722968</figcaption></figure><p>给定<span class="math inline">\((\tilde x_{t-L}^{t-1}, s_{t}^{t-1})\)</span>作为输入。对于与上一帧图像有关联的像素点，网络会利用optical flow来warp（扭曲？）上一帧像素点，得到这一帧新的像素点。对应等式的前半部分。还有一些像素是上一帧图像里没有的，这时候就需要生成来填充。对应等式的后半部分。具体来说：</p><ul><li>用一个optical flow预测网络W来估计从上一帧到这一帧的optical flow <span class="math inline">\(\tilde w_{t-1}\)</span>.</li><li>用一个生成器H来生成需要填充的像素<span class="math inline">\(\tilde h_t\)</span>.</li><li>用一个mask预测网络M来生成mask <span class="math inline">\(\tilde m_t\)</span>. 这个mask不是非0即1的，而是包含了0-1之间的连续值。这样做是为了更好地融合W和H生成的结果。比如说，在 zoom in 的情况下，一个物体逐渐靠近，那么它会逐帧放大。如果仅仅利用optical flow的扭曲结果，那么这个物体就会变得模糊。因此，还需要生成器来填充一些细节。有了soft mask，warp的像素和生成的像素就可以融合。</li></ul></li><li><p>用了coarse-to-fine的方法来生成高分辨率的视频</p></li><li><p>用了2个不同的discriminator来减轻gan训练中的mode collapse问题（模式倒塌，即生成的结果是很逼真，但是多样性不足）。</p><ol type="1"><li>条件图像鉴别器 <span class="math inline">\(D_I\)</span>，顾名思义，鉴别每一帧图像是否真实的</li><li>条件视频鉴别器 <span class="math inline">\(D_V\)</span>，鉴别视频在时序上的动态是否真实自然。给定optical flow，鉴别K个连续的帧</li></ol></li></ol><h3 id="losses">losses</h3><p><span class="math display">\[\mathop{min}\limits_{F} ( \mathop{max}\limits_{D_I}\mathcal{L}_I (F, D_I ) + \mathop{max}\limits_{D_V}  \mathcal{L}_V (F, D_V )) + λ_W L_W (F ),\]</span></p><ol type="1"><li><span class="math inline">\(\mathcal{L}_I\)</span> 是条件图像鉴别器 <span class="math inline">\(D_I\)</span>的gan loss：<img src="https://s2.loli.net/2022/09/01/J1mVkCnzWYZiaR9.png" alt="image-20220530194307049" />，其中，<span class="math inline">\(\phi_I\)</span> 就是从第1～T帧中随机取1帧的操作</li><li><span class="math inline">\(\mathcal{L}_V\)</span>是条件视频鉴别器 <span class="math inline">\(D_V\)</span>的 gan loss：<img src="https://s2.loli.net/2022/09/01/rq6yiaRSHUnEYzg.png" alt="image-20220530194419594" />，和图像的如出一辙，<span class="math inline">\(\phi_V\)</span> 就是从第1～T帧中随机取连续K帧的操作</li><li><span class="math inline">\(L_W\)</span> 是flow estimation loss：<img src="https://s2.loli.net/2022/09/01/AuZf1zlcihE3KsU.png" alt="image-20220530194534485" />，包括两部分，1. 真实flow和估计flow的端点误差 2. 把前一帧扭曲到后一帧的warp loss</li></ol><h3 id="前景-背景先验">前景-背景先验</h3><p>通过语义分割，给模型提供了一个前景、背景的先验信息，同时把补洞网络拆分成了两个：</p><ol type="1"><li>背景补洞网络：这个很容易，因为整个大背景其实可以由optical flow很准确地预测出来，补洞网络只需要补一点点从画面外面刚进来的部分</li><li>前景补洞网络：这个比较难，因为前景物品往往占比不大，但是又动作幅度很大，前景补洞网络需要从零开始生成很多东西</li></ol><p>通过用户实验，证明大部分人觉得有这个前景-背景先验之后，效果更好。</p><h3 id="多模态生成">多模态生成</h3><p>网络F是一个单模态映射函数，这意味着输入一个视频，它也只能生成一个视频。那怎样让它根据同一个输入视频，输出多个不同的视频呢？这里采用了特征嵌入方法（feature embedding scheme）。</p><ol type="1"><li>输入源视频的同时，也输入instance级别的语义分割mask <span class="math inline">\(s_t\)</span></li><li>训练一个图像编码器E，它把每一帧真实图像<span class="math inline">\(x_t\)</span>编码成d维的特征map（论文中d取3）。然后用一个instance-wise的平均池化，来让同一个物体的所有像素分享共同的特征向量。得到的这个instance-wise平均池化后的图像特征map称为<span class="math inline">\(z_t\)</span></li><li>这个<span class="math inline">\(z_t\)</span>，加上mask <span class="math inline">\(s_t\)</span>，再被输入到网络F</li><li>以上是训练的过程。训练结束后，对每个类型的对象的特征向量的高斯分布拟合一个混合高斯分布。</li><li>测试的时候，利用每个物体所对应的类型的分布，可以sample特征向量，进而生成新视频了。给出不同的特征向量，F就能生成不同的视频了</li></ol><figure><img src="https://tcwang0509.github.io/vid2vid/paper_gifs/cityscapes_change_styles.gif" alt="img" /><figcaption>img</figcaption></figure><h3 id="实现的细节">实现的细节</h3><ol type="1"><li>coarse-to-fine的训练：512 × 256, 1024 × 512, and 2048 × 1024 resolutions，这三种分辨率，先从低的开始训练起，逐渐增加到高的。</li><li>mask预测网络M和flow预测网络W共享权重，只有输出层不一样。</li><li>图像鉴别器是一个多尺度PatchGAN</li><li>除了空间上的多尺度，视频鉴别器还会考虑多个帧率，即时间上的多尺度，确保短期和长期都能consistent</li><li>2k分辨率，8个v100 gpus，训练10天……</li><li>datasets：<ol type="1"><li>Cityscapes：训练DeepLabV3网络来获得所有的语义分割mask，用FlowNet2的结果作为optical flow的ground truth，用Mask R- CNN的结果作为instance- level 语义mask的gt</li><li>Apolloscape</li><li>Face video dataset：FaceForensics dataset里的真实视频</li><li>Dance video dataset：从YouTube下载的跳舞视频💃</li></ol></li></ol><h3 id="结果">结果</h3><p>图像生成模型pix2pixHD和视频风格迁移模型COVST作为baseline。FID（论文定义的视频变种）跟baseline比略好，但human preference score（论文定义的由人来打分，评估哪个视频更真实）高很多。</p><p>通过更改语义mask，可以控制生成视频中的物体种类；通过更改特征向量，可以控制生成视频中的物体外观；在未来视频预测上也有很好的性能。</p><h3 id="局限性">局限性</h3><ol type="1"><li>缺少一个物体内部的具体信息，生成转弯的车的时候效果比较差。论文提出或许可以通过增加3D信息作为输入来解决</li><li>在整个视频中，一个物体的外观有时候还是前后不一致</li><li>偶然情况下，一辆车的颜色可能会逐渐发生变化</li><li>当通过更改语义信息来操纵视频生成的时候，例如把树改成房子，偶然会出现一部分变成房子，另一部分树变了形的情况（？是这个意思吗）。这或许可以通过采用更粗糙的语义标签的方式解决，因为这样模型就不会对标签形状过于敏感</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://tcwang0509.github.io/&quot;&gt;Ting-Chun Wang&lt;/a&gt;, &lt;a href=&quot;http://mingyuliu.net/&quot;&gt;Ming-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://www.cs.cmu.edu/~junyanz/&quot;&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href=&quot;https://liuguilin1225.github.io/&quot;&gt;Guilin Liu&lt;/a&gt;, Andrew Tao, &lt;a href=&quot;http://jankautz.com/&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://catanzaro.name/&quot;&gt;Bryan Catanzaro&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发表： NeurIPS 2018&lt;/p&gt;
&lt;p&gt;Project：https://tcwang0509.github.io/vid2vid/&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="Video Generation" scheme="https://jyzhu.top/blog/tags/Video-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Reading Few-shot Video-to-Video Synthesis</title>
    <link href="https://jyzhu.top/blog/Reading-Few-shot-Video-to-Video-Synthesis/"/>
    <id>https://jyzhu.top/blog/Reading-Few-shot-Video-to-Video-Synthesis/</id>
    <published>2022-05-13T08:49:57.000Z</published>
    <updated>2022-09-01T05:00:11.103Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：http://arxiv.org/abs/1910.12713</p><p>作者：<a href="https://tcwang0509.github.io/">Ting-Chun Wang</a>, <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, Andrew Tao, <a href="https://liuguilin1225.github.io/">Guilin Liu</a>, <a href="http://jankautz.com/">Jan Kautz</a>, <a href="http://catanzaro.name/">Bryan Catanzaro</a></p><p>发表： NeurIPS 2019</p><p>Project： https://nvlabs.github.io/few-shot-vid2vid</p><p>Github：https://github.com/NVLabs/few-shot-vid2vid</p><hr /><blockquote><p>如果你去做这个任务，会怎么做？作者做的方法和你想的有什么差异？</p><p>首先这个任务选题对我来说很新，我之前都没有意识到过这方面的问题。如果告诉我有这样的问题，需要去解决的话，我的直观的想法会受到这篇论文作者的上一篇中提到的 特征嵌入方法 所影响：会想也通过将一类物体的特征编码起来，然后通过学习不同个体的特征编码，来实现不同风格的视频生成。</p></blockquote><h2 id="why">Why：</h2><p>当今vid2vid方法的两个局限性：</p><ol type="1"><li>需要大量数据，尤其是需要生成的这个人的视频数据</li><li>泛化能力有限，比如说只能在训练集中包含的人上生成新的pose-to-human视频，不能泛化到训练集中不存在的人上</li></ol><p>所以这篇论文就是想解决这两个问题。</p><h2 id="what">What：</h2><ol type="1"><li><p>任务是Video-to-video synthesis，即利用输入的语义视频（例如人的姿势、街景），生成写实的视频。例如说，人体姿势生成的任务，就是首先收集一个人做大量不同动作的视频，作为训练集；然后向模型中输入动作序列，让模型生成该人做该动作的视频。再比如街景生成，也是以大量街景视频作为训练集，然后向模型中输入语义mask序列，让它生成风格类似的全新街景。</p></li><li><p>这篇论文提出了一个网络，其中包括一个网络权重生成模块（novel network weight generation module）和attention机制</p></li><li><p>这个方法的创新点在于，只需要在测试时，向模型提供少量的在训练集中没出现过的新的人物的图像，它就能生成这个新的人的视频。</p><figure><img src="https://s2.loli.net/2022/08/10/IUKDziV4AtOwsxu.png" alt="image-20220513171420180" /><figcaption>image-20220513171420180</figcaption></figure><p>上图中，左边是现存方法，它们基本上对于每个人，都需要在单独的训练集上训练。右边是这篇论文提出的方法，只需要训练一次，然后输入一些示范图像，就可以泛化到新的人上。</p></li></ol><p>读前疑问：</p><ol type="1"><li>说是利用少量的新的人物的示范图像，生成网络权重。意思是以原本的vid2vid网络的权重作为输出？为什么？我的更直观的想法是，直接用一个新网络，学习新人物的图像，然后把output给concat或者加进旧网络的output中……另外，直接作用于网络权重上，在我的粗浅理解中，会不会造成信息的损失呢？还是说本质上没差？ related work里提到这类网络属于adaptive network，跟常规网络相比有不同的inductive bias（想想也是），有对应的应用任务。或许我之后再了解一下这块。</li><li>标题中的few-shot是什么意思，就是指更少的data、更高的泛化性吗？这是一类任务吧，从少量标注的样本中学习的意思。这个确实就是啊，只需要一点点示范图像，就可以生成图中这个人/物的新video。</li><li>作者的上篇论文是利用gan，这篇又用上了attention，为什么作出这样本质的改变呢？</li></ol><h2 id="how">How：</h2><ol type="1"><li>视频生成任务可以分成3类：<ol type="1"><li>unconditional synthesis：随机生成视频片段</li><li>future video prediction</li><li>vid2vid：把语义输入转变成现实风的视频。这篇论文就是属于这个任务，不过它聚焦的点在于few shot，即通过在测试的时候输入少量图像，让生成的视频可以泛化到没见过的domain上</li></ol></li><li><p>vid2vid是前一篇工作的内容啦。reference：<a href="jyzhu.top/reading-video-to-video-synthesis"><em>Reading vid2vid</em></a></p></li><li><p>few-shot本质上就是多加了个网络E，用来生成原补洞网络H的权重。至于原本还有两个网络W和M，他们都不需要改动，因为他们都是基于上一帧生成的图像进行变形的，代表一种运动，而和视频本质的内容没有关系。</p></li><li><p>精髓一图：</p><figure><img src="https://s2.loli.net/2022/08/10/T6is35cjyHlvaNe.png" alt="image-20220810193845710" /><figcaption>image-20220810193845710</figcaption></figure></li><li><p>用最新的SOTA语义图像生成模型SPADE代替了上一篇论文中的网络H。SPADE包含several spatial modulation branches and a main image synthesis branch。不过网络E只更新SPADE模型中的spatial modulation branches的权重，因为1这样量比较小，2这样可以避免一个直接从input image到output image的短路（我尚没有深究原因）。</p></li><li><p>权重生成模块E。</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;论文地址：http://arxiv.org/abs/1910.12713&lt;/p&gt;
&lt;p&gt;作者：&lt;a href=&quot;https://tcwang0509.github.io/&quot;&gt;Ting-Chun Wang&lt;/a&gt;, &lt;a href=&quot;http://mingyuliu.net/&quot;&gt;Ming-Yu Liu&lt;/a&gt;, Andrew Tao, &lt;a href=&quot;https://liuguilin1225.github.io/&quot;&gt;Guilin Liu&lt;/a&gt;, &lt;a href=&quot;http://jankautz.com/&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://catanzaro.name/&quot;&gt;Bryan Catanzaro&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;发表： NeurIPS 2019&lt;/p&gt;
&lt;p&gt;Project： https://nvlabs.github.io/few-shot-vid2vid&lt;/p&gt;</summary>
    
    
    
    <category term="Computer Notes" scheme="https://jyzhu.top/blog/categories/Computer-Notes/"/>
    
    
    <category term="Computer Vision" scheme="https://jyzhu.top/blog/tags/Computer-Vision/"/>
    
    <category term="Video" scheme="https://jyzhu.top/blog/tags/Video/"/>
    
  </entry>
  
</feed>
